import os
from dotenv import load_dotenv
import warnings
import utils
import gradio as gr
import functools
import time

# Load environment variables from .env file or Secret Manager
_ = load_dotenv("../.env")
tavily_ai_api_key = os.getenv("TAVILY_API_KEY")
huggingface_token = os.getenv("HUGGINGFACE_API_TOKEN")
MONGODB_URI = os.getenv("MONGODB_URI")  # Added MongoDB connection string
warnings.filterwarnings("ignore", message=".*TqdmWarning.*")

from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated, List, Dict, Any, Optional, Literal, cast, Union
import operator
from langgraph.checkpoint.mongodb import MongoDBSaver  # Changed from SQLite to MongoDB
from pymongo import MongoClient  # Added MongoDB client import
from langchain_core.messages import (
    SystemMessage,
    HumanMessage,
    AIMessage,
)
from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch
from huggingface_hub import login
from pydantic import BaseModel, Field
from langchain_text_splitters import RecursiveCharacterTextSplitter
from tavily import TavilyClient
import sqlite3
from datetime import datetime
from uuid import uuid4
from langchain_core.runnables import RunnableConfig
from langchain_core.callbacks.manager import CallbackManager
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.tracers import LangChainTracer
from langsmith import Client as LangSmithClient

# for the output parser
from typing import List
from langchain.output_parsers import PydanticOutputParser
from langchain_core.prompts import PromptTemplate
import json
import asyncio
import time
import re
from langchain_core.runnables import Runnable, RunnableConfig
from langchain_core.globals import set_debug
import networkx as nx
import matplotlib.pyplot as plt
import threading
import queue
import traceback
import psutil
import gc

# Setup LangSmith tracer
langchain_tracer = None
langsmith_project_name = os.getenv("LANGSMITH_PROJECT", "storybook")
if os.getenv("LANGSMITH_TRACING") == "true" and os.getenv("LANGSMITH_API_KEY"):
    langchain_tracer = LangChainTracer(
        project_name=langsmith_project_name,
    )
    # Set debug to get more detailed tracing
    set_debug(True)
    print(f"LangSmith tracing enabled for project: {langsmith_project_name}")
else:
    print("LangSmith tracing not enabled. Check LANGSMITH_API_KEY and LANGSMITH_TRACING environment variables.")

# Initialize the callback manager for streaming
streaming_callback = StreamingStdOutCallbackHandler()
callback_manager = CallbackManager([streaming_callback])
if langchain_tracer:
    callback_manager.add_handler(langchain_tracer)

def extract_chunk_references(message: str) -> List[int]:
    """Extract chunk references from a message."""
    chunk_refs = []
    
    # Look for patterns like "Chunk 3" or "chunks 4-6"
    chunk_patterns = re.findall(r"[Cc]hunk\s+(\d+)(?:\s*-\s*(\d+))?", message)
    
    for start, end in chunk_patterns:
        start_idx = int(start)
        if end and end.strip():  # If it's a range
            end_idx = int(end)
            for i in range(start_idx, end_idx + 1):
                chunk_refs.append(i)
        else:  # If it's a single chunk
            chunk_refs.append(start_idx)
    
    return chunk_refs

# Define the project types and input states
class ProjectType:
    NEW = "new"
    EXISTING = "existing"

class NewProjectInput(TypedDict):
    title: str
    synopsis: str
    manuscript: str
    notes: Optional[Dict[str, Any]]

class ExistingProjectInput(TypedDict):
    project_id: str

class ProjectData(TypedDict):
    id: str
    title: str
    synopsis: str
    manuscript: str
    manuscript_chunks: List[Dict[str, Any]]
    notes: Optional[Dict[str, Any]]
    type: str
    quality_assessment: Dict[str, Any]
    created_at: str

class InputState(TypedDict):
    project_type: str
    project_data: Dict[str, Any]
    task: str

# Define the research state classes
class ResearchState(TypedDict):
    query: str
    results: List[Dict[str, Any]]
    summary: str

class DomainResearchState(ResearchState):
    domain_specific_data: Dict[str, Any]

class CulturalResearchState(ResearchState):
    cultural_context: Dict[str, Any]

class MarketResearchState(ResearchState):
    market_trends: Dict[str, Any]

class FactVerificationState(ResearchState):
    verification_status: Dict[str, bool]

# State for the storybook application
class AgentState(TypedDict):
    # storybook states
    project: Optional[ProjectData]
    phase: Optional[str]
    phase_history: Optional[Dict[str, List[Dict[str, Any]]]]
    current_input: Optional[Dict[str, Any]]
    messages: Optional[List[Dict[str, Any]]]
    # Tracking state
    count: Annotated[int, operator.add]
    lnode: str

class Configuration(BaseModel):
    quality_gates: Dict[str, Dict[str, Any]] = Field(default_factory=dict)
    
    @classmethod
    def from_runnable_config(cls, config: Dict[str, Any]) -> 'Configuration':
        """Extract configuration from a runnable config."""
        configurable = config.get("configurable", {})
        return cls(
            quality_gates=configurable.get("quality_gates", {})
        )

class storybookConfig(BaseModel):
    model_name: str
    temperature: float
    max_tokens: Optional[int] = None
    
    model_config = {
        "extra": "forbid"
    }

def split_manuscript(manuscript: str, chunk_size: int = 1000, chunk_overlap: int = 0) -> List[Dict[str, Any]]:
    """Split a manuscript into manageable chunks using RecursiveCharacterTextSplitter."""
    if not manuscript:
        return []
    
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    
    # Adjust chunk size based on manuscript length for better performance with large manuscripts
    manuscript_length = len(manuscript)
    if manuscript_length > 100000:  # If over 100K characters
        chunk_size = 2000  # Larger chunks
    if manuscript_length > 500000:  # If over 500K characters
        chunk_size = 5000  # Even larger chunks
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    texts = text_splitter.split_text(manuscript)
    
    # Create chunks with metadata
    chunks = []
    for i, text in enumerate(texts):
        chunks.append({
            "chunk_id": i,
            "content": text,
            "start_char": manuscript.find(text),
            "end_char": manuscript.find(text) + len(text),
        })
    
    # If we have too many chunks (could cause performance issues),
    # combine some adjacent chunks to reduce the total number
    max_chunks = 100
    if len(chunks) > max_chunks:
        print(f"Large manuscript detected with {len(chunks)} chunks. Consolidating to improve performance.")
        consolidated_chunks = []
        for i in range(0, len(chunks), len(chunks) // max_chunks + 1):
            end_idx = min(i + len(chunks) // max_chunks + 1, len(chunks))
            content = "".join([chunk["content"] for chunk in chunks[i:end_idx]])
            consolidated_chunks.append({
                "chunk_id": len(consolidated_chunks),
                "content": content,
                "start_char": chunks[i]["start_char"],
                "end_char": chunks[end_idx-1]["end_char"] if end_idx <= len(chunks) else chunks[-1]["end_char"],
                "original_chunks": list(range(i, end_idx))
            })
        chunks = consolidated_chunks
    
    return chunks

def check_quality_gate(gate_name: str, quality_assessment: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """Check if a quality gate is passed."""
    gates = config.get("quality_gates", {})
    gate_config = gates.get(gate_name, {})
    
    if not gate_config:
        # If no gate is defined, default to passing
        return {"passed": True, "message": f"No quality gate defined for {gate_name}"}
    
    # Check each criterion in the gate
    passed = True
    reasons = []
    
    for criterion, threshold in gate_config.items():
        if criterion in quality_assessment:
            value = quality_assessment[criterion]
            if value < threshold:
                passed = False
                reasons.append(f"{criterion}: {value} (below threshold {threshold})")
        else:
            # If the criterion is not in the assessment, consider it failed
            passed = False
            reasons.append(f"{criterion}: not assessed (required)")
    
    return {
        "passed": passed,
        "message": "Quality gate passed" if passed else "Quality gate failed",
        "reasons": reasons
    }

def init_cuda():
    """Initialize CUDA and handle warnings appropriately"""
    try:
        # First, check if PyTorch itself is correctly installed
        print(f"PyTorch version: {torch.__version__}")
        
        if torch.cuda.is_available():
            try:
                # Print available GPU information
                gpu_count = torch.cuda.device_count()
                print(f"Found {gpu_count} GPU devices:")
                
                for i in range(gpu_count):
                    try:
                        device_props = torch.cuda.get_device_properties(i)
                        print(f"  GPU {i}: {device_props.name}, {device_props.total_memory / 1e9:.2f} GB memory")
                    except Exception as device_err:
                        print(f"  GPU {i}: Error getting properties: {str(device_err)}")
                
                # Try a simple tensor operation to confirm CUDA works
                test_tensor = torch.tensor([1.0, 2.0, 3.0], device="cuda:0")
                test_result = test_tensor * 2
                print(f"CUDA tensor test successful: {test_result.device}")
                
                # Set appropriate memory optimization flags if that worked
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True
                print("TF32 enabled for matrix multiplications where supported")

                # Set more conservative memory settings
                os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128,garbage_collection_threshold:0.8"
                
                # Set PyTorch to release memory more aggressively
                if hasattr(torch.cuda, 'empty_cache'):
                    print("Enabling automatic CUDA cache flushing")
                    torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"CUDA initialization warning (non-fatal): {str(e)}")
                print("Continuing with CPU execution")
                os.environ["CUDA_VISIBLE_DEVICES"] = ""  # Disable CUDA
        else:
            print("CUDA not available - using CPU")
            
    except Exception as e:
        print(f"Error during PyTorch/CUDA setup: {str(e)}")
        print("Continuing with CPU execution")
        os.environ["CUDA_VISIBLE_DEVICES"] = ""  # Disable CUDA

# Monitor memory usage
def get_memory_usage():
    """Get current memory usage statistics"""
    # System memory
    sys_memory = psutil.virtual_memory()
    
    # GPU memory if available
    gpu_memory = {}
    if torch.cuda.is_available():
        try:
            for i in range(torch.cuda.device_count()):
                gpu_memory[i] = {
                    "total": torch.cuda.get_device_properties(i).total_memory,
                    "allocated": torch.cuda.memory_allocated(i),
                    "reserved": torch.cuda.memory_reserved(i)
                }
        except Exception as e:
            gpu_memory["error"] = str(e)
    
    return {
        "system": {
            "total": sys_memory.total,
            "available": sys_memory.available,
            "percent_used": sys_memory.percent
        },
        "gpu": gpu_memory
    }

# Optimized memory cleanup
def cleanup_memory():
    """Aggressive memory cleanup to prevent leaks"""
    # Clear CUDA cache
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # Force Python garbage collection
    gc.collect()
    
    # Return current memory stats
    return get_memory_usage()

# Retry decorator for model invocation
def retry_with_exponential_backoff(max_retries=3, initial_delay=1, backoff_factor=2):
    """Retry decorator with exponential backoff."""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            delay = initial_delay
            last_exception = None
            
            for retry in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    print(f"Attempt {retry + 1}/{max_retries} failed with error: {str(e)}")
                    
                    # Clean up memory after a failure
                    memory_stats = cleanup_memory()
                    print(f"Memory cleanup performed. System memory: {memory_stats['system']['percent_used']}% used")
                    
                    # No delay after the last attempt
                    if retry < max_retries - 1:
                        sleep_time = delay * (backoff_factor ** retry)
                        print(f"Retrying in {sleep_time:.1f} seconds...")
                        time.sleep(sleep_time)
            
            # All retries failed
            print(f"All {max_retries} attempts failed. Last error: {str(last_exception)}")
            raise last_exception
            
        return wrapper
    return decorator

class AgentFactory:
    def __init__(self, config, model_config=None, tavily_client=None):
        # Check for HuggingFace token
        self.huggingface_token = os.getenv("HUGGINGFACE_API_TOKEN")
        if not self.huggingface_token:
            raise ValueError("HUGGINGFACE_API_TOKEN not found in environment variables")
        
        # Log into Hugging Face
        login(token=self.huggingface_token)
        
        # Initialize with default model config if none provided
        self.default_model_config = model_config or {
            "model_id": "HuggingFaceH4/zephyr-7b-beta",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,  # Changed from False to True
            "repetition_penalty": 1.03
        }
        
        # Initialize agent-specific model configs
        self.agent_model_configs = {}
        if isinstance(model_config, dict) and model_config.get("agent_configs"):
            self.agent_model_configs = model_config.get("agent_configs")
        
        # Setup callback manager for streaming and tracing BEFORE creating the model
        self.callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
        if langchain_tracer:
            self.callback_manager.add_handler(langchain_tracer)
        
        self.config = config
        self.tavily = tavily_client or TavilyClient(api_key=tavily_ai_api_key)
        
        # Initialize models dictionary to store agent-specific models
        self.models = {}
        
        # Add model cache with LRU policy
        self.model_cache = {}
        self.model_cache_max_size = 3  # Maximum number of models to keep in cache
        self.model_usage_tracker = []  # Track model usage for LRU policy
        
        # MongoDB connection for prompts
        self.mongo_client = None
        mongo_uri = os.getenv("MONGODB_URI")
        if mongo_uri:
            try:
                self.mongo_client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)
                self.mongo_client.admin.command('ping')  # Test connection
                print("MongoDB connection for AgentFactory successful")
            except Exception as e:
                print(f"Warning: Could not connect to MongoDB: {str(e)}. Using default prompts.")
    
    def load_prompt_from_mongodb(self, agent_name):
        """Load agent prompt from MongoDB, returning None if not found"""
        if not self.mongo_client:
            return None
            
        try:
            db = self.mongo_client["storybook"]
            prompts_collection = db["prompts"]
            
            # Try to find the agent's prompt
            doc = prompts_collection.find_one({"agent_name": agent_name})
            if doc and "prompt_text" in doc:
                print(f"Loaded prompt for {agent_name} from MongoDB")
                return doc["prompt_text"]
        except Exception as e:
            print(f"Error loading prompt from MongoDB for {agent_name}: {str(e)}")
            
        return None
    
    def _create_huggingface_model(self, model_config):
        """Create a HuggingFacePipeline model with optimized settings for multi-GPU inference."""
        try:
            # Check for CUDA availability more carefully
            device_type = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"Using device type: {device_type}")
            
            # Set torch dtype based on device and availability
            if device_type == "cuda":
                # Check if bfloat16 is supported, otherwise fall back to float16
                torch_dtype = torch.bfloat16 if hasattr(torch.cuda, 'is_bf16_supported') and torch.cuda.is_bf16_supported() else torch.float16
            else:
                # For CPU, use bfloat16 if supported by CPU architecture, otherwise float32
                torch_dtype = torch.bfloat16 if hasattr(torch.cpu, 'is_bf16_supported') and torch.cpu.is_bf16_supported() else torch.float32
            
            print(f"Using torch dtype: {torch_dtype}")
            
            # Model kwargs with safer settings to prevent device errors
            model_kwargs = {
                "low_cpu_mem_usage": True,
                "torch_dtype": torch_dtype,
            }
            
            # Only set device_map if safe to do so
            if device_type == "cuda":
                # For GPU, use more explicit device mapping to avoid device errors
                if torch.cuda.device_count() == 1:
                    model_kwargs["device_map"] = 0  # Explicitly set to first GPU
                else:
                    # For multiple GPUs, use balanced mapping instead of auto
                    model_kwargs["device_map"] = "balanced"
            else:
                # For CPU, explicitly set to CPU
                model_kwargs["device_map"] = "cpu"
                
            # Create pipeline with optimized settings
            llm = HuggingFacePipeline.from_model_id(
                model_id=model_config.get("model_id", "HuggingFaceH4/zephyr-7b-beta"),
                task=model_config.get("task", "text-generation"),
                pipeline_kwargs={
                    "max_new_tokens": model_config.get("max_new_tokens", 512),
                    "temperature": model_config.get("temperature", 0.1),
                    "do_sample": model_config.get("do_sample", True),  # Changed default to True
                    "repetition_penalty": model_config.get("repetition_penalty", 1.03),
                    "return_full_text": False,
                    "token": self.huggingface_token,
                },
                model_kwargs=model_kwargs
            )
            
            # Get the underlying model and configure it safely
            if hasattr(llm, "pipeline") and hasattr(llm.pipeline, "model"):
                model = llm.pipeline.model
                
                # Only set static KV cache if the attribute exists and we're on CUDA
                if device_type == "cuda" and hasattr(model, "generation_config") and \
                   hasattr(model.generation_config, 'cache_implementation'):
                    try:
                        model.generation_config.cache_implementation = "static"
                        print("Static KV cache enabled for optimized inference")
                    except Exception as cache_err:
                        print(f"Could not set static KV cache: {cache_err}")
            
            # Create ChatHuggingFace from the pipeline
            chat_model = ChatHuggingFace(llm=llm, callbacks=self.callback_manager)
            return chat_model
                
        except Exception as e:
            print(f"Error creating HuggingFace model: {str(e)}")
            # Fallback to a smaller model if there's an issue
            try:
                print("Falling back to small model (google/flan-t5-base)...")
                llm = HuggingFacePipeline.from_model_id(
                    model_id="google/flan-t5-base",  # Small model that should work on almost any hardware
                    task="text2text-generation",
                    pipeline_kwargs={
                        "max_new_tokens": 256,  # Smaller value
                        "temperature": 0.1,
                        "token": self.huggingface_token,
                    },
                    model_kwargs={
                        "device_map": "auto",  # Let it decide the safest option
                        "torch_dtype": torch.float32,  # Use safer float32
                    }
                )
                chat_model = ChatHuggingFace(llm=llm, callbacks=self.callback_manager)
                return chat_model
            except Exception as fallback_error:
                print(f"Fallback model also failed: {fallback_error}")
                # Last resort - create a simple mock model that won't crash
                from langchain_core.language_models.chat_models import BaseChatModel
                
                class SafeFallbackModel(BaseChatModel):
                    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
                        return AIMessage(content="I'm sorry, there was an issue loading the language model. Please check your system configuration or try a different model.")
                
                print("Using emergency fallback model")
                return SafeFallbackModel()
    
    def _update_model_cache(self, agent_name, model):
        """Update the model cache using LRU policy"""
        # If model already in cache, update usage
        if agent_name in self.model_cache:
            # Remove from usage tracker and add to end (most recently used)
            if agent_name in self.model_usage_tracker:
                self.model_usage_tracker.remove(agent_name)
            self.model_usage_tracker.append(agent_name)
            return
            
        # If cache is full, remove least recently used model
        if len(self.model_cache) >= self.model_cache_max_size:
            # Get the least recently used agent
            if self.model_usage_tracker:
                lru_agent = self.model_usage_tracker.pop(0)
                if lru_agent in self.model_cache:
                    print(f"Removing {lru_agent} model from cache (LRU policy)")
                    # Remove the model from cache
                    del self.model_cache[lru_agent]
                    # Force cleanup
                    cleanup_memory()
        
        # Add new model to cache
        self.model_cache[agent_name] = model
        self.model_usage_tracker.append(agent_name)
    
    def get_model(self, agent_name=None):
        """Lazy-load the model for the specified agent or the default model with efficient caching."""
        cache_key = agent_name or "default"
        
        # Check if model is in cache
        if cache_key in self.model_cache:
            print(f"Using cached model for {cache_key}")
            # Update usage tracking
            if cache_key in self.model_usage_tracker:
                self.model_usage_tracker.remove(cache_key)
            self.model_usage_tracker.append(cache_key)
            return self.model_cache[cache_key]
        
        # Not in cache, need to load model
        print(f"Model for {cache_key} not in cache, loading...")
            
        # Determine which model config to use
        model_config = self.default_model_config
        if agent_name and agent_name in self.agent_model_configs:
            model_config = self.agent_model_configs[agent_name]
            print(f"Using agent-specific model for {agent_name}: {model_config['model_id']}")
        
        try:
            print(f"Creating HuggingFace model for {agent_name or 'default'} with {model_config.get('model_id', 'unknown')}")
            model = self._create_huggingface_model(model_config)
            
            # Update model cache
            self._update_model_cache(cache_key, model)
                
            print(f"Model creation successful for {agent_name or 'default'}")
            return model
        except Exception as e:
            print(f"Error during model creation for {agent_name or 'default'}: {str(e)}")
            print("Creating safe fallback model...")
            
            # Create a simple fallback model
            from langchain_core.language_models.chat_models import BaseChatModel
            
            class SafeFallbackModel(BaseChatModel):
                def _generate(self, messages, stop=None, run_manager=None, **kwargs):
                    return AIMessage(content=f"I'm sorry, there was an issue loading the language model for {agent_name or 'default'}. Please check your system configuration or try a different model.")
            
            fallback = SafeFallbackModel()
            # Don't cache fallback models
            return fallback
    
    def create_research_agent(self, research_type: str):
        """Create a research agent function."""
        
        def research_agent_function(state: AgentState) -> AgentState:
            """Research agent function that processes the current state."""
            project = state.get("project", {})
            current_input = state.get("current_input", {})
            
            # Extract research query from task or create one based on context
            research_query = current_input.get("research_query", "")
            if not research_query:
                task = current_input.get("task", "")
                manuscript_excerpt = ""
                if project.get("manuscript_chunks"):
                    manuscript_excerpt = project["manuscript_chunks"][0]["content"][:500]
                
                if research_type == "domain":
                    research_query = f"Technical information about: {task}"
                elif research_type == "cultural":
                    research_query = f"Cultural context related to: {task}"
                elif research_type == "market":
                    research_query = f"Market trends and audience preferences for: {task}"
                else:
                    research_query = f"Information about: {task}"
                
                # Add context from manuscript
                if manuscript_excerpt:
                    research_query += f" Context from manuscript: {manuscript_excerpt}"
            
            try:
                # Use Tavily for research
                search_result = self.tavily.search(query=research_query, search_depth="comprehensive")
                
                # Extract and format results
                results = []
                for result in search_result.get("results", []):
                    results.append({
                        "title": result.get("title", ""),
                        "content": result.get("content", ""),
                        "url": result.get("url", "")
                    })
                
                # Create a summary
                summary = search_result.get("answer", "No summary available")
                
                # Format research results
                research_results = f"RESEARCH TYPE: {research_type.upper()}\n\n"
                research_results += f"QUERY: {research_query}\n\n"
                research_results += f"SUMMARY: {summary}\n\n"
                research_results += "DETAILS:\n" + "\n\n".join(
                    [f"- {r['title']}: {r['content'][:300]}..." for r in results[:3]]
                )
                
                # Update state with research results
                updated_state = state.copy()
                updated_state["current_input"] = current_input.copy()
                updated_state["current_input"]["research_results"] = research_results
                updated_state["count"] = state.get("count", 0) + 1
                updated_state["lnode"] = f"{research_type}_research"
                
                # Add to messages for tracking
                updated_state["messages"] = state.get("messages", []) + [
                    {"role": "system", "content": f"Conducted {research_type} research on: {research_query}"},
                    {"role": "assistant", "content": research_results}
                ]
                
                return updated_state
            
            except Exception as e:
                print(f"Error conducting research: {str(e)}")
                
                # Update state with error
                updated_state = state.copy()
                updated_state["current_input"] = current_input.copy()
                updated_state["current_input"]["research_results"] = f"Research error: {str(e)}"
                updated_state["count"] = state.get("count", 0) + 1
                updated_state["lnode"] = f"{research_type}_research"
                
                # Add to messages for tracking
                updated_state["messages"] = state.get("messages", []) + [
                    {"role": "system", "content": f"Attempted {research_type} research on: {research_query}"},
                    {"role": "assistant", "content": f"Research error: {str(e)}"}
                ]
                
                return updated_state
        
        return research_agent_function
    
    def create_agent(self, agent_name: str, project_id: str):
        """Create a function for a specific agent."""
        # First, try to load prompt from MongoDB
        agent_prompt = None
        if self.mongo_client:
            agent_prompt = self.load_prompt_from_mongodb(agent_name)
        
        # If not found in MongoDB, use default prompts
        if not agent_prompt:
            # Default agent prompts
            agent_prompts = {
                "executive_director": """You are the Executive Director responsible for overall project direction. As an Executive Director, you lead a team with the following specialists: CREATIVE TEAM: - Creative Director: Responsible for artistic vision - Structure Architect: Designs story framework - Plot Development Specialist: Creates engaging plot - World Building Expert: Designs immersive settings - Character Psychology Specialist: Develops realistic characters - Character Voice Designer: Creates unique character expressions - Character Relationship Mapper: Designs character dynamics RESEARCH TEAM: - Domain Knowledge Specialist: Ensures technical accuracy - Cultural Authenticity Expert: Ensures authentic representation - Market Alignment Director: Assesses market viability EDITORIAL TEAM: - Editorial Director: Coordinates editing process - Structural Editor: Analyzes narrative structure - Prose Enhancement Specialist: Improves writing style - Dialogue Refinement Expert: Polishes dialogue FINALIZATION TEAM: - Positioning Specialist: Develops market positioning - Title/Blurb Optimizer: Creates marketing copy - Formatting Standards Expert: Prepares technical formatting Your responsibilities include: 1. Review manuscript sections and identify specific areas needing work 2. Assign SPECIFIC tasks to appropriate team members (e.g., "Creative Director, focus on improving character development in Chapter 2") 3. Make high-level decisions about project direction 4. Track quality issues and delegate them to specialists 5. Manage progression through development phases 6. Ensure quality standards are met before advancing to next phase Always be specific about which parts of the manuscript need work when delegating tasks. IMPORTANT: You must delegate tasks to specific specialists. DO NOT handle tasks yourself or delegate to another high-level director unless absolutely necessary. When you identify an issue or area for improvement, immediately delegate it to the most appropriate specialist from the list above.""", 
                
                "creative_director": """You are the Creative Director responsible for artistic vision. You work with the Executive Director and oversee the following team members: - Structure Architect: Designs story framework - Plot Development Specialist: Creates engaging plot - World Building Expert: Designs immersive settings - Character Psychology Specialist: Develops realistic characters - Character Voice Designer: Creates unique character expressions - Character Relationship Mapper: Designs character dynamics As a Creative Director, you should: 1. Guide creative direction based on Executive Director's instructions 2. Ensure artistic cohesion across the entire manuscript 3. Identify specific creative issues in the manuscript and delegate to specialists 4. Coordinate creative elements between team members 5. Maintain creative standards 6. Balance innovation and conventions IMPORTANT: You must delegate tasks to specific specialists. DO NOT handle tasks yourself or return to the Executive Director without delegating first. When you identify an issue or area for improvement, immediately delegate it to the most appropriate specialist from your team. Only return to the Executive Director after delegating appropriately.""", 
                
                "structure_architect": """You are the Structure Architect responsible for story framework. You report to the Creative Director and specialize in narrative structure. As a Structure Architect, you should: 1. Design and refine story structure based on specific manuscript sections 2. Identify structural weaknesses in assigned chapters/sections 3. Organize story elements for maximum impact 4. Ensure structural integrity across the narrative 5. Balance pacing and flow in specific sections 6. Provide concrete suggestions for structural improvements Focus on the specific manuscript sections you are assigned to analyze, and provide actionable recommendations.""",
                "plot_development_specialist": """You are the Plot Development Specialist responsible for plot design.
                You report to the Creative Director and specialize in plot construction. As a Plot Development Specialist, you should: 1. Design engaging plot structures for specific assigned sections 2. Create compelling story arcs when requested 3. Develop effective plot points for particular chapters 4. Balance pacing and tension in identified manuscript sections 5. Ensure plot coherence across assigned sections 6. Provide specific plot improvements for assigned sections Focus on the exact manuscript sections you are assigned to review, and provide actionable recommendations.""", 
                
                "world_building_expert": """You are the World Building Expert responsible for creating rich, believable settings.
                You report to the Creative Director and specialize in world creation. As a World Building Expert, you should: 1. Design coherent and immersive world systems for assigned sections 2. Develop cultural, social, and physical environments in specific chapters 3. Create consistent rules and logic for the story world 4. Ensure authenticity in setting details 5. Balance world complexity with story needs 6. Integrate setting elements naturally into identified manuscript sections Focus on the specific manuscript sections you are assigned to analyze, and provide actionable recommendations.""", 
                
                "character_psychology_specialist": """You are the Character Psychology Specialist responsible for deep character development.
                You report to the Creative Director and specialize in character psychology. As a Character Psychology Specialist, you should: 1. Design psychologically realistic characters in assigned manuscript sections 2. Develop complex motivations and internal conflicts for specific characters 3. Create believable character growth arcs 4. Ensure consistent character behaviors in the sections you review 5. Design meaningful character transformations 6. Balance internal and external conflicts Focus on the specific characters and manuscript sections you are assigned, and provide actionable recommendations.""", 
                
                "character_voice_designer": """You are the Character Voice Designer responsible for unique character expressions.
                You report to the Creative Director and specialize in character dialogue and voice. As a Character Voice Designer, you should: 1. Create distinct voices for each character in assigned manuscript sections 2. Develop consistent speech patterns for specific characters 3. Reflect character backgrounds in dialogue 4. Ensure authentic character expressions in the sections you review 5. Balance dialogue style with readability 6. Maintain voice consistency across assigned chapters Focus on the specific characters and manuscript sections you are assigned, and provide actionable recommendations.""", 
                
                "character_relationship_mapper": """You are the Character Relationship Mapper responsible for character dynamics.
                You report to the Creative Director and specialize in character relationships. As a Character Relationship Mapper, you should: 1. Design complex character relationships in assigned manuscript sections 2. Map character interactions and dynamics 3. Develop relationship arcs for specific character pairs 4. Ensure realistic relationship evolution 5. Create meaningful connections in the sections you review 6. Balance relationship complexity Focus on the specific characters and manuscript sections you are assigned, and provide actionable recommendations.""", 
                
                "domain_knowledge_specialist": """You are the Domain Knowledge Specialist responsible for technical accuracy.
                You report to the Executive Director and specialize in research and fact-checking. As a Domain Knowledge Specialist, you should: 1. Verify technical and specialized content in assigned manuscript sections 2. Research domain-specific details when requested 3. Ensure accurate representation of professional fields 4. Provide expert knowledge integration 5. Balance accuracy with readability 6. Maintain consistency in technical elements across assigned sections When you receive a research request, conduct the necessary research and provide factual context to the requesting specialist. Focus on the specific manuscript sections you are assigned, and provide actionable recommendations.""", 
                
                "cultural_authenticity_expert": """You are the Cultural Authenticity Expert responsible for cultural representation.
                You report to the Executive Director and specialize in cultural accuracy. As a Cultural Authenticity Expert, you should: 1. Ensure authentic cultural representation in assigned manuscript sections 2. Verify cultural details and practices 3. Prevent stereotypes and misrepresentation 4. Provide cultural context and nuance 5. Balance authenticity with accessibility 6. Maintain cultural sensitivity in all content you review When you receive a research request, conduct the necessary cultural research and provide context to the requesting specialist. Focus on the specific manuscript sections you are assigned, and provide actionable recommendations.""", 
                
                "market_alignment_director": """You are the Market Alignment Director responsible for market viability.
                You report directly to the Executive Director and specialize in commercial aspects. As a Market Alignment Director, you should: 1. Analyze current market trends and audience preferences for specific genres/categories 2. Evaluate commercial potential of assigned manuscript sections 3. Guide market positioning strategy 4. Ensure alignment with target audience expectations 5. Balance artistic vision with market demands 6. Provide strategic recommendations for specific improvements When you receive a research request, conduct market research and provide context to the requesting specialist. Focus on the specific manuscript sections you are assigned, and provide actionable recommendations.""", 
                
                "content_development_director": """You are the Content Development Director responsible for content creation.
                You report to the Executive Director and oversee the following team members: - Chapter Drafters: Create chapter content - Scene Construction Specialists: Build effective scenes - Dialogue Crafters: Write natural dialogue - Continuity Manager: Ensure consistency - Voice Consistency Monitor: Maintain narrative voice - Emotional Arc Designer: Create emotional journeys As a Content Development Director, you should: 1. Guide overall content development based on Executive Director's instructions 2. Ensure consistency in content creation across assigned manuscript sections 3. Delegate specific content tasks to your team members 4. Maintain quality standards in all content 5. Balance creativity with structure 6. Guide content integration IMPORTANT: You must delegate tasks to specific specialists. DO NOT handle tasks yourself or return to the Executive Director without delegating first. When you identify an issue or area for improvement, immediately delegate it to the most appropriate specialist from your team. Only return to the Executive Director after delegating appropriately.""", 
                
                "chapter_drafters": """You are the Chapter Drafter responsible for chapter creation.
                You report to the Content Development Director and specialize in chapter-level content. As a Chapter Drafter, you should: 1. Create engaging chapter content for assigned sections 2. Structure chapter flow and pacing 3. Develop chapter hooks and endings 4. Ensure chapter coherence 5. Balance chapter length and content 6. Maintain narrative momentum 

            IMPORTANT: When given a manuscript section to improve, you must REWRITE that section to significantly enhance its quality. Don't just give advice or suggestions - actually provide the improved text as a complete replacement. Focus on making the prose more engaging, vivid, and professional, as if transforming a draft into published-quality work. Your rewrite should maintain the story elements while dramatically improving the writing style, descriptions, pacing, and overall readability.

            Focus on the specific manuscript sections you are assigned to develop, and provide actionable content.""", 
                
                "scene_construction_specialists": """You are the Scene Construction Specialist responsible for scene creation.
                You report to the Content Development Director and specialize in scene development. As a Scene Construction Specialist, you should: 1. Craft engaging scenes in assigned manuscript sections 2. Build proper scene structure for specific story moments 3. Create effective scene transitions 4. Balance scene elements (description, action, dialogue) 5. Maintain scene tension 6. Ensure each scene has a clear purpose 
                
            IMPORTANT: When given a manuscript section to improve, you must REWRITE that section to create a more effectively structured scene. Don't just give advice or suggestions - actually provide the improved scene as a complete replacement. Focus on enhancing sensory details, pacing, emotional impact, and dramatic structure while maintaining the core story elements.

            Focus on the specific manuscript sections you are assigned to develop, and provide actionable content.""", 
                
                "dialogue_crafters": """You are the Dialogue Crafter responsible for conversation creation.
                You report to the Content Development Director and specialize in dialogue. As a Dialogue Crafter, you should: 1. Write natural dialogue for assigned manuscript sections 2. Create meaningful conversations between specific characters 3. Balance dialogue with action 4. Maintain character voices as defined by the Character Voice Designer 5. Use dialogue for story progression 6. Ensure dialogue authenticity 
                
            IMPORTANT: When given a manuscript section to improve, you must REWRITE the dialogue to make it significantly more natural, distinctive, and engaging. Don't just give advice or suggestions - actually provide the improved dialogue as a complete replacement. Focus on creating character-specific speech patterns, adding subtext, removing exposition from dialogue, and making conversations flow more naturally while still advancing the story.

            Focus on the specific manuscript sections and characters you are assigned, and provide actionable content.""", 
                
                "continuity_manager": """You are the Continuity Manager responsible for story consistency.
                You report to the Content Development Director and specialize in narrative continuity. As a Continuity Manager, you should: 1. Track plot and character continuity across assigned manuscript sections 2. Maintain world consistency between chapters 3. Verify timeline accuracy 4. Check detail consistency (names, places, objects, etc.) 5. Identify specific continuity issues in assigned sections 6. Ensure logical progression Focus on the specific manuscript sections you are assigned to review, and provide actionable recommendations.""", 
                
                "voice_consistency_monitor": """You are the Voice Consistency Monitor responsible for narrative voice.
                You report to the Content Development Director and specialize in voice consistency. As a Voice Consistency Monitor, you should: 1. Maintain consistent narrative voice across assigned manuscript sections 2. Track character voice consistency between chapters 3. Ensure tonal consistency in specific sections 4. Monitor style consistency 5. Balance voice variations 6. Guide voice development Focus on the specific manuscript sections you are assigned to review, and provide actionable recommendations.""", 
                
                "emotional_arc_designer": """You are the Emotional Arc Designer responsible for emotional journeys.
                You report to the Content Development Director and specialize in emotional storytelling. As an Emotional Arc Designer, you should: 1. Create compelling emotional arcs in assigned manuscript sections 2. Design character emotional growth for specific moments 3. Develop emotional resonance in key scenes 4. Balance emotional intensity across chapters 5. Ensure emotional authenticity 6. Guide emotional progression Focus on the specific manuscript sections and characters you are assigned, and provide actionable recommendations.""", 
                
                "editorial_director": """You are the Editorial Director responsible for overall editing.
                You report to the Executive Director and oversee the following team members: - Structural Editor: Analyzes narrative structure - Character Arc Evaluator: Assesses character development - Thematic Coherence Analyst: Ensures theme consistency - Prose Enhancement Specialist: Improves writing style - Dialogue Refinement Expert: Polishes dialogue - Rhythm Cadence Optimizer: Ensures prose flow - Grammar Consistency Checker: Maintains technical accuracy - Fact Verification Specialist: Ensures factual accuracy As an Editorial Director, you should: 1. Guide editorial strategy based on Executive Director's instructions 2. Coordinate editing processes for assigned manuscript sections 3. Delegate specific editing tasks to your team 4. Maintain quality standards in all edits 5. Balance different edit types 6. Guide revision process IMPORTANT: You must delegate tasks to specific specialists. DO NOT handle tasks yourself or return to the Executive Director without delegating first. When you identify an issue or area for improvement, immediately delegate it to the most appropriate specialist from your team. Only return to the Executive Director after delegating appropriately.""", 
                
                "structural_editor": """You are the Structural Editor responsible for story structure.
                You report to the Editorial Director and specialize in narrative structure. As a Structural Editor, you should: 1. Analyze overall structure of assigned manuscript sections 2. Identify structural issues in specific chapters 3. Suggest structural improvements for particular sections 4. Balance story elements 5. Ensure logical flow 6. Guide structural revisions Focus on the specific manuscript sections you are assigned to review, and provide actionable recommendations.""", 
                
                "character_arc_evaluator": """You are the Character Arc Evaluator responsible for character development.
                You report to the Editorial Director and specialize in character arcs. As a Character Arc Evaluator, you should: 1. Assess character arcs in assigned manuscript sections 2. Evaluate character growth for specific characters 3. Identify character inconsistencies in particular chapters 4. Suggest character improvements 5. Ensure arc completion 6. Guide character revisions Focus on the specific characters and manuscript sections you are assigned, and provide actionable recommendations.""", 
                
                "thematic_coherence_analyst": """You are the Thematic Coherence Analyst responsible for theme development.
                You report to the Editorial Director and specialize in thematic analysis. As a Thematic Coherence Analyst, you should: 1. Analyze thematic elements in assigned manuscript sections 2. Ensure theme consistency across chapters 3. Strengthen theme development in specific sections 4. Balance theme presentation 5. Identify theme opportunities 6. Guide theme integration Focus on the specific manuscript sections you are assigned to review, and provide actionable recommendations.""", 
                
                "prose_enhancement_specialist": """You are the Prose Enhancement Specialist responsible for writing quality.
                You report to the Editorial Director and specialize in prose improvement. As a Prose Enhancement Specialist, you should: 1. Improve writing style in assigned manuscript sections 2. Enhance prose quality in specific paragraphs 3. Strengthen descriptions in particular scenes 4. Polish language use 5. Balance prose elements 6. Guide prose refinement 
                
            IMPORTANT: When given a manuscript section to improve, you must REWRITE that section to significantly enhance the prose quality. Don't just give advice or suggestions - actually provide the improved text as a complete replacement. Focus on making the language more elegant, precise, and evocative. Enhance sensory details, vary sentence structure, eliminate clichs, strengthen imagery, and elevate the overall writing quality to professional publishing standards.

            Focus on the specific manuscript sections you are assigned to enhance, and provide actionable improvements.""", 
                
                "dialogue_refinement_expert": """You are the Dialogue Refinement Expert responsible for dialogue quality.
                You report to the Editorial Director and specialize in dialogue improvement. As a Dialogue Refinement Expert, you should: 1. Polish dialogue in assigned manuscript sections 2. Improve conversation flow between specific characters 3. Enhance character voices in particular scenes 4. Strengthen dialogue impact 5. Balance dialogue elements 6. Guide dialogue revision 
                
            IMPORTANT: When given a manuscript section to improve, you must REWRITE the dialogue to significantly enhance its quality. Don't just give advice or suggestions - actually provide the improved dialogue as a complete replacement. Focus on eliminating awkward phrasing, strengthening character voice, adding subtext, removing unnecessary tags or adverbs, and ensuring each line serves multiple purposes (characterization, plot advancement, etc.).

            Focus on the specific manuscript sections and character conversations you are assigned, and provide actionable improvements.""", 
                
                "rhythm_cadence_optimizer": """You are the Rhythm Cadence Optimizer responsible for prose flow.
                You report to the Editorial Director and specialize in prose rhythm. As a Rhythm Cadence Optimizer, you should: 1. Analyze prose rhythm in assigned manuscript sections 2. Improve sentence flow in specific paragraphs 3. Balance pacing through sentence structure 4. Enhance readability 5. Optimize word patterns 6. Guide rhythm refinement Focus on the specific manuscript sections you are assigned to optimize, and provide actionable improvements.""", 
                
                "grammar_consistency_checker": """You are the Grammar Consistency Checker responsible for technical accuracy.
                You report to the Editorial Director and specialize in grammar and consistency. As a Grammar Consistency Checker, you should: 1. Verify grammar rules in assigned manuscript sections 2. Ensure consistent usage of style elements 3. Check punctuation in specific passages 4. Maintain style guidelines 5. Identify pattern issues 6. Guide technical corrections Focus on the specific manuscript sections you are assigned to check, and provide actionable corrections.""", 
                
                "fact_verification_specialist": """You are the Fact Verification Specialist responsible for accuracy.
                You report to the Editorial Director and specialize in fact-checking. As a Fact Verification Specialist, you should: 1. Verify factual content in assigned manuscript sections 2. Check research accuracy for specific topics 3. Validate references in particular passages 4. Ensure detail accuracy 5. Identify fact issues 6. Guide fact correction When you identify facts that need research, coordinate with the Domain Knowledge Specialist. Focus on the specific manuscript sections you are assigned, and provide actionable corrections.""", 
                
                "positioning_specialist": """You are the Positioning Specialist responsible for market positioning.
                You report to the Market Alignment Director and specialize in market positioning. As a Positioning Specialist, you should: 1. Analyze market position of the project based on manuscript content 2. Identify target audience for specific content elements 3. Develop positioning strategy 4. Guide marketing approach 5. Ensure market fit 6. Balance unique elements with market expectations Focus on the specific manuscript sections you are assigned to analyze, and provide actionable recommendations.""", 
                
                "title_blurb_optimizer": """You are the Title/Blurb Optimizer responsible for marketing copy.
                You report to the Market Alignment Director and specialize in promotional text. As a Title/Blurb Optimizer, you should: 1. Optimize title appeal based on manuscript content 2. Craft compelling blurbs that accurately represent the story 3. Create marketing hooks 4. Ensure accurate representation of the manuscript in marketing materials 5. Balance appeal with authenticity 6. Guide marketing language development Focus on the specific sections or elements you are assigned to optimize, and provide actionable copy.""", 
                
                "differentiation_strategist": """You are the Differentiation Strategist responsible for unique positioning.
                You report to the Market Alignment Director and specialize in market differentiation. As a Differentiation Strategist, you should: 1. Identify unique elements in the manuscript 2. Develop differentiation strategy based on content strengths 3. Guide unique positioning 4. Balance uniqueness with market expectations 5. Ensure market distinction 6. Guide positioning approach Focus on the specific manuscript elements you are assigned to analyze, and provide actionable recommendations.""", 
                
                "formatting_standards_expert": """You are the Formatting Standards Expert responsible for technical preparation.
                You report to the Editorial Director and specialize in formatting. As a Formatting Standards Expert, you should: 1. Verify formatting standards in assigned manuscript sections 2. Ensure technical compliance with publishing requirements 3. Check layout elements 4. Maintain consistency in formatting 5. Guide technical preparation 6. Balance format requirements with readability Focus on the specific manuscript sections you are assigned to format, and provide actionable recommendations."""
            }
            
            if agent_name in agent_prompts:
                agent_prompt = agent_prompts[agent_name]
            else:
                raise ValueError(f"Unknown agent: {agent_name}")
        
        # Create a function that uses the agent's prompt to process inputs
        def agent_function(state: AgentState) -> AgentState:
            """Agent function that processes the current state."""
            project = state.get("project", {})
            current_input = state.get("current_input", {})
            
            # Get relevant manuscript content
            manuscript_excerpt = ""
            section_to_review = current_input.get("section_to_review", "")
            chapter_to_review = current_input.get("chapter_to_review", "")
            referenced_chunks = current_input.get("referenced_chunks", [])
            
            if project.get("manuscript_chunks"):
                # If we have a specific section or chapter to review, try to find it
                if section_to_review or chapter_to_review:
                    search_term = section_to_review or chapter_to_review
                    # Convert to lowercase for case-insensitive search
                    search_term_lower = search_term.lower()
                    
                    # Search through chunks for the specified section
                    for chunk in project["manuscript_chunks"]:
                        if search_term_lower in chunk["content"].lower():
                            manuscript_excerpt = chunk["content"]
                            break
                    
                    # If nothing was found but we're the executive director, include more context
                    if not manuscript_excerpt and agent_name == "executive_director":
                        # For executive director, provide a larger portion of the manuscript
                        manuscript_excerpt = "\n\n".join([
                            f"Chunk {chunk['chunk_id']}: {chunk['content']}"
                            for chunk in project["manuscript_chunks"][:10]  # Increased from 3 to 10 chunks
                        ])
                # If specific chunks were referenced in the delegation
                elif referenced_chunks:
                    manuscript_excerpt = "\n\n".join([
                        f"Chunk {chunk['chunk_id']}: {chunk['content']}"
                        for chunk in project["manuscript_chunks"]
                        if chunk["chunk_id"] in referenced_chunks
                    ])
                else:
                    # For executive director, provide comprehensive manuscript view
                    if agent_name == "executive_director":
                        # Provide a larger portion of the manuscript with chunk identifiers
                        manuscript_excerpt = "\n\n".join([
                            f"Chunk {chunk['chunk_id']}: {chunk['content']}"
                            for chunk in project["manuscript_chunks"][:10]  # Increased from 3 to 10 chunks
                        ])
                    # For other directors, provide moderate context
                    elif agent_name in ["creative_director", "editorial_director", "content_development_director", "market_alignment_director"]:
                        manuscript_excerpt = "\n\n".join([
                            f"Chunk {chunk['chunk_id']}: {chunk['content']}"
                            for chunk in project["manuscript_chunks"][:5]  # Moderate amount for other directors
                        ])
                    else:
                        # For specialists, just the first chunk as a sample if no specific chunks are referenced
                        manuscript_excerpt = f"Chunk 0: {project['manuscript_chunks'][0]['content']}"
            
            # Create the context string
            context = (
                f"Project ID: {project.get('id', project_id)}\n"
                f"Title: {project.get('title', 'Untitled')}\n"
                f"Synopsis: {project.get('synopsis', 'No synopsis provided')}\n"
                f"Current task: {current_input.get('task', 'No task specified')}\n"
            )
            
            # Add research context if available
            research_context = ""
            if "research_results" in current_input:
                research_context = f"\nResearch Results:\n{current_input['research_results']}\n"
            
            # Make sure we have valid content
            system_content = agent_prompt.strip()
            if not system_content:
                system_content = "You are an AI assistant helping with a writing project."
            
            # For executive director, add specific instruction to provide detailed feedback
            if agent_name == "executive_director":
                system_content += "\n\nIMPORTANT ADDITIONAL INSTRUCTIONS: You must thoroughly analyze the manuscript chunks provided below. For each chunk, identify specific issues, strengths, and provide detailed feedback. Then delegate SPECIFIC tasks to appropriate specialists. Always reference the exact chunks (e.g., 'Chunk 3 needs work on character development') when delegating tasks."
            
            # For content creators (chapter drafters, dialogue crafters, prose specialists), emphasize rewriting
            if agent_name in ["chapter_drafters", "dialogue_crafters", "prose_enhancement_specialist", "scene_construction_specialists", "dialogue_refinement_expert"]:
                system_content += "\n\nIMPORTANT ADDITIONAL INSTRUCTIONS: Your primary job is to REWRITE and IMPROVE the manuscript sections you're given. Don't just give feedback or suggestions - actually provide completely rewritten text that significantly improves on the original. Focus on transforming the text from draft quality to professional, published-quality writing."
            
            # For specialized agents, add context about previous director instructions
            if agent_name not in ["executive_director", "creative_director", "editorial_director", "content_development_director", "market_alignment_director"]:
                # Find the last message from a director
                director_instructions = []
                for msg in reversed(state.get("messages", [])):
                    if msg.get("role") == "assistant" and any(director in msg.get("content", "") for director in ["Executive Director", "Creative Director", "Editorial Director", "Content Development Director", "Market Alignment Director"]):
                        director_instructions.append(msg.get("content", ""))
                        # Only get the most recent director message
                        break
                
                # Add the director's instructions to the system prompt
                if director_instructions:
                    system_content += "\n\nRecent director instructions:\n" + "\n".join(director_instructions)
            
            task = current_input.get('task', 'No task specified')
            human_content = (
                f"Task: {task}\n\n"
                f"Context:\n{context}\n"
                f"{research_context}"
                f"\nManuscript section to review:\n{manuscript_excerpt[:5000] if manuscript_excerpt else 'No specific manuscript section provided'}"
            ).strip()
            if not human_content:
                human_content = "Please help with this writing project."
            
            # Combine system and human content into a single message
            combined_content = f"{system_content}\n\n{human_content}"
            
            # Create a single HumanMessage with the combined content
            user_message = HumanMessage(content=combined_content)
            
            # Setup LangSmith trace
            config = RunnableConfig()
            if langchain_tracer:
                config["callbacks"] = [langchain_tracer]
                config["tags"] = [f"agent:{agent_name}", f"project:{project_id}", f"phase:{state.get('phase', 'unknown')}"]
                # Set up metadata for the trace
                config["metadata"] = {
                    "agent_name": agent_name,
                    "project_id": project_id,
                    "phase": state.get("phase", "unknown"),
                    "task": task
                }
            
            # Create a streaming queue for real-time token updates
            stream_queue = queue.Queue(maxsize=1000)
            
            # Custom streaming callback to capture tokens in real-time with improved context
            class StreamingTokenCallback(StreamingStdOutCallbackHandler):
                def __init__(self):
                    super().__init__()
                    self.current_agent = agent_name
                    self.token_buffer = ""
                    self.last_token_time = time.time()
                
                def on_llm_start(self, serialized, prompts, **kwargs):
                    self.token_buffer = ""
                    self.last_token_time = time.time()
                    print(f"[Agent: {self.current_agent}] Starting processing...")
                
                def on_llm_new_token(self, token: str, **kwargs):
                    try:
                        # Add token to buffer and queue
                        self.token_buffer += token
                        if not stream_queue.full():
                            stream_queue.put(token)
                        
                        # If we have a period/question mark/exclamation, or it's been more than 1 second,
                        # or buffer is getting large, log what we have
                        current_time = time.time()
                        if ('.' in token or '?' in token or '!' in token or 
                                current_time - self.last_token_time > 1.0 or 
                                len(self.token_buffer) > 80):
                            
                            # Print with agent context
                            print(f"[Agent: {self.current_agent}] {self.token_buffer.strip()}")
                            
                            # Reset buffer and time
                            self.token_buffer = ""
                            self.last_token_time = current_time
                            
                    except Exception as e:
                        print(f"Error in streaming callback: {e}")
                
                def on_llm_end(self, response, **kwargs):
                    # Print any remaining tokens in buffer
                    if self.token_buffer:
                        print(f"[Agent: {self.current_agent}] {self.token_buffer.strip()}")
                        self.token_buffer = ""
                        
                    print(f"[Agent: {self.current_agent}] Processing completed")
            
            # Add streaming callback to config
            streaming_handler = StreamingTokenCallback()
            if "callbacks" in config:
                config["callbacks"].append(streaming_handler)
            else:
                config["callbacks"] = [streaming_handler]
            
            # Define a helper function for model invocation with retry logic
            @retry_with_exponential_backoff(max_retries=2, initial_delay=1)
            def invoke_model_with_retry(model, messages, config):
                """Invoke model with retry and explicit CPU fallback"""
                try:
                    return model.invoke(messages, config=config)
                except Exception as e:
                    if "device" in str(e).lower() or "cuda" in str(e).lower() or "tensor" in str(e).lower():
                        print(f"Device-related error: {e}. Trying with CPU fallback.")
                        # Force CPU operation for this invocation
                        if hasattr(torch, "device"):
                            with torch.device("cpu"):
                                return model.invoke(messages, config=config)
                    raise  # Re-raise if not a device error or if CPU fallback failed
            
            # Add error handling around model invocation
            try:
                # Get the model (lazy loading) - use agent-specific model if configured
                model = self.get_model(agent_name)
                
                # Debug message for monitoring
                print(f"Invoking model for agent: {agent_name}")
                
                # Clear CUDA cache before invocation to reduce memory fragmentation
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # Try with simplified inputs if we encounter device issues
                response = None
                try:
                    # First try with retry mechanism
                    try:
                        response = invoke_model_with_retry(model, [user_message], config)
                        print(f"Model invocation completed for agent: {agent_name}")
                    except Exception as retry_error:
                        if "device" in str(retry_error).lower() or "tensor" in str(retry_error).lower():
                            # If we're having device-related issues, try a more direct approach with CPU
                            print(f"Trying direct CPU approach after device error: {str(retry_error)}")
                            with torch.device("cpu"):
                                # Create a simpler message to avoid complex tensor operations
                                simplified_message = HumanMessage(content=f"You are {agent_name}. Respond to: {task}")
                                response = model.invoke([simplified_message], config=config)
                        else:
                            raise retry_error
                        
                except Exception as final_error:
                    # If even that fails, create a failure message
                    error_content = f"Error invoking model for {agent_name}: {str(final_error)}"
                    print(error_content)
                    response = AIMessage(content=error_content)
                
            except Exception as e:
                error_msg = f"Error invoking model for {agent_name}: {str(e)}"
                print(error_msg)
                traceback.print_exc()
                
                # Create a meaningful error response
                response = AIMessage(content=f"Error: {error_msg}\n\nI encountered a technical issue while processing your request. Please try again with a simpler query, or consider switching to a different model.")
            
            # Collect and format streamed tokens
            streamed_content = ""
            while not stream_queue.empty():
                try:
                    token = stream_queue.get(block=False)
                    streamed_content += token
                except queue.Empty:
                    break
            
            # Update state
            updated_state = state.copy()
            updated_state["messages"] = state.get("messages", []) + [
                {"role": "user", "content": combined_content},
                {"role": "assistant", "content": response.content if hasattr(response, 'content') else str(response)}
            ]
            updated_state["count"] = state.get("count", 0) + 1
            updated_state["lnode"] = agent_name
            
            # Store streaming content for display in UI
            if "stream_content" not in updated_state:
                updated_state["stream_content"] = []
            if streamed_content:
                updated_state["stream_content"].append({
                    "agent": agent_name,
                    "content": streamed_content
                })
            
            # Clean up memory after agent processing
            cleanup_memory()
            
            return updated_state
        
        return agent_function

def create_phase_graph(phase: str, project_id: str, config: Dict[str, Any], agent_factory=None) -> StateGraph:
    """Create a graph for a specific phase of the storybook process."""
    # Use provided agent_factory or create a new one
    if agent_factory is None:
        agent_factory = AgentFactory(config)
    
    # Create the graph builder without checkpoint config initially
    builder = StateGraph(AgentState)
    
    # Get MongoDB connection string from environment
    mongo_uri = os.getenv("MONGODB_URI")
    if not mongo_uri:
        print("Warning: MONGODB_URI not found in environment variables. Checkpointing will be disabled.")
        
    # Define available agents for each phase
    agents = {
        "initialization": [
            "executive_director",
            "creative_director",
            "market_alignment_director",
            "domain_knowledge_specialist",  # Add these specialists
            "cultural_authenticity_expert", # to initialization phase
            "positioning_specialist",
            "title_blurb_optimizer",
            "differentiation_strategist"
        ],
        "development": [
            "executive_director", "creative_director", "structure_architect",
            "plot_development_specialist", "world_building_expert",
            "character_psychology_specialist", "character_voice_designer",
            "character_relationship_mapper", "domain_knowledge_specialist",
            "cultural_authenticity_expert", "market_alignment_director"
        ],
        "creation": [
            "executive_director", "creative_director", "content_development_director",
            "chapter_drafters", "scene_construction_specialists", "dialogue_crafters",
            "continuity_manager", "voice_consistency_monitor", "emotional_arc_designer",
            "domain_knowledge_specialist"
        ],
        "refinement": [
            "executive_director", "creative_director", "editorial_director",
            "structural_editor", "character_arc_evaluator", "thematic_coherence_analyst",
            "prose_enhancement_specialist", "dialogue_refinement_expert",
            "rhythm_cadence_optimizer", "grammar_consistency_checker",
            "fact_verification_specialist", "market_alignment_director"
        ],
        "finalization": [
            "executive_director", "editorial_director", "market_alignment_director",
            "positioning_specialist", "title_blurb_optimizer",
            "differentiation_strategist", "formatting_standards_expert"
        ]
    }
    
    # Add agent nodes to the graph for this phase
    phase_agents = agents.get(phase, [])
    for agent_name in phase_agents:
        builder.add_node(agent_name, agent_factory.create_agent(agent_name, project_id))
    
    # Add research nodes
    builder.add_node("domain_research", agent_factory.create_research_agent("domain"))
    builder.add_node("cultural_research", agent_factory.create_research_agent("cultural"))
    builder.add_node("market_research", agent_factory.create_research_agent("market"))
    
    # Add the starting node - all phases start with executive director
    builder.set_entry_point("executive_director")
    
    # Define phase-specific routing
    if phase == "initialization":
        def route_after_executive_director(state: AgentState) -> str:
            """Route after the executive director node in initialization phase."""
            task = state.get("current_input", {}).get("task", "").lower()
            messages = state.get("messages", [])
            
            # Get the last assistant message if there is one
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if executive director specified research needs or delegated to specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the executive director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection for all specialists
                specialist_mappings = {
                    # Creative team
                    "creative director": "creative_director",
                    "structure architect": "structure_architect",
                    "plot development specialist": "plot_development_specialist",
                    "world building expert": "world_building_expert",
                    "character psychology specialist": "character_psychology_specialist",
                    "character voice designer": "character_voice_designer",
                    "character relationship mapper": "character_relationship_mapper",
                    
                    # Research team
                    "domain knowledge specialist": "domain_knowledge_specialist",
                    "cultural authenticity expert": "cultural_authenticity_expert",
                    
                    # Editorial team
                    "editorial director": "editorial_director",
                    
                    # Market team
                    "market alignment director": "market_alignment_director",
                    
                    # Other specialists
                    "positioning specialist": "positioning_specialist",
                    "title/blurb optimizer": "title_blurb_optimizer",
                    "differentiation strategist": "differentiation_strategist"
                }
                
                # Check for specialist delegations
                for specialist_name, node_name in specialist_mappings.items():
                    if specialist_name in last_message_lower and node_name in phase_agents:
                        print(f"Detected delegation to {specialist_name} in executive director's message")
                        return node_name
                
                # Check for research instructions more broadly
                if ("research" in last_message_lower or "investigate" in last_message_lower):
                    if "market" in last_message_lower or "audience" in last_message_lower:
                        return "market_research"
                    elif "cultural" in last_message_lower or "heritage" in last_message_lower:
                        return "cultural_research"
                    else:
                        return "domain_research"
            
            # Count agent visits to prevent loops
            exec_visits = sum(1 for msg in messages if 
                             msg.get("role") == "user" and
                             "Executive Director" in msg.get("content", ""))
            
            # Force termination after too many visits to prevent infinite loops
            if exec_visits > 10:  # Increased from 5 to 10
                print("Forcing termination after 10 executive director visits to prevent infinite loops")
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                quality_assessment["planning_quality"] = 0.8
                quality_assessment["market_alignment"] = 0.7
                project["quality_assessment"] = quality_assessment
                
                return END
            
            # Default routing based on task keywords if no delegation was detected
            if "creative" in task or "story" in task:
                return "creative_director"
            elif "market" in task or "audience" in task:
                return "market_alignment_director"
            elif "research" in task or "information" in task:
                return "domain_research"
            elif exec_visits > 5:  # If we've visited executive_director too many times
                # Check quality gate to possibly end this phase
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                gate_result = check_quality_gate(
                    "initialization_to_development",
                    quality_assessment,
                    {"quality_gates": config.get("quality_gates", {})}
                )
                
                if gate_result["passed"]:
                    return END
                else:
                    # If we've been to the executive director multiple times without delegation, try creative
                    if exec_visits > 2:
                        return "creative_director"
                    else:
                        # When in doubt, stick with executive director for another iteration
                        return "market_alignment_director"

        def route_after_creative_director(state: AgentState) -> str:
            """Route after the creative director in initialization phase."""
            messages = state.get("messages", [])
            
            # Get the last assistant message if there is one
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if creative director referred to specific needs or specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the creative director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection - look for specialist references
                if "structure architect" in last_message_lower:
                    return "structure_architect"
                
                if "plot development specialist" in last_message_lower:
                    return "plot_development_specialist"
                
                if "world building expert" in last_message_lower:
                    return "world_building_expert"
                
                if "character psychology specialist" in last_message_lower:
                    return "character_psychology_specialist"
                
                if "character voice designer" in last_message_lower:
                    return "character_voice_designer"
                
                if "character relationship mapper" in last_message_lower:
                    return "character_relationship_mapper"
                
                # Check for market or research needs
                if "market" in last_message_lower or "audience" in last_message_lower:
                    return "market_alignment_director"
                
                if "research" in last_message_lower or "domain knowledge" in last_message_lower:
                    return "domain_research"
            
            # Count agent visits to prevent loops
            creative_visits = sum(1 for msg in messages if
                                msg.get("role") == "user" and
                                "Creative Director" in msg.get("content", ""))
            
            # Prevent infinite loops
            if creative_visits > 5:  # Increased from 2 to 5
                print("Forcing return to executive_director after 5 creative director visits to prevent infinite loops")
                return "executive_director" 
            elif creative_visits > 2:  # If we've been to creative_director too many times
                # Go back to executive_director to reassess
                return "executive_director"
            else:
                # Stay with creative director for one more iteration to encourage delegation
                return "executive_director"
            
        def route_after_market_alignment_director(state: AgentState) -> str:
            """Route after the market alignment director in initialization phase."""
            messages = state.get("messages", [])
            
            # Check if market director wants to do research or delegates
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the market director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection
                if "positioning specialist" in last_message_lower:
                    return "positioning_specialist"
                
                if "title/blurb optimizer" in last_message_lower or "marketing copy" in last_message_lower:
                    return "title_blurb_optimizer"
                
                if "differentiation strategist" in last_message_lower:
                    return "differentiation_strategist"
                
                # Check for research needs
                if "research" in last_message_lower or "investigate" in last_message_lower:
                    return "market_research"
            
            # Count visits to market alignment director
            market_visits = sum(1 for msg in messages if
                               msg.get("role") == "user" and
                               "Market Alignment Director" in msg.get("content", ""))
            
            # Prevent infinite loops
            if market_visits > 5:  # Increased from 2 to 5
                print("Forcing quality assessment update after 5 market director visits to prevent infinite loops")
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                quality_assessment["market_alignment"] = 0.7
                project["quality_assessment"] = quality_assessment
                return "executive_director"
            elif market_visits > 2:
                # Return to executive director after a few visits
                return "executive_director"
            else:
                # Return to executive director
                return "executive_director"
            
        def route_after_research(state: AgentState) -> str:
            """Route after research nodes."""
            # Route back to the appropriate specialist based on research type
            research_type = state.get("lnode", "")
            
            if research_type == "domain_research":
                return "domain_knowledge_specialist"
            elif research_type == "cultural_research":
                return "cultural_authenticity_expert"
            elif research_type == "market_research":
                return "market_alignment_director"
            else:
                return "executive_director"
            
        # Add conditional edges
        builder.add_conditional_edges(
            "executive_director",
            route_after_executive_director
        )
        
        builder.add_conditional_edges(
            "creative_director",
            route_after_creative_director
        )
        
        builder.add_conditional_edges(
            "market_alignment_director",
            route_after_market_alignment_director
        )
        
        # Add research routing
        builder.add_conditional_edges(
            "domain_research",
            route_after_research
        )
        
        builder.add_conditional_edges(
            "cultural_research",
            route_after_research
        )
        
        builder.add_conditional_edges(
            "market_research",
            route_after_research
        )
        
        # Only add edges if both source and target nodes exist in this phase
        if "domain_knowledge_specialist" in phase_agents:
            builder.add_edge("domain_knowledge_specialist", "executive_director")
    
    elif phase == "development":
        def route_after_executive_director(state: AgentState) -> str:
            """Route after the executive director node in development phase."""
            task = state.get("current_input", {}).get("task", "").lower()
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if executive director specified different specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the executive director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                specialist_mappings = {
                    # Creative team
                    "creative director": "creative_director",
                    "structure architect": "structure_architect",
                    "plot development specialist": "plot_development_specialist",
                    "world building expert": "world_building_expert",
                    "character psychology specialist": "character_psychology_specialist",
                    "character voice designer": "character_voice_designer",
                    "character relationship mapper": "character_relationship_mapper",
                    
                    # Research team
                    "domain knowledge specialist": "domain_knowledge_specialist",
                    "cultural authenticity expert": "cultural_authenticity_expert",
                    
                    # Market team
                    "market alignment director": "market_alignment_director"
                }
                
                # Check for specialist delegations
                for specialist_name, node_name in specialist_mappings.items():
                    if specialist_name in last_message_lower and node_name in phase_agents:
                        print(f"Detected delegation to {specialist_name} in executive director's message")
                        return node_name
                
                # Check for research needs
                if "research" in last_message_lower:
                    if "market" in last_message_lower:
                        return "market_research"
                    elif "cultural" in last_message_lower:
                        return "cultural_research"
                    else:
                        return "domain_research"
            
            # Count executive director visits to prevent loops
            exec_visits = sum(1 for msg in messages if
                              msg.get("role") == "user" and
                              "Executive Director" in msg.get("content", ""))

            # Force termination after too many visits to prevent infinite loops
            if exec_visits > 10:  # Increased from 5 to 10
                print("Forcing termination after 10 executive director visits to prevent infinite loops")
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                quality_assessment["structure_quality"] = 0.8
                quality_assessment["character_depth"] = 0.8
                quality_assessment["world_building"] = 0.8
                project["quality_assessment"] = quality_assessment
                
                return END

            # Default routing based on task keywords
            if "creative" in task or "story" in task:
                return "creative_director"
            elif "market" in task or "trend" in task:
                return "market_alignment_director"
            elif "research" in task or "knowledge" in task:
                return "domain_knowledge_specialist"
            elif exec_visits > 5:  # If we've visited executive_director too many times
                # Check quality gate to possibly end this phase
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                gate_result = check_quality_gate(
                    "development_to_creation",
                    quality_assessment,
                    {"quality_gates": config.get("quality_gates", {})}
                )
                if gate_result["passed"]:
                    return END
                else:
                    # If we've been to executive several times, try creative director
                    if exec_visits > 2:
                        return "creative_director"
                    else:
                        # Try structure architect for a change to encourage more diverse delegation
                        return "structure_architect"

        def route_after_creative_director(state: AgentState) -> str:
            """Route after the creative director node."""
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Count visits to different specialists to detect routing loops
            creative_visits = sum(1 for msg in messages if
                                msg.get("role") == "user" and
                                "Creative Director" in msg.get("content", ""))
                                
            structure_visits = sum(1 for msg in messages if
                                 msg.get("role") == "user" and
                                 "Structure Architect" in msg.get("content", ""))
            
            # Detect potential loop between Creative Director and Structure Architect
            recent_pattern = []
            for i in range(min(6, len(messages))):
                if i < len(messages) and messages[-(i+1)].get("role") == "user":
                    content = messages[-(i+1)].get("content", "")
                    if "Creative Director" in content:
                        recent_pattern.append("creative_director")
                    elif "Structure Architect" in content:
                        recent_pattern.append("structure_architect")
            
            # If we're in a loop between Creative Director and Structure Architect
            if len(recent_pattern) >= 4:
                if recent_pattern[0] == "creative_director" and recent_pattern[1] == "structure_architect" and \
                   recent_pattern[2] == "creative_director" and recent_pattern[3] == "structure_architect":
                    # Break the loop by going back to executive director
                    return "executive_director"
            
            # Check if creative director delegated to a specific specialist
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the creative director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection for specialists
                specialist_mappings = {
                    "structure architect": "structure_architect",
                    "plot development specialist": "plot_development_specialist",
                    "world building expert": "world_building_expert",
                    "character psychology specialist": "character_psychology_specialist",
                    "character voice designer": "character_voice_designer",
                    "character relationship mapper": "character_relationship_mapper"
                }
                
                # If we detect a potential loop with Structure Architect, avoid that routing
                if structure_visits > 2 and "structure architect" in last_message_lower:
                    # Try other specialists instead
                    if "plot development" in last_message_lower:
                        return "plot_development_specialist"
                    elif "world building" in last_message_lower:
                        return "world_building_expert"
                    elif "character" in last_message_lower:
                        return "character_psychology_specialist"
                    else:
                        # Break the loop by going to executive director
                        return "executive_director"
                
                # Check for specialist delegations
                for specialist_name, node_name in specialist_mappings.items():
                    if specialist_name in last_message_lower and node_name in phase_agents:
                        # Before routing to a specialist, check we're not creating a loop
                        specialist_visits = sum(1 for msg in messages if
                                             msg.get("role") == "user" and
                                             specialist_name.title() in msg.get("content", ""))
                        
                        # If we've already visited this specialist many times, consider it a loop
                        if specialist_visits > 2:
                            # Try to break the loop by going to executive director
                            return "executive_director"
                        
                        print(f"Detected delegation to {specialist_name} in creative director's message")
                        return node_name
                
                # Check for research or domain knowledge needs
                if "domain knowledge" in last_message_lower or "research" in last_message_lower:
                    return "domain_knowledge_specialist"
            
            # Prevent loops with Creative Director
            if creative_visits > 5:  # Increased from 2 to 5
                print("Forcing return to executive_director after 5 creative director visits to prevent infinite loops")
                return "executive_director"
            # After a few visits, if no delegation, return to executive
            elif creative_visits > 2:
                return "executive_director"
            else:
                # Return to executive_director
                return "executive_director"

        # Add routing for specialists to prevent loops
        def route_after_structure_architect(state: AgentState) -> str:
            """Route after the structure architect node to prevent loops with Creative Director."""
            messages = state.get("messages", [])
            
            # Count visits to Structure Architect
            structure_visits = sum(1 for msg in messages if
                                  msg.get("role") == "user" and
                                  "Structure Architect" in msg.get("content", ""))
                                  
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Prevent infinite loops
            if structure_visits > 5:  # Increased from 2 to 5
                print("Forcing return to executive_director after 5 structure architect visits to prevent infinite loops")
                return "executive_director"
            # Check for potential loops
            elif structure_visits > 2:
                # If we've been to Structure Architect multiple times, go to Executive Director
                return "executive_director"
            
            # Check if the structure architect recommended other specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Enhanced delegation detection
                specialist_mappings = {
                    "plot development specialist": "plot_development_specialist",
                    "world building expert": "world_building_expert",
                    "character psychology specialist": "character_psychology_specialist",
                    "character voice designer": "character_voice_designer",
                    "character relationship mapper": "character_relationship_mapper"
                }
                
                # Check for specialist delegations
                for specialist_name, node_name in specialist_mappings.items():
                    if specialist_name in last_message_lower and node_name in phase_agents:
                        print(f"Detected delegation to {specialist_name} in structure architect's message")
                        return node_name
                
                if "executive" in last_message_lower:
                    return "executive_director"
            
            # Default to creative director (but only the first couple times)
            return "creative_director"

        # Add routing for research nodes
        def route_after_research(state: AgentState) -> str:
            """Route after research nodes."""
            research_type = state.get("lnode", "")
            
            if research_type == "domain_research":
                return "domain_knowledge_specialist"
            elif research_type == "cultural_research":
                return "cultural_authenticity_expert"
            elif research_type == "market_research":
                return "market_alignment_director"
            else:
                return "executive_director"

        builder.add_conditional_edges(
            "executive_director",
            route_after_executive_director
        )
        builder.add_conditional_edges(
            "creative_director",
            route_after_creative_director
        )
        
        # Add routing for Structure Architect to prevent loops
        builder.add_conditional_edges(
            "structure_architect",
            route_after_structure_architect
        )
        
        # Add research routing
        builder.add_conditional_edges(
            "domain_research",
            route_after_research
        )
        
        builder.add_conditional_edges(
            "cultural_research",
            route_after_research
        )
        
        builder.add_conditional_edges(
            "market_research",
            route_after_research
        )
        # Connect domain knowledge specialist to domain research
        builder.add_edge("domain_knowledge_specialist", "domain_research")
        builder.add_edge("cultural_authenticity_expert", "cultural_research")
        builder.add_edge("market_alignment_director", "market_research")
        
        # Use conditional routing for specialists rather than direct edges to prevent loops
        def route_after_specialist(state: AgentState) -> str:
            """Route after a specialist to prevent loops."""
            messages = state.get("messages", [])
            
            # Count visits to this specialist
            specialist_type = state.get("lnode", "")
            specialist_visits = sum(1 for msg in messages if
                                   msg.get("role") == "user" and
                                   specialist_type.replace("_", " ").title() in msg.get("content", ""))
                                   
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Prevent infinite loops
            if specialist_visits > 5:  # Increased from 2 to 5
                print(f"Forcing return to executive_director after 5 {specialist_type} visits to prevent infinite loops")
                return "executive_director"
            
            # If specialist has mentioned other specialists, route there
            if last_message:
                last_message_lower = last_message.lower()
                
                # Enhanced delegation detection
                specialist_mappings = {
                    "plot development specialist": "plot_development_specialist",
                    "world building expert": "world_building_expert",
                    "character psychology specialist": "character_psychology_specialist",
                    "character voice designer": "character_voice_designer",
                    "character relationship mapper": "character_relationship_mapper"
                }
                
                # Check for specialist delegations
                for specialist_name, node_name in specialist_mappings.items():
                    if (specialist_name in last_message_lower and 
                        node_name in phase_agents and 
                        node_name != specialist_type):
                        print(f"Detected delegation to {specialist_name} in {specialist_type}'s message")
                        return node_name
                        
                if "executive director" in last_message_lower:
                    return "executive_director"
            
            # Avoid loops by going to executive director after multiple visits
            if specialist_visits > 2:
                return "executive_director"
                
            # Default to creative director
            return "creative_director"
            
        # Add conditional edges for other specialists instead of direct edges
        for agent in ["plot_development_specialist", "world_building_expert",
                     "character_psychology_specialist", "character_voice_designer",
                     "character_relationship_mapper"]:
            if agent in agents[phase]:
                builder.add_conditional_edges(agent, route_after_specialist)
            
    elif phase == "creation":
        def route_after_executive_director(state: AgentState) -> str:
            """Route after the executive director node in creation phase, focused on manuscript improvement."""
            task = state.get("current_input", {}).get("task", "").lower()
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if executive director specified different specialists, prioritizing content creators
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the executive director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection for all specialists
                specialist_mappings = {
                    # Content team
                    "content development director": "content_development_director",
                    "chapter draft": "chapter_drafters",
                    "rewrite chapter": "chapter_drafters",
                    "dialogue": "dialogue_crafters",
                    "conversation": "dialogue_crafters",
                    "scene construction": "scene_construction_specialists",
                    "build scene": "scene_construction_specialists",
                    "continuity manager": "continuity_manager",
                    "voice consistency": "voice_consistency_monitor",
                    "emotional arc": "emotional_arc_designer",
                    
                    # Creative team
                    "creative director": "creative_director",
                    
                    # Research
                    "domain knowledge": "domain_knowledge_specialist",
                    "research": "domain_knowledge_specialist"
                }
                
                # Check for specialist delegations
                for keyword, node_name in specialist_mappings.items():
                    if keyword in last_message_lower and node_name in phase_agents:
                        print(f"Detected delegation to {node_name} in executive director's message")
                        return node_name
            
            # Count executive director visits to prevent loops
            exec_visits = sum(1 for msg in messages if
                              msg.get("role") == "user" and
                              "Executive Director" in msg.get("content", ""))
            
            # Force termination after too many visits to prevent infinite loops
            if exec_visits > 10:  # Increased from 5 to 10
                print("Forcing termination after 10 executive director visits to prevent infinite loops")
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                quality_assessment["content_quality"] = 0.8
                quality_assessment["narrative_flow"] = 0.8
                quality_assessment["dialogue_quality"] = 0.8
                project["quality_assessment"] = quality_assessment
                
                return END
                              
            # Default routing based on task keywords
            if "chapter" in task or "write" in task or "content" in task:
                return "chapter_drafters"
            elif "dialogue" in task or "conversation" in task:
                return "dialogue_crafters"
            elif "scene" in task:
                return "scene_construction_specialists"
            elif "content" in task or "development" in task:
                return "content_development_director"
            elif exec_visits > 5:  # If we've visited executive_director too many times
                # Check quality gate to possibly end this phase
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                gate_result = check_quality_gate(
                    "creation_to_refinement",
                    quality_assessment,
                    {"quality_gates": config.get("quality_gates", {})}
                )
                if gate_result["passed"]:
                    return END
                else:
                    # After a few iterations, try content specialists
                    if exec_visits > 1:
                        # Check which content creator has been used less
                        chapter_visits = sum(1 for msg in messages if
                                            msg.get("role") == "user" and
                                            "Chapter Drafter" in msg.get("content", ""))
                        dialogue_visits = sum(1 for msg in messages if
                                            msg.get("role") == "user" and
                                            "Dialogue Crafter" in msg.get("content", ""))
                                            
                        # Choose the specialist that's been used less
                        if chapter_visits <= dialogue_visits:
                            return "chapter_drafters"
                        else:
                            return "dialogue_crafters"
                    else:
                        # First try content development director
                        return "content_development_director"

        def route_after_content_director(state: AgentState) -> str:
            """Route after the content development director node."""
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if content director delegated to a specific specialist
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the content director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection for content team specialists
                specialist_mappings = {
                    "chapter draft": "chapter_drafters",
                    "rewrite chapter": "chapter_drafters",
                    "dialogue": "dialogue_crafters",
                    "conversation": "dialogue_crafters",
                    "scene construction": "scene_construction_specialists",
                    "build scene": "scene_construction_specialists",
                    "continuity manager": "continuity_manager",
                    "voice consistency": "voice_consistency_monitor",
                    "emotional arc": "emotional_arc_designer",
                    "domain knowledge": "domain_knowledge_specialist",
                    "research": "domain_knowledge_specialist"
                }
                
                # Check for specialist delegations
                for keyword, node_name in specialist_mappings.items():
                    if keyword in last_message_lower and node_name in phase_agents:
                        print(f"Detected delegation to {node_name} in content director's message")
                        return node_name
            
            # Count content director visits
            content_visits = sum(1 for msg in messages if
                                msg.get("role") == "user" and
                                "Content Development Director" in msg.get("content", ""))
            
            # Prevent infinite loops
            if content_visits > 5:  # Increased from 2 to 5
                print("Forcing return to executive_director after 5 content director visits to prevent infinite loops")
                return "executive_director"
            
            # Count specialist visits
            chapter_visits = sum(1 for msg in messages if
                                msg.get("role") == "user" and
                                "Chapter Drafter" in msg.get("content", ""))
            dialogue_visits = sum(1 for msg in messages if
                                msg.get("role") == "user" and
                                "Dialogue Crafter" in msg.get("content", ""))
            
            if content_visits > 2:
                # After multiple visits without delegation, try specialists directly
                # Choose the specialist that's been used less
                if chapter_visits <= dialogue_visits:
                    return "chapter_drafters"
                else:
                    return "dialogue_crafters"
            else:
                # Give another chance for delegation
                return "executive_director"

        # Add routing for research
        def route_after_research(state: AgentState) -> str:
            """Route after research nodes."""
            research_type = state.get("lnode", "")
            
            if research_type == "domain_research":
                return "domain_knowledge_specialist"
            else:
                return "executive_director"

        # Add routing for content creators to go back to executive director for review
        def route_after_content_creator(state: AgentState) -> str:
            """Route after content creators back to executive director for review."""
            # Always return to executive director for review after content creation
            return "executive_director"

        builder.add_conditional_edges(
            "executive_director",
            route_after_executive_director
        )
        builder.add_conditional_edges(
            "content_development_director",
            route_after_content_director
        )
        
        # Add research routing
        builder.add_conditional_edges(
            "domain_research",
            route_after_research
        )
        
        # Add routing for content creators
        for creator in ["chapter_drafters", "dialogue_crafters", "scene_construction_specialists"]:
            if creator in phase_agents:
                builder.add_conditional_edges(creator, route_after_content_creator)
                
        # Connect domain knowledge specialist to domain research
        builder.add_edge("domain_knowledge_specialist", "domain_research")
        
        # Connect specialized agents to their supervisors
        for agent in ["continuity_manager", "voice_consistency_monitor", "emotional_arc_designer"]:
            if agent in agents[phase]:  # Only add edges for agents that exist in this phase
                builder.add_edge(agent, "content_development_director")
                
        builder.add_edge("creative_director", "executive_director")
            
    elif phase == "refinement":
        def route_after_executive_director(state: AgentState) -> str:
            """Route after the executive director node in refinement phase."""
            task = state.get("current_input", {}).get("task", "").lower()
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if executive director specified different specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the executive director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection for all refinement specialists
                specialist_mappings = {
                    # Editorial team
                    "editorial director": "editorial_director",
                    "creative director": "creative_director",
                    "market alignment director": "market_alignment_director",
                    "structural editor": "structural_editor",
                    "character arc evaluator": "character_arc_evaluator",
                    "thematic coherence analyst": "thematic_coherence_analyst",
                    "prose enhancement": "prose_enhancement_specialist",
                    "improve writing": "prose_enhancement_specialist",
                    "dialogue refinement": "dialogue_refinement_expert",
                    "improve dialogue": "dialogue_refinement_expert",
                    "rhythm cadence optimizer": "rhythm_cadence_optimizer",
                    "grammar consistency checker": "grammar_consistency_checker",
                    "fact verification specialist": "fact_verification_specialist"
                }
                
                # Check for specialist delegations
                for keyword, node_name in specialist_mappings.items():
                    if keyword in last_message_lower and node_name in phase_agents:
                        print(f"Detected delegation to {node_name} in executive director's message")
                        return node_name
            
            # Count executive director visits to prevent loops
            exec_visits = sum(1 for msg in messages if
                              msg.get("role") == "user" and
                              "Executive Director" in msg.get("content", ""))
            
            # Force termination after too many visits to prevent infinite loops
            if exec_visits > 10:  # Increased from 5 to 10
                print("Forcing termination after 10 executive director visits to prevent infinite loops")
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                quality_assessment["editing_quality"] = 0.9
                quality_assessment["prose_quality"] = 0.9
                quality_assessment["thematic_coherence"] = 0.8
                project["quality_assessment"] = quality_assessment
                
                return END
                              
            # Default routing based on task keywords
            if "prose" in task or "writing" in task or "style" in task:
                return "prose_enhancement_specialist"
            elif "dialogue" in task or "conversation" in task:
                return "dialogue_refinement_expert"
            elif "edit" in task:
                return "editorial_director"
            elif "creative" in task:
                return "creative_director"
            elif "market" in task:
                return "market_alignment_director"
            elif exec_visits > 5:  # If we've visited executive_director too many times
                # Check quality gate to possibly end this phase
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                gate_result = check_quality_gate(
                    "refinement_to_finalization",
                    quality_assessment,
                    {"quality_gates": config.get("quality_gates", {})}
                )
                if gate_result["passed"]:
                    return END
                else:
                    # After a few iterations, focus on direct prose improvement
                    if exec_visits > 1:
                        # Check which refinement specialist has been used less
                        prose_visits = sum(1 for msg in messages if
                                          msg.get("role") == "user" and
                                          "Prose Enhancement Specialist" in msg.get("content", ""))
                        dialogue_visits = sum(1 for msg in messages if
                                            msg.get("role") == "user" and
                                            "Dialogue Refinement Expert" in msg.get("content", ""))
                                            
                        # Choose the specialist that's been used less
                        if prose_visits <= dialogue_visits:
                            return "prose_enhancement_specialist"
                        else:
                            return "dialogue_refinement_expert"
                    else:
                        # First try editorial director
                        return "editorial_director"

        def route_after_editorial_director(state: AgentState) -> str:
            """Route after the editorial director node with focus on manuscript improvement."""
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if editorial director delegated to specific specialists for manuscript improvement
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the editorial director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection for editorial team specialists
                specialist_mappings = {
                    "structural editor": "structural_editor",
                    "character arc evaluator": "character_arc_evaluator",
                    "thematic coherence analyst": "thematic_coherence_analyst",
                    "prose enhancement": "prose_enhancement_specialist",
                    "improve writing": "prose_enhancement_specialist",
                    "dialogue refinement": "dialogue_refinement_expert",
                    "improve dialogue": "dialogue_refinement_expert",
                    "rhythm cadence optimizer": "rhythm_cadence_optimizer",
                    "grammar consistency checker": "grammar_consistency_checker",
                    "fact verification specialist": "fact_verification_specialist"
                }
                
                # Check for specialist delegations
                for keyword, node_name in specialist_mappings.items():
                    if keyword in last_message_lower and node_name in phase_agents:
                        print(f"Detected delegation to {node_name} in editorial director's message")
                        return node_name
            
            # Count editorial director visits
            editorial_visits = sum(1 for msg in messages if
                                  msg.get("role") == "user" and
                                  "Editorial Director" in msg.get("content", ""))
            
            # Prevent infinite loops
            if editorial_visits > 5:  # Increased from 2 to 5
                print("Forcing return to executive_director after 5 editorial director visits to prevent infinite loops")
                return "executive_director"
                                  
            # Count specialist visits
            prose_visits = sum(1 for msg in messages if
                              msg.get("role") == "user" and
                              "Prose Enhancement Specialist" in msg.get("content", ""))
            dialogue_visits = sum(1 for msg in messages if
                                msg.get("role") == "user" and
                                "Dialogue Refinement Expert" in msg.get("content", ""))
            
            if editorial_visits > 2:
                # After multiple visits, try a specialist directly
                # Choose the specialist that's been used less
                if prose_visits <= dialogue_visits:
                    return "prose_enhancement_specialist"
                else:
                    return "dialogue_refinement_expert"
            else:
                # Give another chance for delegation
                return "executive_director"

        # Add routing for fact verification and research
        def route_after_fact_verification(state: AgentState) -> str:
            """Route after fact verification specialist."""
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if fact verification needs research
            if last_message and "research" in last_message.lower():
                return "domain_research"
            else:
                return "editorial_director"
                
        def route_after_research(state: AgentState) -> str:
            """Route after domain research."""
            return "fact_verification_specialist"
            
        # Add routing for prose and dialogue refinement specialists to go back to executive
        def route_after_refinement_specialist(state: AgentState) -> str:
            """Route after refinement specialists back to executive director for review."""
            # Always return to executive director for review after refinement
            return "executive_director"

        builder.add_conditional_edges(
            "executive_director",
            route_after_executive_director
        )
        builder.add_conditional_edges(
            "editorial_director",
            route_after_editorial_director
        )
        
        builder.add_conditional_edges(
            "fact_verification_specialist",
            route_after_fact_verification
        )
        
        builder.add_conditional_edges(
            "domain_research",
            route_after_research
        )
        
        # Add routing for refinement specialists
        for specialist in ["prose_enhancement_specialist", "dialogue_refinement_expert"]:
            if specialist in phase_agents:
                builder.add_conditional_edges(specialist, route_after_refinement_specialist)
                
        # Connect specialized agents to their supervisors
        for agent in ["structural_editor", "character_arc_evaluator",
                     "thematic_coherence_analyst", "rhythm_cadence_optimizer",
                     "grammar_consistency_checker"]:
            if agent in agents[phase]:  # Only add edges for agents that exist in this phase
                builder.add_edge(agent, "editorial_director")
                
        # Connect other directors back to executive director
        for agent in ["creative_director", "market_alignment_director"]:
            if agent in agents[phase]:
                builder.add_edge(agent, "executive_director")
            
    elif phase == "finalization":
        def route_after_executive_director(state: AgentState) -> str:
            """Route after the executive director node in finalization phase."""
            task = state.get("current_input", {}).get("task", "").lower()
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if executive director specified different specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the executive director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection for all finalization specialists
                specialist_mappings = {
                    "editorial director": "editorial_director",
                    "market alignment director": "market_alignment_director",
                    "positioning specialist": "positioning_specialist",
                    "title/blurb optimizer": "title_blurb_optimizer",
                    "marketing copy": "title_blurb_optimizer",
                    "differentiation strategist": "differentiation_strategist",
                    "formatting standards expert": "formatting_standards_expert"
                }
                
                # Check for specialist delegations
                for keyword, node_name in specialist_mappings.items():
                    if keyword in last_message_lower and node_name in phase_agents:
                        print(f"Detected delegation to {node_name} in executive director's message")
                        return node_name
            
            # Count executive director visits to prevent loops
            exec_visits = sum(1 for msg in messages if
                              msg.get("role") == "user" and
                              "Executive Director" in msg.get("content", ""))
            
            # Force termination after too many visits to prevent infinite loops
            if exec_visits > 10:  # Increased from 5 to 10
                print("Forcing termination after 10 executive director visits to prevent infinite loops")
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                quality_assessment["market_readiness"] = 0.9
                quality_assessment["overall_quality"] = 0.9
                project["quality_assessment"] = quality_assessment
                
                return END
                              
            # Default routing based on task keywords
            if "edit" in task:
                return "editorial_director"
            elif "market" in task:
                return "market_alignment_director"
            elif exec_visits > 5:  # If we've visited executive_director too many times
                # Check quality gate to possibly end this phase
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                gate_result = check_quality_gate(
                    "finalization_to_complete",
                    quality_assessment,
                    {"quality_gates": config.get("quality_gates", {})}
                )
                if gate_result["passed"]:
                    return END
                else:
                    # After a few iterations, try market alignment
                    if exec_visits > 1:
                        return "market_alignment_director"
                    else:
                        return "editorial_director"

        def route_after_market_director(state: AgentState) -> str:
            """Route after the market alignment director node."""
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if market director delegated to a specific specialist
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the market director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection for marketing specialists
                specialist_mappings = {
                    "positioning specialist": "positioning_specialist",
                    "title/blurb optimizer": "title_blurb_optimizer",
                    "marketing copy": "title_blurb_optimizer", 
                    "differentiation strategist": "differentiation_strategist"
                }
                
                # Check for specialist delegations
                for keyword, node_name in specialist_mappings.items():
                    if keyword in last_message_lower and node_name in phase_agents:
                        print(f"Detected delegation to {node_name} in market director's message")
                        return node_name
                
                # Check for research needs
                if "research" in last_message_lower:
                    return "market_research"
            
            # Count market director visits
            market_visits = sum(1 for msg in messages if
                               msg.get("role") == "user" and
                               "Market Alignment Director" in msg.get("content", ""))
            
            # Prevent infinite loops
            if market_visits > 5:  # Increased from 2 to 5
                print("Forcing return to executive_director after 5 market director visits to prevent infinite loops")
                return "executive_director"
                
            if market_visits > 2:
                # After multiple visits, try a specialist directly
                return "positioning_specialist"
            else:
                # Give another chance for delegation
                return "title_blurb_optimizer"
                
        def route_after_research(state: AgentState) -> str:
            """Route after market research."""
            return "market_alignment_director"

        builder.add_conditional_edges(
            "executive_director",
            route_after_executive_director
        )
        builder.add_conditional_edges(
            "market_alignment_director",
            route_after_market_director
        )
        
        builder.add_conditional_edges(
            "market_research",
            route_after_research
        )
        # Connect specialized agents to their supervisors
        for agent in ["positioning_specialist", "title_blurb_optimizer", "differentiation_strategist"]:
            if agent in agents[phase]:  # Only add edges for agents that exist in this phase
                builder.add_edge(agent, "market_alignment_director")
        # Connect other specialists
        if "formatting_standards_expert" in agents[phase]:
            builder.add_edge("formatting_standards_expert", "editorial_director")
            
        # Connect other directors back to executive director
        builder.add_edge("editorial_director", "executive_director")
    
    # Try to compile with the MongoDB checkpointer
    try:
        if mongo_uri:
            # Create MongoDB client for checkpointing
            print(f"Creating MongoDB checkpointer for project {project_id}, phase {phase}")
            mongo_client = MongoClient(mongo_uri)
            checkpointer = MongoDBSaver(mongo_client)
            return builder.compile(checkpointer=checkpointer)  # Removed recursion_limit parameter
        else:
            print(f"MongoDB connection string not available. Proceeding without checkpointing for {project_id}, phase {phase}")
            return builder.compile()  # Removed recursion_limit parameter
    except Exception as e:
        print(f"Warning: Failed to create MongoDB checkpointer with error: {str(e)}. Proceeding without checkpointing.")
        return builder.compile()  # Removed recursion_limit parameter

def create_main_graph(config: Dict[str, Any], agent_factory=None) -> StateGraph:
    """Create a main composite graph that includes all phase graphs."""
    # Create a graph builder
    builder = StateGraph(AgentState)
    
    # Create all phase graphs with shared agent_factory
    phase_graphs = {}
    for phase in ["initialization", "development", "creation", "refinement", "finalization"]:
        phase_graphs[phase] = create_phase_graph(phase, "composite_project", config, agent_factory)
    
    # Add phase graphs as nodes to the main graph
    for phase, graph in phase_graphs.items():
        builder.add_node(phase, graph)
    
    # Set entry point to initialization phase
    builder.set_entry_point("initialization")
    
    # Define transitions between phases
    def route_after_phase(state: AgentState) -> str:
        """Route after a phase completes."""
        current_phase = state.get("phase", "initialization")
        
        # Define the phase progression
        phase_order = ["initialization", "development", "creation", "refinement", "finalization"]
        
        # Find the current phase index
        try:
            current_index = phase_order.index(current_phase)
        except ValueError:
            # If the current phase is not in the list, start with initialization
            return "initialization"
        
        # Move to the next phase if not at the end
        if current_index < len(phase_order) - 1:
            next_phase = phase_order[current_index + 1]
            # Update the state phase
            state["phase"] = next_phase
            return next_phase
        else:
            # If we're at the final phase, we're done
            return END
    
    # Add conditional edges between phases
    for phase in ["initialization", "development", "creation", "refinement"]:
        builder.add_conditional_edges(
            phase,
            route_after_phase
        )
    
    # Add a final edge from finalization to END
    builder.add_edge("finalization", END)
    
    return builder.compile()  # Removed recursion_limit parameter

# This function creates a comprehensive graph with all agents exposed
def create_storybook_graph(config: Dict[str, Any], agent_factory=None) -> StateGraph:
    """Create a comprehensive storybook graph with all agents fully exposed."""
    builder = StateGraph(AgentState)
    
    # Create agent factory if not provided
    if agent_factory is None:
        agent_factory = AgentFactory(config)
    
    # Define all possible agents including specialists and research nodes
    all_agents = {
        # Directors
        "executive_director", "creative_director", "editorial_director",
        "content_development_director", "market_alignment_director",
        
        # Creative Team
        "structure_architect", "plot_development_specialist", "world_building_expert",
        "character_psychology_specialist", "character_voice_designer",
        "character_relationship_mapper",
        
        # Content Team
        "chapter_drafters", "scene_construction_specialists", "dialogue_crafters",
        "continuity_manager", "voice_consistency_monitor", "emotional_arc_designer",
        
        # Editorial Team
        "structural_editor", "character_arc_evaluator", "thematic_coherence_analyst",
        "prose_enhancement_specialist", "dialogue_refinement_expert",
        "rhythm_cadence_optimizer", "grammar_consistency_checker",
        "fact_verification_specialist",
        
        # Research Team
        "domain_knowledge_specialist", "cultural_authenticity_expert",
        
        # Marketing Team
        "positioning_specialist", "title_blurb_optimizer", "differentiation_strategist",
        "formatting_standards_expert",
        
        # Research Nodes
        "domain_research", "cultural_research", "market_research"
    }
    
    # Add all agents as nodes
    for agent_name in all_agents:
        if agent_name.endswith('_research'):
            research_type = agent_name.split('_')[0]
            builder.add_node(agent_name, agent_factory.create_research_agent(research_type))
        else:
            builder.add_node(agent_name, agent_factory.create_agent(agent_name, "storybook_project"))
    
    # Add the starting node - all phases start with executive director
    builder.set_entry_point("executive_director")
    
    # Define all possible connections between agents
    connections = {
        "executive_director": [
            "creative_director", "editorial_director", "content_development_director",
            "market_alignment_director", "domain_knowledge_specialist",
            "cultural_authenticity_expert"
        ],
        "creative_director": [
            "structure_architect", "plot_development_specialist", "world_building_expert",
            "character_psychology_specialist", "character_voice_designer",
            "character_relationship_mapper", "executive_director"
        ],
        "editorial_director": [
            "structural_editor", "character_arc_evaluator", "thematic_coherence_analyst",
            "prose_enhancement_specialist", "dialogue_refinement_expert",
            "rhythm_cadence_optimizer", "grammar_consistency_checker",
            "fact_verification_specialist", "executive_director"
        ],
        "content_development_director": [
            "chapter_drafters", "scene_construction_specialists", "dialogue_crafters",
            "continuity_manager", "voice_consistency_monitor", "emotional_arc_designer",
            "executive_director"
        ],
        "market_alignment_director": [
            "positioning_specialist", "title_blurb_optimizer", "differentiation_strategist",
            "market_research", "executive_director"
        ],
        # Research connections
        "domain_knowledge_specialist": ["domain_research", "executive_director"],
        "cultural_authenticity_expert": ["cultural_research", "executive_director"],
        "fact_verification_specialist": ["domain_research", "editorial_director"]
    }
    
    # Add all connections
    for source, targets in connections.items():
        for target in targets:
            builder.add_edge(source, target)
    
    # Add return paths from specialists to their directors
    specialist_to_director = {
        # Creative team to creative director
        "structure_architect": "creative_director",
        "plot_development_specialist": "creative_director",
        "world_building_expert": "creative_director",
        "character_psychology_specialist": "creative_director",
        "character_voice_designer": "creative_director",
        "character_relationship_mapper": "creative_director",
        
        # Content team to content development director
        "chapter_drafters": "content_development_director",
        "scene_construction_specialists": "content_development_director",
        "dialogue_crafters": "content_development_director",
        "continuity_manager": "content_development_director",
        "voice_consistency_monitor": "content_development_director",
        "emotional_arc_designer": "content_development_director",
        
        # Editorial team to editorial director
        "structural_editor": "editorial_director",
        "character_arc_evaluator": "editorial_director",
        "thematic_coherence_analyst": "editorial_director",
        "prose_enhancement_specialist": "editorial_director",
        "dialogue_refinement_expert": "editorial_director",
        "rhythm_cadence_optimizer": "editorial_director",
        "grammar_consistency_checker": "editorial_director",
        
        # Marketing team to market alignment director
        "positioning_specialist": "market_alignment_director",
        "title_blurb_optimizer": "market_alignment_director",
        "differentiation_strategist": "market_alignment_director"
    }
    
    # Add return paths
    for specialist, director in specialist_to_director.items():
        builder.add_edge(specialist, director)
    
    # Add research return paths
    builder.add_edge("domain_research", "domain_knowledge_specialist")
    builder.add_edge("cultural_research", "cultural_authenticity_expert")
    builder.add_edge("market_research", "market_alignment_director")
    
    # Set entry point
    builder.set_entry_point("executive_director")
    
    # Add end state connections
    builder.add_edge("executive_director", END)
    
    # Try to use MongoDB checkpointer if available
    mongo_uri = os.getenv("MONGODB_URI")
    try:
        if mongo_uri:
            mongo_client = MongoClient(mongo_uri)
            checkpointer = MongoDBSaver(mongo_client)
            return builder.compile(checkpointer=checkpointer)  # Removed recursion_limit parameter
        else:
            return builder.compile()  # Removed recursion_limit parameter
    except Exception as e:
        print(f"Warning: Failed to create MongoDB checkpointer: {str(e)}. Proceeding without checkpointing.")
        return builder.compile()  # Removed recursion_limit parameter

def create_agent_model_configs():
    """Create model configurations for each agent based on optimal performance characteristics."""
    return {
        # Directors
        "executive_director": {
            "model_id": "mistralai/Mixtral-8x7B-Instruct-v0.1",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "creative_director": {
            "model_id": "mistralai/Mistral-7B-Instruct-v0.2",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "editorial_director": {
            "model_id": "meta-llama/Llama-2-13b-chat-hf",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "content_development_director": {
            "model_id": "google/gemma-7b-it",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "market_alignment_director": {
            "model_id": "HuggingFaceH4/zephyr-7b-beta",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        
        # Creative Team
        "structure_architect": {
            "model_id": "microsoft/phi-2",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "plot_development_specialist": {
            "model_id": "HuggingFaceH4/zephyr-7b-beta",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "world_building_expert": {
            "model_id": "google/gemma-7b-it",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "character_psychology_specialist": {
            "model_id": "meta-llama/Llama-2-7b-chat-hf",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "character_voice_designer": {
            "model_id": "mistralai/Mistral-7B-Instruct-v0.2",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "character_relationship_mapper": {
            "model_id": "microsoft/phi-2",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        
        # Content Team
        "chapter_drafters": {
            "model_id": "meta-llama/Llama-2-13b-chat-hf",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "scene_construction_specialists": {
            "model_id": "google/gemma-7b-it",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "dialogue_crafters": {
            "model_id": "mistralai/Mistral-7B-Instruct-v0.2",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "continuity_manager": {
            "model_id": "microsoft/phi-2",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "voice_consistency_monitor": {
            "model_id": "HuggingFaceH4/zephyr-7b-beta",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "emotional_arc_designer": {
            "model_id": "meta-llama/Llama-2-7b-chat-hf",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        
        # Editorial Team
        "prose_enhancement_specialist": {
            "model_id": "mistralai/Mixtral-8x7B-Instruct-v0.1",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "dialogue_refinement_expert": {
            "model_id": "mistralai/Mistral-7B-Instruct-v0.2",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "structural_editor": {
            "model_id": "microsoft/phi-2",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "character_arc_evaluator": {
            "model_id": "microsoft/phi-2",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "thematic_coherence_analyst": {
            "model_id": "microsoft/phi-2",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "rhythm_cadence_optimizer": {
            "model_id": "microsoft/phi-2",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "grammar_consistency_checker": {
            "model_id": "google/flan-t5-large",
            "task": "text2text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "fact_verification_specialist": {
            "model_id": "microsoft/phi-2",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        
        # Research Team
        "domain_knowledge_specialist": {
            "model_id": "google/flan-t5-xl",
            "task": "text2text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "cultural_authenticity_expert": {
            "model_id": "google/flan-t5-xl",
            "task": "text2text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        
        # Marketing Team
        "positioning_specialist": {
            "model_id": "microsoft/phi-2",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "title_blurb_optimizer": {
            "model_id": "meta-llama/Llama-2-7b-chat-hf",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "differentiation_strategist": {
            "model_id": "microsoft/phi-2",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        },
        "formatting_standards_expert": {
            "model_id": "microsoft/phi-2",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,
            "repetition_penalty": 1.03
        }
    }

class storybook:
    def __init__(self, model_config=None):
        # Check for HuggingFace token
        self.huggingface_token = os.getenv("HUGGINGFACE_API_TOKEN")
        if not self.huggingface_token:
            raise ValueError("HUGGINGFACE_API_TOKEN not found in environment variables")
        
        # Log into Hugging Face
        login(token=self.huggingface_token)
        
        # Setup MongoDB connection
        self.MONGODB_URI = os.getenv("MONGODB_URI")
        if not self.MONGODB_URI:
            print("Warning: MONGODB_URI not found in environment variables. Using default connection.")
            self.MONGODB_URI = "mongodb://localhost:27017/"
            
        try:
            # Verify MongoDB connection
            self.mongo_client = MongoClient(self.MONGODB_URI, serverSelectionTimeoutMS=5000)
            self.mongo_client.admin.command('ping')  # Test connection
            print("MongoDB connection successful")
        except Exception as e:
            print(f"Warning: Could not connect to MongoDB: {str(e)}. Checkpointing may not work correctly.")
            self.mongo_client = None
        
        # Default model config if none provided
        # Add agent-specific model configs from the function
        if model_config is None:
            model_config = {
                "model_id": "HuggingFaceH4/zephyr-7b-beta",
                "task": "text-generation",
                "temperature": 0.1,
                "max_new_tokens": 512,
                "do_sample": True,  # Changed from False to True
                "repetition_penalty": 1.03,
                "agent_configs": create_agent_model_configs()
            }
        self.model_config = model_config
        
        self.tavily = TavilyClient(api_key=tavily_ai_api_key)
        
        # Setup storybook configuration
        self.storybook_config = {
            "MONGODB_URI": self.MONGODB_URI,
            "mongodb_database_name": "storybook",
            "quality_gates": {
                "initialization_to_development": {
                    "planning_quality": 0.7,
                    "market_alignment": 0.6
                },
                "development_to_creation": {
                    "structure_quality": 0.7,
                    "character_depth": 0.7,
                    "world_building": 0.7
                },
                "creation_to_refinement": {
                    "content_quality": 0.7,
                    "narrative_flow": 0.7,
                    "dialogue_quality": 0.7
                },
                "refinement_to_finalization": {
                    "editing_quality": 0.8,
                    "prose_quality": 0.8,
                    "thematic_coherence": 0.7
                },
                "finalization_to_complete": {
                    "market_readiness": 0.8,
                    "overall_quality": 0.8
                }
            }
        }
        
        # Create a single agent factory for shared model access
        self.agent_factory = AgentFactory(self.storybook_config, self.model_config)
        
        # Build the storybook graphs for each phase (sharing agent factory)
        self.phase_graphs = {}
        for phase in ["initialization", "development", "creation", "refinement", "finalization"]:
            self.phase_graphs[phase] = create_phase_graph(phase, "default_project", self.storybook_config, self.agent_factory)
            
        # Build the main composite graph
        self.graph = create_main_graph(self.storybook_config, self.agent_factory)
        
        # Build the comprehensive storybook graph with all agents
        self.storybook_graph = create_storybook_graph(self.storybook_config, self.agent_factory)
    
    def update_model_config(self, model_config):
        """Update the model configuration and update the agent factory."""
        self.model_config = model_config
        
        # Create a new agent factory with the updated config
        self.agent_factory = AgentFactory(self.storybook_config, self.model_config)
        
        # Rebuild the storybook graphs for each phase
        self.phase_graphs = {}
        for phase in ["initialization", "development", "creation", "refinement", "finalization"]:
            self.phase_graphs[phase] = create_phase_graph(phase, "default_project", self.storybook_config, self.agent_factory)
            
        # Rebuild the main composite graph
        self.graph = create_main_graph(self.storybook_config, self.agent_factory)
        
        # Rebuild the comprehensive storybook graph
        self.storybook_graph = create_storybook_graph(self.storybook_config, self.agent_factory)
    
    def initialize_storybook_project(self, title: str, synopsis: str, manuscript: str, notes: Optional[Dict[str, Any]] = None):
        """Initialize a new storybook project"""
        project_id = str(uuid4())
        
        # Split the manuscript into manageable chunks
        manuscript_chunks = split_manuscript(manuscript)
        
        initial_state = {
            "project": {
                "id": project_id,
                "title": title,
                "synopsis": synopsis,
                "manuscript": manuscript,
                "manuscript_chunks": manuscript_chunks,
                "notes": notes or {},
                "type": "creation",
                "quality_assessment": {
                    "planning_quality": 0.6,  # Initial values
                    "market_alignment": 0.5,
                },
                "created_at": datetime.now().isoformat()
            },
            "phase": "initialization",
            "current_input": {
                "task": "Initial project assessment and planning",
                "phase": "initialization"
            },
            "phase_history": {},
            "messages": [],
            "count": 0,
            "lnode": ""
        }
        
        return initial_state
    
    def run_storybook_phase(self, state, phase="initialization", progress_callback=None, config=None):
        """Run a specific phase of the storybook workflow"""
        if phase not in self.phase_graphs:
            raise ValueError(f"Unknown phase: {phase}")
        
        # Make sure the state includes the phase
        if "phase" not in state:
            state["phase"] = phase
            
        # Create a config with the required checkpoint parameters if not provided
        if config is None:
            project_id = state.get("project", {}).get("id", "default_project")
            thread_id = f"{project_id}_{phase}_{int(time.time())}"  # Generate a unique thread ID
            
            config = {
                "configurable": {
                    "thread_id": thread_id,  # Required for checkpointing
                    "checkpoint_id": project_id,  # Optional but useful for organization
                    "checkpoint_ns": phase,  # Optional namespace for checkpointing
                }
            }
        
        # Custom callback to capture streaming content from agents
        class StreamingCapture(StreamingStdOutCallbackHandler):
            def __init__(self, progress_callback):
                super().__init__()
                self.progress_callback = progress_callback
                self.current_agent = None
                self.token_buffer = ""
                self.last_token_time = time.time()
                
            def on_llm_start(self, serialized, prompts, **kwargs):
                self.token_buffer = ""
                self.last_token_time = time.time()
                run_id = kwargs.get("run_id", "")
                if "." in run_id:
                    # Extract agent name from run_id
                    parts = run_id.split(".")
                    self.current_agent = parts[-1]
                    if self.progress_callback:
                        timestamp = datetime.now().strftime('%H:%M:%S')
                        self.progress_callback(f"[{timestamp}]  Agent {self.current_agent} started")
            
            def on_llm_new_token(self, token: str, **kwargs):
                try:
                    # Add token to buffer
                    self.token_buffer += token
                    
                    # If we have a period or question mark, or it's been more than 1 second,
                    # or buffer is getting large, send what we have
                    current_time = time.time()
                    if ('.' in token or '?' in token or '!' in token or 
                            current_time - self.last_token_time > 1.0 or 
                            len(self.token_buffer) > 80):
                        
                        # Send to callback if available
                        if self.current_agent and self.progress_callback:
                            self.progress_callback(f"[Agent: {self.current_agent}] {self.token_buffer.strip()}")
                        
                        # Reset buffer and time
                        self.token_buffer = ""
                        self.last_token_time = current_time
                        
                except Exception as e:
                    if self.progress_callback:
                        self.progress_callback(f"Error in streaming callback: {e}")
            
            def on_llm_end(self, response, **kwargs):
                # Send any remaining tokens in buffer
                if self.token_buffer and self.current_agent and self.progress_callback:
                    self.progress_callback(f"[Agent: {self.current_agent}] {self.token_buffer.strip()}")
                    self.token_buffer = ""
                    
                if self.current_agent and self.progress_callback:
                    timestamp = datetime.now().strftime('%H:%M:%S')
                    self.progress_callback(f"[{timestamp}]  Agent {self.current_agent} completed")
                
                self.current_agent = None
        
        # Apply streaming callback if we have a progress callback
        if progress_callback:
            progress_callback(f"Starting phase {phase}")
            if "callbacks" not in config:
                config["callbacks"] = []
            config["callbacks"].append(StreamingCapture(progress_callback))
        
        try:
            # Run the phase graph with timeout protection and the checkpoint config
            result = self.phase_graphs[phase].invoke(state, config=config)
            
            # Call progress callback if provided
            if progress_callback:
                progress_callback(f"Completed phase {phase}")
                
            return result
            
        except Exception as e:
            error_msg = f"Error during phase execution: {str(e)}"
            print(error_msg)
            traceback.print_exc()
            
            if progress_callback:
                progress_callback(f"ERROR: {error_msg}")
            
            # Return the original state with error info
            updated_state = state.copy()
            updated_state["error"] = error_msg
            return updated_state
        
    def run_storybook(self, state):
        """Run the full storybook workflow using the main graph"""
        try:
            # Create a config with the required checkpoint parameters
            project_id = state.get("project", {}).get("id", "default_project")
            thread_id = f"{project_id}_main_{int(time.time())}"  # Generate a unique thread ID
            
            config = {
                "configurable": {
                    "thread_id": thread_id,  # Required for checkpointing
                    "checkpoint_id": project_id,  # Optional but useful
                    "checkpoint_ns": "main",  # Optional namespace
                }
            }
            
            result = self.graph.invoke(state, config=config)
            return result
        except Exception as e:
            error_msg = f"Error during storybook execution: {str(e)}"
            print(error_msg)
            traceback.print_exc()
            
            # Return the original state with error info
            updated_state = state.copy()
            updated_state["error"] = error_msg
            return updated_state
    
    def get_available_checkpoints(self, project_id=None):
        """List available checkpoints from MongoDB"""
        if not self.mongo_client:
            print("MongoDB client not available. Cannot retrieve checkpoints.")
            return []
        
        try:
            checkpoints = []
            db = self.mongo_client[self.storybook_config["mongodb_database_name"]]
            
            # First check if the collection exists
            collection_names = db.list_collection_names()
            if "checkpoints" not in collection_names:
                print(f"No checkpoints collection found in database. Available collections: {collection_names}")
                return []
                
            checkpoint_collection = db["checkpoints"]
            
            # Debug: Count documents in collection
            doc_count = checkpoint_collection.count_documents({})
            print(f"Found {doc_count} total checkpoint documents in collection")
            
            # Query for all checkpoints or filter by project_id
            query = {}
            if project_id:
                query = {"configurable.checkpoint_id": project_id}
                
            print(f"Using query: {query}")
            
            # First try a simple find to see what documents look like
            sample_docs = list(checkpoint_collection.find(query).limit(5))
            print(f"Sample checkpoint documents: {len(sample_docs)}")
            
            for doc in sample_docs:
                print(f"Document ID: {doc.get('id', 'unknown')}, Config: {doc.get('configurable', {})}")
            
            # Now try aggregation
            try:
                # Aggregate to get the latest checkpoints for each thread
                pipeline = [
                    {"$match": query},
                    {"$sort": {"v": -1}},  # Sort by version in descending order
                    {"$group": {
                        "_id": "$configurable.thread_id",
                        "doc": {"$first": "$$ROOT"}
                    }},
                    {"$replaceRoot": {"newRoot": "$doc"}}
                ]
                
                print("Running aggregation pipeline...")
                cursor = checkpoint_collection.aggregate(pipeline)
                
                for doc in cursor:
                    try:
                        config = doc.get("configurable", {})
                        checkpoint_id = config.get("checkpoint_id", "unknown")
                        phase = config.get("checkpoint_ns", "unknown")
                        thread_id = config.get("thread_id", "unknown")
                        
                        # Get metadata about the checkpoint
                        created_at = doc.get("ts", datetime.now()).strftime('%Y-%m-%d %H:%M:%S')
                        
                        checkpoints.append({
                            "project_id": checkpoint_id,
                            "phase": phase,
                            "checkpoint_id": doc.get("id"),
                            "thread_id": thread_id,
                            "last_modified": created_at
                        })
                    except Exception as e:
                        print(f"Error processing checkpoint {doc.get('id')}: {str(e)}")
            
            except Exception as agg_err:
                print(f"Aggregation error: {str(agg_err)}")
                # Fallback to simpler approach
                print("Using fallback approach...")
                docs = list(checkpoint_collection.find(query).sort("ts", -1).limit(50))
                for doc in docs:
                    try:
                        config = doc.get("configurable", {})
                        checkpoint_id = config.get("checkpoint_id", "unknown")
                        phase = config.get("checkpoint_ns", "unknown")
                        thread_id = config.get("thread_id", "unknown")
                        
                        # Get metadata about the checkpoint
                        created_at = doc.get("ts", datetime.now()).strftime('%Y-%m-%d %H:%M:%S')
                        
                        checkpoints.append({
                            "project_id": checkpoint_id,
                            "phase": phase,
                            "checkpoint_id": doc.get("id"),
                            "thread_id": thread_id,
                            "last_modified": created_at
                        })
                    except Exception as e:
                        print(f"Error processing checkpoint {doc.get('id')}: {str(e)}")
            
            # Sort by last modified time
            checkpoints.sort(key=lambda x: x["last_modified"], reverse=True)
            print(f"Returning {len(checkpoints)} checkpoints")
            return checkpoints
            
        except Exception as e:
            print(f"Error retrieving checkpoints from MongoDB: {str(e)}")
            traceback.print_exc()
            return []
    
    def load_checkpoint(self, checkpoint_id):
        """Load a specific checkpoint from MongoDB by checkpoint ID"""
        if not self.mongo_client:
            raise ValueError("MongoDB client not available. Cannot load checkpoint.")
        
        try:
            # Extract phase from checkpoint_id (if we stored it in a pattern)
            # This is simplified - you might need to adapt based on how you store checkpoints
            db = self.mongo_client[self.storybook_config["mongodb_database_name"]]
            checkpoint_collection = db["checkpoints"]
            
            # Find the checkpoint document
            checkpoint_doc = checkpoint_collection.find_one({"id": checkpoint_id})
            
            if not checkpoint_doc:
                # Try alternative approach - it might be stored with different field name
                checkpoint_doc = checkpoint_collection.find_one({"_id": checkpoint_id})
                
            if not checkpoint_doc:
                print(f"Checkpoint with ID {checkpoint_id} not found. Searching with partial match...")
                # Try partial match as last resort
                checkpoint_docs = list(checkpoint_collection.find({"id": {"$regex": checkpoint_id}}))
                if checkpoint_docs:
                    checkpoint_doc = checkpoint_docs[0]
                    print(f"Found checkpoint with partial match: {checkpoint_doc.get('id')}")
                    
            if not checkpoint_doc:
                raise ValueError(f"Checkpoint with ID {checkpoint_id} not found")
            
            # Get the config info from the checkpoint
            config = checkpoint_doc.get("configurable", {})
            phase = config.get("checkpoint_ns", "initialization")
            thread_id = config.get("thread_id", "")
            
            # Get the appropriate graph
            if phase not in self.phase_graphs:
                raise ValueError(f"Unknown phase in checkpoint: {phase}")
                
            graph = self.phase_graphs[phase]
            
            # Create MongoDB checkpoint saver
            mongo_saver = MongoDBSaver(self.mongo_client)
            
            # Construct the config needed to retrieve the checkpoint
            retrieve_config = {"configurable": {"thread_id": thread_id}}
            
            # Get the state from the checkpoint
            state = mongo_saver.get_state(retrieve_config)
            
            return state
        except Exception as e:
            error_msg = f"Error loading checkpoint: {str(e)}"
            print(error_msg)
            traceback.print_exc()
            raise ValueError(error_msg)
    
    # Export manuscript to different formats
    def export_manuscript(self, state, format="text"):
        """Export the manuscript to different formats"""
        try:
            manuscript = state.get("project", {}).get("manuscript", "")
            title = state.get("project", {}).get("title", "Untitled")
            
            if format == "text":
                return manuscript
            elif format == "markdown":
                # Simple Markdown formatting - could be enhanced
                md_content = f"# {title}\n\n{manuscript}"
                return md_content
            elif format == "html":
                # Simple HTML formatting - could be enhanced
                html_content = f"<h1>{title}</h1>\n\n<div>{manuscript.replace('\n', '<br>')}</div>"
                return html_content
            else:
                return manuscript
        except Exception as e:
            print(f"Error exporting manuscript: {str(e)}")
            return "Error exporting manuscript"
    
    # Get manuscript statistics
    def get_manuscript_statistics(self, state):
        """Get statistics about the manuscript"""
        try:
            manuscript = state.get("project", {}).get("manuscript", "")
            
            # Word count
            words = manuscript.split()
            word_count = len(words)
            
            # Character count
            char_count = len(manuscript)
            char_count_no_spaces = len(manuscript.replace(" ", ""))
            
            # Paragraph count (approximation)
            paragraphs = manuscript.split("\n\n")
            paragraph_count = len(paragraphs)
            
            # Sentence count (approximation)
            sentences = re.split(r'[.!?]+', manuscript)
            sentence_count = len(sentences)
            
            # Average word length
            avg_word_length = sum(len(word) for word in words) / max(1, word_count)
            
            # Average sentence length
            avg_sentence_length = word_count / max(1, sentence_count)
            
            # Readability score (simplified Flesch-Kincaid)
            readability = 206.835 - 1.015 * (word_count / max(1, paragraph_count)) - 84.6 * (sum(1 for word in words if len(word) > 1) / max(1, word_count))
            
            return {
                "word_count": word_count,
                "character_count": char_count,
                "character_count_no_spaces": char_count_no_spaces,
                "paragraph_count": paragraph_count,
                "sentence_count": sentence_count,
                "avg_word_length": round(avg_word_length, 2),
                "avg_sentence_length": round(avg_sentence_length, 2),
                "readability_score": round(readability, 2)
            }
        except Exception as e:
            print(f"Error calculating statistics: {str(e)}")
            return {"error": str(e)}

class writer_gui:
    def __init__(self, storybook_instance):
        # Initialize model choices first
        self.model_choices = [
            "mistralai/Mixtral-8x7B-Instruct-v0.1",  # Added
            "meta-llama/Llama-2-13b-chat-hf",        # Added
            "google/gemma-7b-it",                    # Added
            "HuggingFaceH4/zephyr-7b-beta",
            "meta-llama/Llama-2-7b-chat-hf",
            "mistralai/Mistral-7B-Instruct-v0.2",
            "google/flan-t5-base",
            "google/flan-t5-large",                  # Added
            "google/flan-t5-xl",                     # Added
            "facebook/bart-large-cnn",
            "gpt2",
            "distilgpt2",
            "microsoft/phi-2"
        ]
        
        # Then initialize other attributes
        self.storybook = storybook_instance
        self.partial_message = ""
        self.thread_id = -1
        self.thread = {"configurable": {"thread_id": str(self.thread_id)}}
        self.is_running = False
        self.agent_visits = {}
        self.project_id = None
        
        self.default_model_config = {
            "model_id": "HuggingFaceH4/zephyr-7b-beta",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": True,  # Changed from False to True
            "repetition_penalty": 1.03
        }
        
        # Create interface last
        self.demo = self.create_interface()

    async def stream_response(self, message, word_delay=0.01):
        """Stream a response word by word with a delay"""
        words = message.split()
        partial = ""
        for word in words:
            partial += word + " "
            yield partial
            await asyncio.sleep(word_delay)
            
    async def run_all_phases(self, title, synopsis, manuscript, notes_text, task,
                       model_id, model_task, temperature, max_new_tokens,
                       repetition_penalty, do_sample, agent_model_configs=None, max_iterations_per_phase=5):
        """Run all phases of the storybook workflow in sequence"""
        if self.is_running:
            yield "Already running a session. Please wait for it to complete.", "", "", "", "", []
            return
        
        self.is_running = True
        stream_buffer = []
        phases = ["initialization", "development", "creation", "refinement", "finalization"]
        
        try:
            # Create model configuration
            main_model_config = {
                "model_id": model_id,
                "task": model_task,
                "temperature": temperature,
                "max_new_tokens": max_new_tokens,
                "repetition_penalty": repetition_penalty,
                "do_sample": do_sample
            }
            
            # Include agent-specific model configs if provided
            if agent_model_configs and len(agent_model_configs) > 0:
                model_config = {
                    **main_model_config,
                    "agent_configs": agent_model_configs
                }
            else:
                model_config = main_model_config
            
            # Update storybook with model config
            try:
                self.storybook.update_model_config(model_config)
                stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Model configuration updated successfully")
                if agent_model_configs:
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Agent-specific models configured for {len(agent_model_configs)} agents")
            except Exception as e:
                error_msg = f"Error updating model configuration: {str(e)}"
                stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] {error_msg}")
            
            # Parse notes
            notes = {}
            if notes_text:
                for line in notes_text.strip().split('\n'):
                    if ':' in line:
                        key, value = line.split(':', 1)
                        notes[key.strip()] = value.strip()
            
            # Initialize project
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Initializing new project for all phases...")
            initial_state = self.storybook.initialize_storybook_project(title, synopsis, manuscript, notes)
            self.project_id = initial_state["project"]["id"]
            
            # Calculate manuscript statistics
            stats = self.storybook.get_manuscript_statistics(initial_state)
            stats_text = ", ".join(f"{k}: {v}" for k, v in stats.items() if k != "error")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Manuscript statistics: {stats_text}")
            
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Initialized project with ID: {self.project_id}")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Using default model: {model_id}")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, "initialization", "initialization", self.project_id, "", stream_buffer
            
            # Set initial task - focus on improving the manuscript
            if task:
                initial_state["current_input"]["task"] = task
            else:
                initial_state["current_input"]["task"] = "Review and improve the manuscript to transform it from draft to professional quality"
            
            # Start with state from initialization
            state = initial_state
            
            # Run each phase in sequence
            for phase_idx, phase in enumerate(phases):
                stream_buffer.append(f"\n[{datetime.now().strftime('%H:%M:%S')}] --- Starting Phase: {phase.upper()} ({phase_idx + 1}/{len(phases)}) ---")
                self.partial_message = "\n".join(stream_buffer)
                yield self.partial_message, phase, phase, self.project_id, "", stream_buffer
                
                # Update state for new phase
                state["phase"] = phase
                state["current_input"]["phase"] = phase
                
                # Set phase-specific task
                if phase == "initialization":
                    state["current_input"]["task"] = "Analyze manuscript and identify key areas for improvement"
                elif phase == "development":
                    state["current_input"]["task"] = "Develop story structure, characters, and world-building elements"
                elif phase == "creation":
                    state["current_input"]["task"] = "Rewrite and improve content to elevate writing quality"
                elif phase == "refinement":
                    state["current_input"]["task"] = "Polish prose and dialogue to professional quality"
                elif phase == "finalization":
                    state["current_input"]["task"] = "Finalize manuscript and prepare for publication"
                
                # Run this phase for multiple iterations
                for iteration in range(max_iterations_per_phase):
                    stream_buffer.append(f"\n[{datetime.now().strftime('%H:%M:%S')}] --- {phase.upper()} Phase - Iteration {iteration + 1}/{max_iterations_per_phase} ---")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, state.get('lnode', phase), phase, self.project_id, "", stream_buffer
                    
                    # Create a queue for progress updates
                    progress_queue = queue.Queue()
                    result_container = [None]
                    error_container = [None]
                    
                    # Function to run in a separate thread
                    def run_in_thread():
                        try:
                            progress_queue.put(f"[{datetime.now().strftime('%H:%M:%S')}] Starting {phase} phase processing...")
                            
                            # Define progress callback
                            def progress_cb(message):
                                progress_queue.put(f"{message}")
                            
                            # Set config with checkpoint info
                            config = {
                                "configurable": {
                                    "thread_id": f"{self.project_id}_{phase}_{int(time.time())}",
                                    "checkpoint_id": self.project_id,
                                    "checkpoint_ns": phase,
                                }
                            }
                            
                            # Run the phase with the config
                            result = self.storybook.run_storybook_phase(state, phase, progress_cb, config)
                            result_container[0] = result
                            
                            progress_queue.put(f"[{datetime.now().strftime('%H:%M:%S')}] Completed {phase} phase processing")
                        except Exception as e:
                            error_container[0] = str(e)
                            progress_queue.put(f"[{datetime.now().strftime('%H:%M:%S')}] ERROR processing {phase}: {str(e)}")
                    
                    # Start thread
                    thread = threading.Thread(target=run_in_thread)
                    thread.start()
                    
                    # Wait for thread with timeout and updates
                    max_wait_time = 1200  # 20 minutes instead of 5
                    start_time = time.time()
                    heartbeat_counter = 0
                    
                    while thread.is_alive():
                        # Check timeout
                        if time.time() - start_time > max_wait_time:
                            error_msg = f"Operation timed out for {phase} phase after 20 minutes"
                            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] ERROR: {error_msg}")
                            error_container[0] = error_msg
                            break
                            
                        # Check for progress
                        messages_added = False
                        while not progress_queue.empty():
                            message = progress_queue.get()
                            stream_buffer.append(message)
                            messages_added = True
                        
                        # Heartbeat - much less frequent now and only if no messages were added
                        heartbeat_counter += 1
                        if heartbeat_counter >= 180 and not messages_added:  # Every 3 minutes if no activity
                            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Process still active...")
                            heartbeat_counter = 0
                            messages_added = True
                        
                        if messages_added:
                            self.partial_message = "\n".join(stream_buffer)
                            yield self.partial_message, state.get('lnode', phase), phase, self.project_id, "", stream_buffer
                        
                        await asyncio.sleep(0.1)  # Check more frequently - 10 times per second
                        
                        # Force memory cleanup periodically
                        if heartbeat_counter % 60 == 0:  # Every minute
                            memory_stats = cleanup_memory()
                            system_memory = memory_stats["system"]
                            if system_memory["percent_used"] > 90:  # If system memory usage is high
                                stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] WARNING: High memory usage ({system_memory['percent_used']}%). Performing cleanup.")
                    
                    # Get remaining updates
                    while not progress_queue.empty():
                        message = progress_queue.get()
                        stream_buffer.append(message)
                    
                    # Check for errors
                    if error_container[0]:
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error in {phase} phase: {error_container[0]}")
                        self.partial_message = "\n".join(stream_buffer)
                        yield self.partial_message, "error", phase, self.project_id, "", stream_buffer
                        break
                    
                    # Get the result
                    state = result_container[0]
                    
                    if state and hasattr(state, 'get') and state.get("error"):
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error in {phase} phase: {state['error']}")
                        self.partial_message = "\n".join(stream_buffer)
                        yield self.partial_message, "error", phase, self.project_id, "", stream_buffer
                        break
                    
                    # Extract last message
                    if state.get("messages", []):
                        last_message = state["messages"][-1]
                        agent_name = state.get("lnode", "unknown")
                        content = last_message.get('content', '')
                        content_preview = content[:300]
                        if len(content) > 300:
                            content_preview += "..."
                            
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] [{agent_name}]: {content_preview}")
                    
                    # Update quality assessment
                    quality_assessment = state.get("project", {}).get("quality_assessment", {})
                    quality_str = "Quality Assessment:\n" + "\n".join(
                        [f"- {k}: {v:.2f}" for k, v in quality_assessment.items()]
                    )
                    
                    # Check if completed
                    if state.get("lnode") == "END":
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] {phase.upper()} phase complete. Moving to next phase.")
                        break
                    
                    # Yield current state
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, state.get('lnode', phase), phase, self.project_id, quality_str, stream_buffer
                
                stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Completed {phase.upper()} phase.")
            
            # All phases complete
            stream_buffer.append(f"\n[{datetime.now().strftime('%H:%M:%S')}] --- All phases completed successfully! ---")
            stream_buffer.append(f"\n[{datetime.now().strftime('%H:%M:%S')}] Your draft manuscript has been transformed into a polished novel!")
            
            # Calculate final manuscript statistics
            final_stats = self.storybook.get_manuscript_statistics(state)
            final_stats_text = ", ".join(f"{k}: {v}" for k, v in final_stats.items() if k != "error")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Final manuscript statistics: {final_stats_text}")
            
            # Final quality assessment
            quality_assessment = state.get("project", {}).get("quality_assessment", {})
            quality_str = "Final Quality Assessment:\n" + "\n".join(
                [f"- {k}: {v:.2f}" for k, v in quality_assessment.items()]
            )
            
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, "complete", "finalization", self.project_id, quality_str, stream_buffer
            
        except Exception as e:
            error_details = traceback.format_exc()
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Unhandled error: {str(e)}")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error details: {error_details}")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, "error", phases[0], self.project_id, "", stream_buffer
        
        finally:
            self.is_running = False

    async def run_storybook_with_model(self, title, synopsis, manuscript, notes_text, task, phase,
                                     model_id, model_task, temperature, max_new_tokens,
                                     repetition_penalty, do_sample, agent_model_configs=None, max_iterations=10):
        """Run the storybook workflow with streaming updates and custom model config"""
        if self.is_running:
            yield "Already running a session. Please wait for it to complete.", "", "", "", "", []
            return
        
        self.is_running = True
        iterations = 0
        state = None
        quality_str = ""
        stream_buffer = []
        
        try:
            # Create model configuration
            main_model_config = {
                "model_id": model_id,
                "task": model_task,
                "temperature": temperature,
                "max_new_tokens": max_new_tokens,
                "repetition_penalty": repetition_penalty,
                "do_sample": do_sample
            }
            
            # Include agent-specific model configs if provided
            if agent_model_configs and len(agent_model_configs) > 0:
                model_config = {
                    **main_model_config,
                    "agent_configs": agent_model_configs
                }
            else:
                model_config = main_model_config
            
            # Check for empty manuscript and provide default
            if not manuscript.strip():
                manuscript = "This is a sample manuscript for testing."
                stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] WARNING: Empty manuscript provided. Using sample text.")
            
            # Check for empty title and provide default
            if not title.strip():
                title = "Untitled Project"
                stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] WARNING: Empty title provided. Using 'Untitled Project'.")
                
            # Check for empty synopsis and provide default
            if not synopsis.strip():
                synopsis = "No synopsis provided."
                stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] WARNING: Empty synopsis provided. Using default.")
                
            # Update storybook with model config - with error handling
            try:
                self.storybook.update_model_config(model_config)
                stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Model configuration updated successfully")
                if agent_model_configs:
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Agent-specific models configured for {len(agent_model_configs)} agents")
            except Exception as e:
                error_msg = f"Error updating model configuration: {str(e)}"
                stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] {error_msg}")
                # Continue with existing configuration
            
            # Parse notes from text input
            notes = {}
            if notes_text:
                for line in notes_text.strip().split('\n'):
                    if ':' in line:
                        key, value = line.split(':', 1)
                        notes[key.strip()] = value.strip()
            
            # Initialize a new project
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Initializing new project...")
            initial_state = self.storybook.initialize_storybook_project(title, synopsis, manuscript, notes)
            self.project_id = initial_state["project"]["id"]
            
            # Calculate manuscript statistics
            stats = self.storybook.get_manuscript_statistics(initial_state)
            stats_text = ", ".join(f"{k}: {v}" for k, v in stats.items() if k != "error")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Manuscript statistics: {stats_text}")
            
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Initialized project with ID: {self.project_id}")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Using default model: {model_id}")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, "initialization", phase, self.project_id, "", stream_buffer
            
            # Set the phase in the state
            initial_state["phase"] = phase
            initial_state["current_input"]["phase"] = phase
            
            # Set the task - if not provided, set a default for the phase
            if task:
                initial_state["current_input"]["task"] = task
            else:
                # Set appropriate default task based on phase
                if phase == "initialization":
                    initial_state["current_input"]["task"] = "Analyze manuscript and identify key areas for improvement"
                elif phase == "development":
                    initial_state["current_input"]["task"] = "Develop story structure, characters, and world-building elements"
                elif phase == "creation":
                    initial_state["current_input"]["task"] = "Rewrite and improve content to elevate writing quality"
                elif phase == "refinement":
                    initial_state["current_input"]["task"] = "Polish prose and dialogue to professional quality"
                elif phase == "finalization":
                    initial_state["current_input"]["task"] = "Finalize manuscript and prepare for publication"
            
            # Run the workflow for the specified phase
            state = initial_state
            
            while iterations < max_iterations:
                try:
                    # Add iteration header
                    stream_buffer.append(f"\n[{datetime.now().strftime('%H:%M:%S')}] --- Iteration {iterations + 1} in phase '{phase}' ---")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, state.get('lnode', 'starting'), phase, self.project_id, "", stream_buffer
                    
                    # Setup for agent processing
                    agent_name = state.get("lnode", "starting")
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Working with agent: {agent_name}")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, agent_name, phase, self.project_id, "", stream_buffer
                    
                    # Create a thread-safe queue for progress updates
                    progress_queue = queue.Queue()
                    result_container = [None]
                    error_container = [None]
                    
                    # Function to process in a background thread
                    def run_in_thread():
                        try:
                            # Log start
                            progress_queue.put(f"[{datetime.now().strftime('%H:%M:%S')}] Started processing with agent: {agent_name}")
                            
                            # Define progress callback for real-time updates
                            def progress_cb(message):
                                progress_queue.put(f"{message}")
                            
                            # Set config
                            config = {
                                "configurable": {
                                    "thread_id": f"{self.project_id}_{phase}_{int(time.time())}",
                                    "checkpoint_id": self.project_id,
                                    "checkpoint_ns": phase,
                                },
                            }
                            
                            # Run the phase with progress updates and config
                            result = self.storybook.run_storybook_phase(state, phase, progress_cb, config)
                            
                            # Store the result
                            result_container[0] = result
                            
                            # Log completion
                            progress_queue.put(f"[{datetime.now().strftime('%H:%M:%S')}] Completed processing with agent: {agent_name}")
                        except Exception as e:
                            error_details = traceback.format_exc()
                            error_container[0] = str(e)
                            progress_queue.put(f"[{datetime.now().strftime('%H:%M:%S')}] ERROR during processing: {str(e)}")
                            progress_queue.put(f"[{datetime.now().strftime('%H:%M:%S')}] Error details: {error_details}")
                    
                    # Start the thread
                    thread = threading.Thread(target=run_in_thread)
                    thread.start()
                    
                    # Maximum time to wait for the thread (in seconds)
                    max_wait_time = 1200  # 20 minutes instead of 5 minutes
                    start_time = time.time()
                    
                    # Heartbeat counter for less frequent full updates
                    heartbeat_counter = 0
                    
                    # Add an initial progress message
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Processing started (this may take a while)...")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, agent_name, phase, self.project_id, "", stream_buffer
                    
                    # Wait for the thread with timeout and periodic updates
                    while thread.is_alive():
                        # Check for timeout
                        if time.time() - start_time > max_wait_time:
                            error_msg = "Operation timed out after 20 minutes" # Changed from 5 to 20 minutes
                            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] ERROR: {error_msg}")
                            error_container[0] = error_msg
                            break
                            
                        # Check for progress updates
                        messages_added = False
                        while not progress_queue.empty():
                            message = progress_queue.get()
                            stream_buffer.append(message)
                            messages_added = True
                        
                        # Add heartbeat only if no messages have been added in a long time
                        heartbeat_counter += 1
                        if heartbeat_counter >= 300 and not messages_added:  # Every 30 seconds (300 * 0.1s)
                            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Process still active...")
                            heartbeat_counter = 0
                            messages_added = True
                        
                        # Only yield if we added messages
                        if messages_added:
                            self.partial_message = "\n".join(stream_buffer)
                            yield self.partial_message, agent_name, phase, self.project_id, "", stream_buffer
                        
                        # Check more frequently (10 times per second) for better responsiveness
                        await asyncio.sleep(0.1)
                        
                        # Force memory cleanup periodically
                        if heartbeat_counter % 60 == 0:  # Every minute
                            memory_stats = cleanup_memory()
                            system_memory = memory_stats["system"]
                            if system_memory["percent_used"] > 90:  # If system memory usage is high
                                stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] WARNING: High memory usage ({system_memory['percent_used']}%). Performing cleanup.")
                    
                    # Get any remaining progress updates
                    while not progress_queue.empty():
                        message = progress_queue.get()
                        stream_buffer.append(message)
                    
                    # Check for errors
                    if error_container[0] is not None:
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error occurred: {error_container[0]}")
                        self.partial_message = "\n".join(stream_buffer)
                        yield self.partial_message, "error", phase, self.project_id, "", stream_buffer
                        break
                    
                    # Get the result
                    state = result_container[0]
                    
                    # Handle state with error attribute (set in run_storybook_phase)
                    if state and hasattr(state, 'get') and state.get("error"):
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error occurred: {state['error']}")
                        self.partial_message = "\n".join(stream_buffer)
                        yield self.partial_message, "error", phase, self.project_id, "", stream_buffer
                        break
                    
                    # Extract the last message for display
                    if state.get("messages", []):
                        last_message = state["messages"][-1]
                        agent_name = state.get("lnode", "unknown")
                        content = last_message.get('content', '')
                        content_preview = content[:300]
                        if len(content) > 300:
                            content_preview += "..."
                            
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] [{agent_name}]: {content_preview}")
                        
                    # Update quality assessment display
                    quality_assessment = state.get("project", {}).get("quality_assessment", {})
                    quality_str = "Quality Assessment:\n" + "\n".join(
                        [f"- {k}: {v:.2f}" for k, v in quality_assessment.items()]
                    )
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Updated quality assessment")
                    
                    # Check if workflow has ended
                    if state.get("lnode") == "END":
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Workflow completed successfully.")
                        break
                    
                    iterations += 1
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, state.get('lnode', 'working'), phase, self.project_id, quality_str, stream_buffer
                    
                except Exception as e:
                    error_details = traceback.format_exc()
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error in iteration: {str(e)}")
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error details: {error_details}")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, "error", phase, self.project_id, "", stream_buffer
                    break
            
            # Calculate final manuscript statistics
            final_stats = self.storybook.get_manuscript_statistics(state)
            final_stats_text = ", ".join(f"{k}: {v}" for k, v in final_stats.items() if k != "error")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Final manuscript statistics: {final_stats_text}")
            
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Workflow finished after {iterations} iterations.")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, state.get('lnode', 'complete'), phase, self.project_id, quality_str, stream_buffer
        
        except Exception as e:
            error_details = traceback.format_exc()
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Unhandled error: {str(e)}")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error details: {error_details}")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, "error", phase, self.project_id, "", stream_buffer
        
        finally:
            self.is_running = False

    def get_checkpoints(self):
        """Get available checkpoints for loading with enhanced error handling"""
        try:
            # First check if MongoDB client is available
            if not self.storybook.mongo_client:
                print("MongoDB client not available. Setting up connection...")
                try:
                    # Try to reconnect
                    mongo_uri = os.getenv("MONGODB_URI", "mongodb://localhost:27017/")
                    self.storybook.mongo_client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)
                    # Test connection
                    self.storybook.mongo_client.admin.command('ping')
                    print("MongoDB connection successful")
                except Exception as conn_err:
                    print(f"Could not connect to MongoDB: {str(conn_err)}")
                    return ["Error: MongoDB connection failed"], []
            
            # Get checkpoints with detailed logging
            print("Retrieving checkpoints from MongoDB...")
            checkpoints = self.storybook.get_available_checkpoints()
            print(f"Retrieved {len(checkpoints)} checkpoints")
            
            if not checkpoints:
                # If no checkpoints, return an informative message
                return ["No checkpoints available - run a project first"], []
                
            checkpoint_list = [f"{cp['project_id']} - {cp['phase']} ({cp['last_modified']})" for cp in checkpoints]
            checkpoint_ids = [cp["checkpoint_id"] for cp in checkpoints]
            
            print(f"Processed {len(checkpoint_list)} checkpoints")
            return checkpoint_list, checkpoint_ids
        except Exception as e:
            error_msg = f"Error getting checkpoints: {str(e)}"
            print(error_msg)
            traceback.print_exc()
            return [f"Error: {error_msg}"], []

    def get_checkpoint_id_safely(self, selected_value, checkpoint_ids):
        """Safely get a checkpoint ID from various input types."""
        if selected_value is None:
            return None
        
        # Handle list input
        if isinstance(selected_value, list) and selected_value:
            selected_value = selected_value[0]  # Take the first element
        
        # Try to use it as an index
        try:
            if isinstance(selected_value, (int, float)) or (isinstance(selected_value, str) and selected_value.isdigit()):
                idx = int(selected_value)
                if 0 <= idx < len(checkpoint_ids):
                    return checkpoint_ids[idx]
        except (ValueError, IndexError):
            pass
        
        # If that fails, try to match by string
        if isinstance(selected_value, str) and checkpoint_ids:
            # Try to find an ID that contains the selected value
            for checkpoint_id in checkpoint_ids:
                if isinstance(checkpoint_id, str) and selected_value in checkpoint_id:
                    return checkpoint_id
        
        return None

    def load_project_from_checkpoint(self, checkpoint_id):
        """Load a project from a checkpoint ID"""
        if not checkpoint_id:
            return "No checkpoint selected", "", "", "", "", []
        
        try:
            state = self.storybook.load_checkpoint(checkpoint_id)
            
            # Extract project details
            project = state.get("project", {})
            self.project_id = project.get("id", "unknown")
            phase = state.get("phase", "unknown")
            
            # Calculate manuscript statistics
            stats = self.storybook.get_manuscript_statistics(state)
            stats_text = ", ".join(f"{k}: {v}" for k, v in stats.items() if k != "error")
            
            # Prepare status message
            stream_buffer = [
                f"[{datetime.now().strftime('%H:%M:%S')}] Loaded project {self.project_id} from checkpoint",
                f"[{datetime.now().strftime('%H:%M:%S')}] Title: {project.get('title', 'Untitled')}",
                f"[{datetime.now().strftime('%H:%M:%S')}] Phase: {phase}",
                f"[{datetime.now().strftime('%H:%M:%S')}] Last agent: {state.get('lnode', 'unknown')}",
                f"[{datetime.now().strftime('%H:%M:%S')}] Manuscript statistics: {stats_text}"
            ]
            
            # Include quality assessment
            quality_assessment = project.get("quality_assessment", {})
            quality_str = "Quality Assessment:\n" + "\n".join(
                [f"- {k}: {v:.2f}" for k, v in quality_assessment.items()]
            )
            
            # Return the state information to update UI
            message = "\n".join(stream_buffer)
            return message, state.get('lnode', 'unknown'), phase, self.project_id, quality_str, stream_buffer, state
        except Exception as e:
            error_message = f"Error loading checkpoint: {str(e)}"
            return error_message, "", "", "", "", [error_message], None

    async def continue_from_checkpoint(self, state, task, phase, agent_model_configs=None, max_iterations=10):
        """Continue processing from a loaded checkpoint"""
        if self.is_running:
            yield "Already running a session. Please wait for it to complete.", "", "", "", "", []
            return
        
        if not state:
            yield "No state loaded from checkpoint", "", "", "", "", []
            return
            
        self.is_running = True
        iterations = 0
        quality_str = ""
        stream_buffer = []
        
        try:
            # If agent-specific model configs are provided, update the model config
            if agent_model_configs and len(agent_model_configs) > 0:
                try:
                    current_config = self.storybook.model_config.copy()
                    model_config = {
                        **current_config,
                        "agent_configs": agent_model_configs
                    }
                    self.storybook.update_model_config(model_config)
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Updated model configuration with agent-specific models")
                except Exception as e:
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error updating model config: {str(e)}")
            
            # Update the task and phase if provided
            if task:
                state["current_input"]["task"] = task
            
            if phase:
                state["phase"] = phase
                state["current_input"]["phase"] = phase
            
            self.project_id = state["project"]["id"]
            current_phase = state["phase"]
            
            # Calculate manuscript statistics
            stats = self.storybook.get_manuscript_statistics(state)
            stats_text = ", ".join(f"{k}: {v}" for k, v in stats.items() if k != "error")
            
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Continuing project with ID: {self.project_id}")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Phase: {current_phase}")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Task: {state['current_input']['task']}")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Manuscript statistics: {stats_text}")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, state.get('lnode', 'starting'), current_phase, self.project_id, "", stream_buffer
            
            # Run the workflow for the specified phase
            while iterations < max_iterations:
                try:
                    # Add iteration header
                    stream_buffer.append(f"\n[{datetime.now().strftime('%H:%M:%S')}] --- Iteration {iterations + 1} in phase '{current_phase}' ---")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, state.get('lnode', 'continuing'), current_phase, self.project_id, "", stream_buffer
                    
                    # Setup for agent processing
                    agent_name = state.get("lnode", "continuing")
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Working with agent: {agent_name}")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, agent_name, current_phase, self.project_id, "", stream_buffer
                    
                    # Create a thread-safe queue for progress updates
                    progress_queue = queue.Queue()
                    result_container = [None]
                    error_container = [None]
                    
                    # Function to process in a background thread
                    def run_in_thread():
                        try:
                            # Log start
                            progress_queue.put(f"[{datetime.now().strftime('%H:%M:%S')}] Started processing with agent: {agent_name}")
                            
                            # Define progress callback for real-time updates
                            def progress_cb(message):
                                progress_queue.put(f"{message}")
                            
                            # Set config 
                            config = {
                                "configurable": {
                                    "thread_id": f"{self.project_id}_{current_phase}_{int(time.time())}",
                                    "checkpoint_id": self.project_id,
                                    "checkpoint_ns": current_phase,
                                }
                            }
                            
                            # Run the phase with progress updates and config
                            result = self.storybook.run_storybook_phase(state, current_phase, progress_cb, config)
                            
                            # Store the result
                            result_container[0] = result
                            
                            # Log completion
                            progress_queue.put(f"[{datetime.now().strftime('%H:%M:%S')}] Completed processing with agent: {agent_name}")
                        except Exception as e:
                            error_details = traceback.format_exc()
                            error_container[0] = str(e)
                            progress_queue.put(f"[{datetime.now().strftime('%H:%M:%S')}] ERROR during processing: {str(e)}")
                            progress_queue.put(f"[{datetime.now().strftime('%H:%M:%S')}] Error details: {error_details}")
                    
                    # Start the thread
                    thread = threading.Thread(target=run_in_thread)
                    thread.start()
                    
                    # Maximum time to wait for the thread (in seconds)
                    max_wait_time = 1200  # 20 minutes instead of 5 minutes
                    start_time = time.time()
                    
                    # Heartbeat counter for less frequent full updates
                    heartbeat_counter = 0
                    
                    # Add an initial progress message
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Processing started (this may take a while)...")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, agent_name, current_phase, self.project_id, "", stream_buffer
                    
                    # Wait for the thread with timeout and periodic updates
                    while thread.is_alive():
                        # Check for timeout
                        if time.time() - start_time > max_wait_time:
                            error_msg = "Operation timed out after 20 minutes"  # Changed from 5 to 20
                            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] ERROR: {error_msg}")
                            error_container[0] = error_msg
                            break
                            
                        # Check for progress updates
                        messages_added = False
                        while not progress_queue.empty():
                            message = progress_queue.get()
                            stream_buffer.append(message)
                            messages_added = True
                        
                        # Add heartbeat only if no messages have been added for a while
                        heartbeat_counter += 1
                        if heartbeat_counter >= 300 and not messages_added:  # Every 30 seconds (300 * 0.1s)
                            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Process still active...")
                            heartbeat_counter = 0
                            messages_added = True
                        
                        # Only yield if we added messages
                        if messages_added:
                            self.partial_message = "\n".join(stream_buffer)
                            yield self.partial_message, agent_name, current_phase, self.project_id, "", stream_buffer
                        
                        # Check for updates more frequently (10 times per second)
                        await asyncio.sleep(0.1)
                        
                        # Force memory cleanup periodically
                        if heartbeat_counter % 60 == 0:  # Every minute
                            memory_stats = cleanup_memory()
                            system_memory = memory_stats["system"]
                            if system_memory["percent_used"] > 90:  # If system memory usage is high
                                stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] WARNING: High memory usage ({system_memory['percent_used']}%). Performing cleanup.")
                    
                    # Get any remaining progress updates
                    while not progress_queue.empty():
                        message = progress_queue.get()
                        stream_buffer.append(message)
                    
                    # Check for errors
                    if error_container[0] is not None:
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error occurred: {error_container[0]}")
                        self.partial_message = "\n".join(stream_buffer)
                        yield self.partial_message, "error", current_phase, self.project_id, "", stream_buffer
                        break
                    
                    # Get the result
                    state = result_container[0]
                    
                    # Handle state with error attribute
                    if state and hasattr(state, 'get') and state.get("error"):
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error occurred: {state['error']}")
                        self.partial_message = "\n".join(stream_buffer)
                        yield self.partial_message, "error", current_phase, self.project_id, "", stream_buffer
                        break
                    
                    # Extract the last message for display
                    if state.get("messages", []):
                        last_message = state["messages"][-1]
                        agent_name = state.get("lnode", "unknown")
                        content = last_message.get('content', '')
                        content_preview = content[:300]
                        if len(content) > 300:
                            content_preview += "..."
                            
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] [{agent_name}]: {content_preview}")
                        
                    # Update quality assessment display
                    quality_assessment = state.get("project", {}).get("quality_assessment", {})
                    quality_str = "Quality Assessment:\n" + "\n".join(
                        [f"- {k}: {v:.2f}" for k, v in quality_assessment.items()]
                    )
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Updated quality assessment")
                    
                    # Check if workflow has ended
                    if state.get("lnode") == "END":
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Workflow completed successfully.")
                        break
                    
                    iterations += 1
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, state.get('lnode', 'working'), current_phase, self.project_id, quality_str, stream_buffer
                    
                except Exception as e:
                    error_details = traceback.format_exc()
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error in iteration: {str(e)}")
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error details: {error_details}")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, "error", current_phase, self.project_id, "", stream_buffer
                    break
            
            # Calculate final manuscript statistics
            final_stats = self.storybook.get_manuscript_statistics(state)
            final_stats_text = ", ".join(f"{k}: {v}" for k, v in final_stats.items() if k != "error")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Final manuscript statistics: {final_stats_text}")
            
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Workflow finished after {iterations} iterations.")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, state.get('lnode', 'complete'), current_phase, self.project_id, quality_str, stream_buffer
            
        except Exception as e:
            error_details = traceback.format_exc()
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Unhandled error: {str(e)}")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error details: {error_details}")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, "error", state.get("phase", "unknown"), self.project_id, "", stream_buffer
        
        finally:
            self.is_running = False
    
    # Function to export manuscript from current state
    def export_manuscript(self, state, format_type="text"):
        """Export manuscript in various formats"""
        if not state:
            return "No project loaded", None
        
        try:
            # Use storybook's export function
            content = self.storybook.export_manuscript(state, format=format_type)
            
            # Get project title for filename
            title = state.get("project", {}).get("title", "Untitled")
            clean_title = "".join(c if c.isalnum() else "_" for c in title)
            
            # Generate appropriate filename based on format
            if format_type == "text":
                filename = f"{clean_title}.txt"
                mime = "text/plain"
            elif format_type == "markdown":
                filename = f"{clean_title}.md"
                mime = "text/markdown"
            elif format_type == "html":
                filename = f"{clean_title}.html"
                mime = "text/html"
            else:
                filename = f"{clean_title}.txt"
                mime = "text/plain"
                
            return content, (filename, mime)
        except Exception as e:
            error_msg = f"Error exporting manuscript: {str(e)}"
            print(error_msg)
            traceback.print_exc()
            return error_msg, None
    
    # Add functions to load and save prompts from MongoDB
    def load_prompt_from_mongodb(self, agent_name):
        """Load an agent's prompt from MongoDB, with error handling"""
        if not self.storybook.mongo_client:
            return f"MongoDB client not available. Using default prompt for {agent_name}", ""
            
        try:
            db = self.storybook.mongo_client["storybook"]
            prompts_collection = db["prompts"]
            
            # Try to find the agent's prompt
            doc = prompts_collection.find_one({"agent_name": agent_name})
            if doc and "prompt_text" in doc:
                return f"Loaded prompt for {agent_name} from MongoDB", doc["prompt_text"]
            else:
                return f"No prompt found for {agent_name} in MongoDB, using default", ""
                
        except Exception as e:
            return f"Error loading prompt from MongoDB: {str(e)}", ""
    
    def save_prompt_to_mongodb(self, agent_name, prompt_text):
        """Save an agent's prompt to MongoDB, with error handling"""
        if not self.storybook.mongo_client:
            return f"MongoDB client not available. Cannot save prompt for {agent_name}."
            
        if not agent_name or not prompt_text.strip():
            return "Agent name and prompt text are required."
            
        try:
            db = self.storybook.mongo_client["storybook"]
            prompts_collection = db["prompts"]
            
            # Update if exists, insert if not
            result = prompts_collection.update_one(
                {"agent_name": agent_name},
                {"$set": {"prompt_text": prompt_text, "updated_at": datetime.now()}},
                upsert=True
            )
            
            if result.matched_count > 0:
                return f"Updated prompt for {agent_name} in MongoDB"
            else:
                return f"Inserted new prompt for {agent_name} in MongoDB"
                
        except Exception as e:
            return f"Error saving prompt to MongoDB: {str(e)}"

    def create_interface(self):
        theme = gr.themes.Ocean(
            primary_hue="amber",
            secondary_hue="fuchsia",
            neutral_hue="slate",
            font=[gr.themes.GoogleFont('Work Sans'), 'ui-sans-serif', 'system-ui', 'sans-serif'],
        )
        
        # Define all available agents for model selection
        all_agents = [
            # Directors
            "executive_director", "creative_director", "editorial_director",
            "content_development_director", "market_alignment_director",
            
            # Creative Team
            "structure_architect", "plot_development_specialist", "world_building_expert",
            "character_psychology_specialist", "character_voice_designer", "character_relationship_mapper",
            
            # Content Team
            "chapter_drafters", "scene_construction_specialists", "dialogue_crafters",
            "continuity_manager", "voice_consistency_monitor", "emotional_arc_designer",
            
            # Editorial Team
            "structural_editor", "character_arc_evaluator", "thematic_coherence_analyst",
            "prose_enhancement_specialist", "dialogue_refinement_expert", "rhythm_cadence_optimizer", 
            "grammar_consistency_checker", "fact_verification_specialist",
            
            # Research Team
            "domain_knowledge_specialist", "cultural_authenticity_expert",
            
            # Marketing Team
            "positioning_specialist", "title_blurb_optimizer", "differentiation_strategist",
            "formatting_standards_expert"
        ]
        
        # Group agents by team
        agent_teams = {
            "Directors": ["executive_director", "creative_director", "editorial_director", 
                         "content_development_director", "market_alignment_director"],
            "Creative Team": ["structure_architect", "plot_development_specialist", "world_building_expert",
                            "character_psychology_specialist", "character_voice_designer", "character_relationship_mapper"],
            "Content Team": ["chapter_drafters", "scene_construction_specialists", "dialogue_crafters",
                           "continuity_manager", "voice_consistency_monitor", "emotional_arc_designer"],
            "Editorial Team": ["structural_editor", "character_arc_evaluator", "thematic_coherence_analyst", 
                             "prose_enhancement_specialist", "dialogue_refinement_expert", "rhythm_cadence_optimizer",
                             "grammar_consistency_checker", "fact_verification_specialist"],
            "Research Team": ["domain_knowledge_specialist", "cultural_authenticity_expert"],
            "Marketing Team": ["positioning_specialist", "title_blurb_optimizer", "differentiation_strategist",
                             "formatting_standards_expert"]
        }
        
        # Function for system stats display
        def get_system_stats():
            memory_stats = get_memory_usage()
            system_memory = memory_stats["system"]
            gpu_memory = memory_stats["gpu"]
            
            stats = [f"System Memory: {system_memory['percent_used']}% used ({system_memory['available'] / 1e9:.1f} GB free)"]
            
            if gpu_memory:
                for device_id, device_data in gpu_memory.items():
                    if isinstance(device_id, int):  # Skip error key if present
                        used_gb = device_data["allocated"] / 1e9
                        total_gb = device_data["total"] / 1e9
                        percent = (device_data["allocated"] / device_data["total"]) * 100 if device_data["total"] > 0 else 0
                        stats.append(f"GPU {device_id}: {percent:.1f}% used ({used_gb:.1f}/{total_gb:.1f} GB)")
            
            return "\n".join(stats)
        
        with gr.Blocks(
            theme=theme,
            analytics_enabled=False
        ) as demo:
            gr.Markdown("# Storybook Writer")
            
            # First Row - Status Panel
            with gr.Row():
                sb_lnode_bx = gr.Textbox(label="Current Agent", scale=1)
                sb_phase_bx = gr.Textbox(label="Current Phase", scale=1)
                sb_thread_bx = gr.Textbox(label="Project ID", scale=1)
                sb_quality_bx = gr.Textbox(label="Quality Assessment", scale=2)
                sb_system_stats = gr.Textbox(label="System Stats", scale=1, value=get_system_stats())
            
            # Hidden state for loaded checkpoints
            loaded_state = gr.State(None)
            
            # Stream output pane that will show real-time updates
            stream_output = gr.State([])
            
            # Dictionary to store agent model configs
            agent_models = gr.State({})
            
            with gr.Tabs() as tabs:
                # Project Content Tab (First for emphasis)
                with gr.Tab("1. Manuscript Input"):
                    gr.Markdown("### Enter your draft manuscript and project details")
                    gr.Markdown("The system will transform your draft into a polished novel-quality manuscript using specialized agents.")
                    with gr.Row():
                        with gr.Column():
                            title_bx = gr.Textbox(
                                label="Title (required)",
                                value="The Hidden Quest",
                                lines=1
                            )
                            synopsis_bx = gr.Textbox(
                                label="Synopsis (required)",
                                value="A young adventurer discovers a mysterious artifact that leads to an epic journey through dangerous lands and magical encounters.",
                                lines=3
                            )
                            notes_bx = gr.Textbox(
                                label="Notes (optional - key: value format)",
                                value="target_audience: Young Adult\ngenre: Fantasy\nwriting_style: Descriptive\ntone: Adventure, Mystery",
                                lines=4
                            )
                    
                    gr.Markdown("### Enter your draft manuscript below:")
                    with gr.Row():
                        manuscript_bx = gr.Textbox(
                            label="Manuscript (required)",
                            value="Chapter 1: The Discovery\n\nThe sun was setting over the village of Elmwood as James finished his chores. He wiped sweat from his brow and looked at the distant mountains. Something was calling to him from beyond the fields he knew so well.\n\n\"James! Dinner's ready,\" his mother called from their cottage.\n\nHe sighed and turned back. Another day, same as always. But tomorrow would be different. Tomorrow he would explore the old ruins his friend Thomas had told him about.\n\nAs he ate dinner, he couldn't help but think about what he might find there. Legend said the ruins held powerful artifacts from the ancient kingdom that once ruled these lands.\n\nThe next morning, James woke early and packed his bag with food, water, and a small knife. He snuck out before his parents woke up.\n\nThe ruins were a three-hour walk from the village, deep in the forest that everyone avoided. People said the forest was haunted, but James didn't believe in such nonsense.\n\nWhen he arrived at the ruins, he was surprised by how intact they still were. Stone pillars rose from the ground, covered in vines and moss. There was an entrance that led underground.\n\n\"Well, here goes nothing,\" he said to himself as he lit his torch and descended into the darkness.\n\nThe air was cool and damp. The walls were decorated with strange symbols he couldn't understand. He walked carefully, listening for any sounds that might mean danger.\n\nAfter what seemed like hours, he entered a large chamber. In the center, on a stone pedestal, was a small crystal orb that glowed with an inner light.\n\n\"Wow, what is this?\" he whispered as he approached. When he touched it, visions flooded his mind - distant lands, strange creatures, and a looming darkness threatening to consume everything.\n\nHe pulled his hand back in shock, but knew immediately what he had to do. This artifact was important, and somehow, he was now connected to its purpose.\n\nJames carefully placed the orb in his bag and made his way back to the surface, unaware that his discovery had already set ancient powers in motion.\n\nAs he walked home, the weight of the orb seemed to grow heavier with each step. His simple life was about to change forever.",
                            lines=15
                        )
                    
                    # Add manuscript import/export options
                    with gr.Row():
                        manuscript_file = gr.File(label="Import manuscript from file", file_types=["text", ".txt", ".md"])
                        export_format = gr.Dropdown(
                            label="Export Format", 
                            choices=["text", "markdown", "html"], 
                            value="text"
                        )
                        export_btn = gr.Button("Export Manuscript")
                    
                    # Main button for full manuscript transformation
                    with gr.Row():
                        transform_btn = gr.Button("Run Editing Workflow (All Phases)", variant="primary", size="lg")
                        memory_cleanup_btn = gr.Button("Clean Memory", size="sm")
                
                # Task Control Tab            
                with gr.Tab("2. Process Control"):
                    with gr.Row():
                        task_bx = gr.Textbox(
                            label="Specific Task (optional)",
                            value="Review the manuscript and improve character development, dialogue, and descriptive elements",
                            scale=2
                        )
                        phase_select = gr.Dropdown(
                            choices=["initialization", "development", "creation", "refinement", "finalization"],
                            value="creation",
                            label="Phase",
                            scale=1
                        )
                    
                    with gr.Row():
                        run_phase_btn = gr.Button("Run Single Phase", variant="secondary")
                        clear_btn = gr.Button("Clear Output")
                    
                    # Add more focused task buttons
                    with gr.Row():
                        gr.Markdown("### Quick Task Buttons")
                    with gr.Row():
                        improve_prose_btn = gr.Button("Improve Prose Quality", variant="secondary", size="sm")
                        enhance_dialogue_btn = gr.Button("Enhance Dialogue", variant="secondary", size="sm")
                        develop_characters_btn = gr.Button("Develop Characters", variant="secondary", size="sm")
                        structure_plot_btn = gr.Button("Structure Plot", variant="secondary", size="sm")
                    
                    # Add agent workflow visualization
                    with gr.Row():
                        show_graph_btn = gr.Button("Show Agent Workflow Graph", variant="secondary", size="sm")
                    
                    gr.Markdown("### Phase Descriptions:")
                    gr.Markdown("""
                    - **Initialization**: Executive Director assesses the manuscript and identifies areas for improvement
                    - **Development**: Story structure, characters, and world-building are enhanced
                    - **Creation**: Chapter Drafters and Dialogue Crafters rewrite sections to improve content
                    - **Refinement**: Prose Enhancement and Dialogue Refinement experts polish the writing
                    - **Finalization**: The manuscript is given final checks and prepared for publication
                    
                    For best results with draft improvement, use the **Creation** and **Refinement** phases.
                    """)
                
                # Model Configuration Tab
                with gr.Tab("3. Model Configuration"):
                    gr.Markdown("### Default Model Configuration")
                    gr.Markdown("This configuration applies to all agents unless overridden in the Agent-Specific section below.")
                    
                    with gr.Row():
                        model_id = gr.Dropdown(
                            label="Default Model",
                            choices=self.model_choices,
                            value=self.default_model_config["model_id"]
                        )
                        model_task = gr.Dropdown(
                            label="Task",
                            choices=["text-generation", "text2text-generation", "summarization"],
                            value=self.default_model_config["task"]
                        )
                        
                    with gr.Row():
                        temperature = gr.Slider(
                            label="Temperature",
                            minimum=0.0,
                            maximum=1.0,
                            value=self.default_model_config["temperature"],
                            step=0.01
                        )
                        max_new_tokens = gr.Slider(
                            label="Max New Tokens",
                            minimum=10,
                            maximum=1024,
                            value=self.default_model_config["max_new_tokens"],
                            step=1
                        )
                        
                    with gr.Row():
                        repetition_penalty = gr.Slider(
                            label="Repetition Penalty",
                            minimum=1.0,
                            maximum=2.0,
                            value=self.default_model_config["repetition_penalty"],
                            step=0.01
                        )
                        do_sample = gr.Checkbox(
                            label="Do Sample",
                            value=self.default_model_config["do_sample"]
                        )
                    
                    # Add preset configurations for common use cases
                    with gr.Row():
                        gr.Markdown("### Quick Presets")
                    with gr.Row():
                        speed_preset_btn = gr.Button("Speed-Optimized", variant="secondary", size="sm")
                        quality_preset_btn = gr.Button("Quality-Optimized", variant="secondary", size="sm")
                        balanced_preset_btn = gr.Button("Balanced", variant="secondary", size="sm")
                        creative_preset_btn = gr.Button("Creative Mode", variant="secondary", size="sm")
                    
                    gr.Markdown("### Agent-Specific Model Configuration")
                    gr.Markdown("Select different models for specific agents. If not specified, the default model will be used.")
                    
                    # Create lists to store input component references
                    agent_enables = []
                    agent_model_selects = []
                    agent_temperatures = []
                    agent_tokens_list = []
                    
                    # Create nested accordions for each team
                    for team_name, team_agents in agent_teams.items():
                        with gr.Accordion(f"{team_name}", open=False):
                            for agent in team_agents:
                                agent_display = agent.replace("_", " ").title()
                                with gr.Accordion(f"{agent_display}", open=False):
                                    with gr.Row():
                                        enable = gr.Checkbox(label=f"Use Custom Model", value=False)
                                        model = gr.Dropdown(
                                            label="Model",
                                            choices=self.model_choices,
                                            value=self.default_model_config["model_id"],
                                            interactive=True
                                        )
                                        # Store component references
                                        agent_enables.append(enable)
                                        agent_model_selects.append(model)
                                    with gr.Row():
                                        temp = gr.Slider(
                                            label="Temperature",
                                            minimum=0.0,
                                            maximum=1.0,
                                            value=self.default_model_config["temperature"],
                                            step=0.01
                                        )
                                        token = gr.Slider(
                                            label="Max New Tokens",
                                            minimum=10,
                                            maximum=1024,
                                            value=self.default_model_config["max_new_tokens"],
                                            step=1
                                        )
                                        # Store component references
                                        agent_temperatures.append(temp)
                                        agent_tokens_list.append(token)
                    
                    # Function to collect agent model configs
                    def collect_agent_configs(*values):
                        num_agents = len(all_agents)
                        enables = values[:num_agents]
                        models = values[num_agents:num_agents*2]
                        temperatures = values[num_agents*2:num_agents*3]
                        tokens_values = values[num_agents*3:num_agents*4]
                        
                        configs = {}
                        for i, agent in enumerate(all_agents):
                            if enables[i]:  # If this agent's custom model is enabled
                                configs[agent] = {
                                    "model_id": models[i],
                                    "task": "text-generation",
                                    "temperature": temperatures[i],
                                    "max_new_tokens": tokens_values[i],
                                    "do_sample": True if temperatures[i] > 0.01 else False,
                                    "repetition_penalty": 1.03
                                }
                        return configs
                    
                    # Button to apply model configurations
                    apply_models_btn = gr.Button("Apply Model Configuration", variant="secondary")
                    apply_models_btn.click(
                        fn=collect_agent_configs,
                        inputs=agent_enables + agent_model_selects + agent_temperatures + agent_tokens_list,
                        outputs=[agent_models]
                    )
                
                # Checkpoint Tab
                with gr.Tab("4. Checkpoints"):
                    with gr.Row():
                        refresh_checkpoints_btn = gr.Button("Refresh Checkpoints")
                        checkpoint_list = gr.Dropdown(
                            label="Available Checkpoints",
                            choices=[],
                            value=None,
                            interactive=True,
                            allow_custom_value=True
                        )
                        checkpoint_ids = gr.State([])
                    
                    with gr.Row():
                        load_checkpoint_btn = gr.Button("Load Selected Checkpoint")
                        continue_checkpoint_btn = gr.Button("Continue from Checkpoint")

                # Prompt Editor Tab (New)
                with gr.Tab("5. Prompt Editor"):
                    gr.Markdown("### Edit Agent Prompts")
                    gr.Markdown("Customize the system prompts used by each agent. Changes will be saved to MongoDB.")
                    
                    # Agent selection and prompt editing
                    with gr.Row():
                        prompt_agent_select = gr.Dropdown(
                            label="Select Agent",
                            choices=[agent.replace("_", " ").title() for agent in all_agents],
                            value="Executive Director"
                        )
                        refresh_prompt_btn = gr.Button("Load Prompt", variant="secondary")
                    
                    # Status message for prompt loading/saving
                    prompt_status = gr.Textbox(label="Status", value="Select an agent and click Load Prompt", interactive=False)
                    
                    # Prompt editing area
                    prompt_text = gr.Textbox(
                        label="Agent Prompt",
                        value="",
                        lines=10
                    )
                    
                    # Save button for prompts
                    save_prompt_btn = gr.Button("Save Prompt to MongoDB", variant="primary")
                
                # Results & Analysis Tab (New)
                with gr.Tab("6. Results & Analysis"):
                    with gr.Row():
                        gr.Markdown("### Manuscript Analysis")
                        
                    with gr.Row():
                        analyze_btn = gr.Button("Analyze Current Manuscript", variant="secondary")
                        
                    with gr.Row():
                        stats_display = gr.JSON(label="Manuscript Statistics")
                    
                    # Side-by-side comparison for before/after
                    with gr.Row():
                        gr.Markdown("### Before/After Comparison")
                    
                    with gr.Row():
                        original_text = gr.Textbox(label="Original Text", lines=10)
                        improved_text = gr.Textbox(label="Improved Text", lines=10)
                    
                    with gr.Row():
                        compare_btn = gr.Button("Compare Sections", variant="secondary")
                        
                    # Visualization of quality metrics
                    with gr.Row():
                        gr.Markdown("### Quality Assessment Over Time")
                    
                    with gr.Row():
                        quality_chart = gr.Plot(label="Quality Metrics")
            
            # Streaming Output area with real-time updates
            gr.Markdown("### Process Log")
            sb_live = gr.Textbox(label="Output Log", lines=20, autoscroll=True)
            
            # Visual workflow graph display 
            agent_flow_img = gr.Image(label="Agent Workflow Diagram", visible=False)
            
            # Output area for exported manuscript
            exported_text = gr.Textbox(label="Exported Manuscript", visible=False, lines=20)
            export_file = gr.File(label="Download Exported File", visible=False)
            
            # Event handlers
            # Refresh checkpoints
            refresh_checkpoints_btn.click(
                fn=self.get_checkpoints,
                inputs=[],
                outputs=[checkpoint_list, checkpoint_ids]
            )
            
            # Load checkpoint
            load_checkpoint_btn.click(
                fn=lambda checkpoint_list, checkpoint_ids: self.get_checkpoint_id_safely(checkpoint_list, checkpoint_ids),
                inputs=[checkpoint_list, checkpoint_ids],
                outputs=[gr.State(None)]
            ).then(
                fn=lambda checkpoint_id_state: self.load_project_from_checkpoint(checkpoint_id_state),
                inputs=[gr.State(None)],
                outputs=[sb_live, sb_lnode_bx, sb_phase_bx, sb_thread_bx, sb_quality_bx, stream_output, loaded_state]
            )
            
            # Continue from checkpoint
            continue_checkpoint_btn.click(
                fn=lambda: (
                    gr.update(value="Processing...", interactive=False),
                    gr.update(value="Continuing from checkpoint...")
                ),
                inputs=None,
                outputs=[continue_checkpoint_btn, sb_live]
            ).then(
                fn=self.continue_from_checkpoint,
                inputs=[loaded_state, task_bx, phase_select, agent_models],
                outputs=[sb_live, sb_lnode_bx, sb_phase_bx, sb_thread_bx, sb_quality_bx, stream_output],
                show_progress=True
            ).then(
                fn=lambda: gr.update(value="Continue from Checkpoint", interactive=True),
                inputs=None,
                outputs=continue_checkpoint_btn
            )
            
            # Run single phase
            run_phase_btn.click(
                fn=lambda: (
                    gr.update(value="Processing...", interactive=False),
                    gr.update(value="Starting single phase workflow...")
                ),
                inputs=None,
                outputs=[run_phase_btn, sb_live]
            ).then(
                fn=self.run_storybook_with_model,
                inputs=[
                    title_bx, synopsis_bx, manuscript_bx, notes_bx, task_bx, phase_select,
                    model_id, model_task, temperature, max_new_tokens, repetition_penalty, do_sample, agent_models
                ],
                outputs=[sb_live, sb_lnode_bx, sb_phase_bx, sb_thread_bx, sb_quality_bx, stream_output],
                show_progress=True
            ).then(
                fn=lambda: gr.update(value="Run Single Phase", variant="secondary", interactive=True),
                inputs=None,
                outputs=run_phase_btn
            )
            
            transform_btn.click(
                fn=lambda: (
                    gr.update(value="Processing All Phases...", interactive=False),
                    gr.update(value="Starting Manuscript Revision...")
                ),
                inputs=None,
                outputs=[transform_btn, sb_live]
            ).then(
                fn=self.run_all_phases,
                inputs=[
                    title_bx, synopsis_bx, manuscript_bx, notes_bx, task_bx,
                    model_id, model_task, temperature, max_new_tokens, repetition_penalty, do_sample, agent_models
                ],
                outputs=[sb_live, sb_lnode_bx, sb_phase_bx, sb_thread_bx, sb_quality_bx, stream_output],
                show_progress=True
            ).then(
                fn=lambda: gr.update(value="Run Editing Workflow (All Phases)", variant="primary", interactive=True),
                inputs=None,
                outputs=transform_btn
            )
            
            # Clear output
            clear_btn.click(
                fn=lambda: (
                    "", 
                    "", 
                    "",
                    "",
                    "",
                    []
                ),
                inputs=[],
                outputs=[sb_live, sb_lnode_bx, sb_phase_bx, sb_thread_bx, sb_quality_bx, stream_output]
            )
            
            # Export manuscript
            export_btn.click(
                fn=self.export_manuscript,
                inputs=[loaded_state, export_format],
                outputs=[exported_text, export_file]
            ).then(
                fn=lambda: (gr.update(visible=True), gr.update(visible=True)),
                inputs=None,
                outputs=[exported_text, export_file]
            )
            
            # Analyze manuscript
            analyze_btn.click(
                fn=lambda state: self.storybook.get_manuscript_statistics(state) if state else {"error": "No manuscript loaded"},
                inputs=[loaded_state],
                outputs=[stats_display]
            )
            
            # File upload handler for manuscript import
            def import_manuscript_file(file):
                if file is None:
                    return "No file uploaded"
                try:
                    content = file.decode('utf-8')
                    return content
                except Exception as e:
                    return f"Error importing file: {str(e)}"
                
            manuscript_file.change(
                fn=import_manuscript_file,
                inputs=[manuscript_file],
                outputs=[manuscript_bx]
            )
            
            # Task preset buttons
            improve_prose_btn.click(
                fn=lambda: ("Analyze the manuscript and improve the prose quality, focusing on descriptive language, imagery, and sentence flow", "refinement"),
                inputs=None,
                outputs=[task_bx, phase_select]
            )
            
            enhance_dialogue_btn.click(
                fn=lambda: ("Review and enhance dialogue to make it more natural, character-specific, and engaging", "creation"),
                inputs=None,
                outputs=[task_bx, phase_select]
            )
            
            develop_characters_btn.click(
                fn=lambda: ("Analyze character development and improve character depth, motivations, and relationships", "development"),
                inputs=None,
                outputs=[task_bx, phase_select]
            )
            
            structure_plot_btn.click(
                fn=lambda: ("Review plot structure for pacing, tension, and coherence, and suggest improvements", "development"),
                inputs=None,
                outputs=[task_bx, phase_select]
            )
            
            # Model config presets
            def update_model_preset(preset_type):
                if preset_type == "speed":
                    return "microsoft/phi-2", "text-generation", 0.1, 256, 1.03, True
                elif preset_type == "quality":
                    return "mistralai/Mixtral-8x7B-Instruct-v0.1", "text-generation", 0.1, 512, 1.03, True
                elif preset_type == "balanced":
                    return "mistralai/Mistral-7B-Instruct-v0.2", "text-generation", 0.1, 384, 1.03, True
                elif preset_type == "creative":
                    return "meta-llama/Llama-2-7b-chat-hf", "text-generation", 0.7, 512, 1.03, True
                else:
                    return "HuggingFaceH4/zephyr-7b-beta", "text-generation", 0.1, 512, 1.03, True
            
            speed_preset_btn.click(
                fn=lambda: update_model_preset("speed"),
                inputs=None,
                outputs=[model_id, model_task, temperature, max_new_tokens, repetition_penalty, do_sample]
            )
            
            quality_preset_btn.click(
                fn=lambda: update_model_preset("quality"),
                inputs=None,
                outputs=[model_id, model_task, temperature, max_new_tokens, repetition_penalty, do_sample]
            )
            
            balanced_preset_btn.click(
                fn=lambda: update_model_preset("balanced"),
                inputs=None,
                outputs=[model_id, model_task, temperature, max_new_tokens, repetition_penalty, do_sample]
            )
            
            creative_preset_btn.click(
                fn=lambda: update_model_preset("creative"),
                inputs=None,
                outputs=[model_id, model_task, temperature, max_new_tokens, repetition_penalty, do_sample]
            )
            
            # Display agent workflow graph
            def create_agent_workflow_image():
                try:
                    plt = visualize_storybook_graph(self.storybook.storybook_graph)
                    # Save to a temporary file
                    temp_file = "agent_workflow_graph.png"
                    plt.savefig(temp_file, dpi=300, bbox_inches='tight')
                    plt.close()
                    return temp_file
                except Exception as e:
                    print(f"Error creating workflow graph: {str(e)}")
                    # Return a simple error message as text
                    return None
            
            show_graph_btn.click(
                fn=create_agent_workflow_image,
                inputs=None,
                outputs=[agent_flow_img]
            ).then(
                fn=lambda: gr.update(visible=True),
                inputs=None,
                outputs=[agent_flow_img]
            )
            
            # Memory cleanup button
            memory_cleanup_btn.click(
                fn=lambda: f"Memory cleanup performed: {cleanup_memory()['system']['percent_used']}% system memory in use",
                inputs=None,
                outputs=[sb_live]
            ).then(
                fn=get_system_stats,
                inputs=None,
                outputs=[sb_system_stats]
            )
            
            # Add prompt editor handlers
            def prepare_agent_name_for_lookup(display_name):
                """Convert display name like 'Executive Director' to lookup key 'executive_director'"""
                return display_name.lower().replace(" ", "_")
            
            # Load prompt button
            refresh_prompt_btn.click(
                fn=lambda agent_display: self.load_prompt_from_mongodb(prepare_agent_name_for_lookup(agent_display)),
                inputs=[prompt_agent_select],
                outputs=[prompt_status, prompt_text]
            )
            
            # Save prompt button
            save_prompt_btn.click(
                fn=lambda agent_display, prompt_text: self.save_prompt_to_mongodb(
                    prepare_agent_name_for_lookup(agent_display), 
                    prompt_text
                ),
                inputs=[prompt_agent_select, prompt_text],
                outputs=[prompt_status]
            )
            
            # Help Section
            with gr.Accordion("Help & Information", open=False):
                gr.Markdown(
                    "# How to Use Storybook Writer\n\n"
                    "## Quick Start\n"
                    "1. Enter your manuscript in the 'Manuscript Input' tab or import from a text file\n"
                    "2. Click 'Run Editing Workflow' to run all phases or focus on a specific phase\n"
                    "3. Watch as the system improves your draft with specialized agents\n"
                    "4. When complete, export your manuscript in your preferred format\n\n"
                    
                    "## The Editorial Process\n"
                    "1. **Executive Director** reviews your manuscript and identifies areas for improvement\n"
                    "2. **Content specialists** (Chapter Drafters, Dialogue Crafters) rewrite sections\n"
                    "3. **Editorial specialists** (Prose Enhancement, Dialogue Refinement) polish the writing\n"
                    "4. This process repeats until your manuscript reaches professional quality\n\n"
                    
                    "## Advanced Options\n"
                    "- **Process Control**: Run specific phases or focus on particular tasks\n"
                    "- **Model Configuration**: Set different models for each agent type\n"
                    "- **Checkpoints**: Save progress and continue later\n"
                    "- **Prompt Editor**: Customize the system prompts for each agent\n"
                    "- **Results & Analysis**: View statistics and compare before/after improvements\n\n"
                    
                    "## Model Presets\n"
                    "- **Speed-Optimized**: Faster processing with smaller models (best for limited hardware)\n"
                    "- **Quality-Optimized**: Best output quality but slower processing\n" 
                    "- **Balanced**: Good compromise between speed and quality\n"
                    "- **Creative Mode**: More innovative and varied outputs but less predictable\n\n"
                    
                    "## Task Presets\n"
                    "- **Improve Prose Quality**: Enhance descriptive language and flow\n"
                    "- **Enhance Dialogue**: Make conversations more natural and character-specific\n"
                    "- **Develop Characters**: Deepen character motivations and relationships\n"
                    "- **Structure Plot**: Improve story structure and pacing\n\n"
                    
                    "## Best Practices\n"
                    "- Provide a complete draft manuscript for best results\n"
                    "- Include notes about genre, audience, and style\n"
                    "- Use checkpoint system to save progress between sessions\n"
                    "- If you encounter memory issues, use the 'Clean Memory' button\n"
                    "- Export your work regularly in your preferred format\n"
                    "- Customize agent prompts to focus on specific aspects of writing\n"
                )
                
            # Move the checkpoint initialization inside the Blocks context
            demo.load(
                fn=self.get_checkpoints,
                inputs=None,
                outputs=[checkpoint_list, checkpoint_ids]
            )
            
            # Regular refresh of system stats
            def update_stats_timer():
                while True:
                    time.sleep(30)  # Update every 30 seconds
                    yield get_system_stats()
            
            demo.load(
                fn=get_system_stats,
                inputs=None,
                outputs=[sb_system_stats]
            )

        # Enable queuing for the Gradio interface to support streaming updates
        demo.queue()
        return demo

    def launch(self):
        # Use queue() for better streaming support
        self.demo.queue().launch(share=False)  # Set share to False to avoid sharing issues

def visualize_storybook_graph(graph: StateGraph):
    """Create a visual representation of the storybook graph."""
    G = nx.DiGraph()
    
    # Add nodes
    for node in graph.nodes:
        G.add_node(node)
    
    # Add edges
    for node, edges in graph.edges.items():
        for edge in edges:
            G.add_edge(node, edge)
    
    # Set up the plot
    plt.figure(figsize=(20, 20))
    pos = nx.spring_layout(G, k=2, iterations=50)
    
    # Group nodes by type
    node_colors = []
    for node in G.nodes():
        if node == "executive_director":
            color = "red"  # Executive Director
        elif "_director" in node:
            color = "orange"  # Other directors
        elif "research" in node:
            color = "blue"  # Research nodes
        elif node == END:
            color = "black"  # End node
        else:
            color = "green"  # Specialists
        node_colors.append(color)
    
    # Draw the graph with better aesthetics
    nx.draw(G, pos, with_labels=True, 
            node_color=node_colors,
            node_size=2000, 
            font_size=8, 
            font_weight='bold',
            arrows=True, 
            edge_color='gray', 
            arrowsize=20,
            font_color='white',
            edgecolors='black',
            linewidths=1.5)
    
    plt.title("Storybook Agent Workflow Graph", fontsize=24)
    return plt

# Create default model config with agent-specific configs
default_model_config = {
    "model_id": "HuggingFaceH4/zephyr-7b-beta",  # Default model
    "task": "text-generation",
    "temperature": 0.1,
    "max_new_tokens": 512,
    "do_sample": True,  # Changed from False to True
    "repetition_penalty": 1.03,
    "agent_configs": create_agent_model_configs()  # Add agent-specific configs
}

# Create the main storybook instance with agent-specific model configs
sb_instance = storybook(default_model_config)
graph = sb_instance.graph

# Create a comprehensive workflow graph that includes all agents
# This will be accessed via the langgraph server API
storybook = sb_instance.storybook_graph

# Initialize CUDA handling
init_cuda()

# Suppress NVML warnings
warnings.filterwarnings("ignore", ".*Can't initialize NVML.*")

if __name__ == "__main__":
    app = writer_gui(sb_instance)
    app.launch()