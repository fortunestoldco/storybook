import os
from dotenv import load_dotenv
import warnings
import utils
import gradio as gr

# Load environment variables from .env file or Secret Manager
_ = load_dotenv("../.env")
tavily_ai_api_key = os.getenv("TAVILY_API_KEY")
huggingface_token = os.getenv("HUGGINGFACE_API_TOKEN")
warnings.filterwarnings("ignore", message=".*TqdmWarning.*")

from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated, List, Dict, Any, Optional, Literal, cast, Union
import operator
from langgraph.checkpoint.sqlite import SqliteSaver
from langchain_core.messages import (
    SystemMessage,
    HumanMessage,
    AIMessage,
)
from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch
from huggingface_hub import login
from pydantic import BaseModel, Field
from langchain_text_splitters import RecursiveCharacterTextSplitter
from tavily import TavilyClient
import os
import sqlite3
from datetime import datetime
from uuid import uuid4
from langchain_core.runnables import RunnableConfig
from langchain_core.callbacks.manager import CallbackManager
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.tracers import LangChainTracer
from langsmith import Client as LangSmithClient

# for the output parser
from typing import List
from langchain.output_parsers import PydanticOutputParser
from langchain_core.prompts import PromptTemplate
import json
import asyncio
import time
import re
from langchain_core.runnables import Runnable, RunnableConfig
from langchain_core.globals import set_debug
import networkx as nx
import matplotlib.pyplot as plt

# Setup LangSmith tracer
langchain_tracer = None
langsmith_project_name = "storybook"
if os.getenv("LANGCHAIN_TRACING_V2") == "true" and os.getenv("LANGCHAIN_API_KEY"):
    langchain_tracer = LangChainTracer(
        project_name=langsmith_project_name,
    )
    # Set debug to get more detailed tracing
    set_debug(True)
    print("LangSmith tracing enabled")
else:
    print("LangSmith tracing not enabled. Check LANGCHAIN_API_KEY and LANGCHAIN_TRACING_V2 environment variables.")

# Initialize the callback manager for streaming
streaming_callback = StreamingStdOutCallbackHandler()
callback_manager = CallbackManager([streaming_callback])
if langchain_tracer:
    callback_manager.add_handler(langchain_tracer)

def extract_chunk_references(message: str) -> List[int]:
    """Extract chunk references from a message."""
    chunk_refs = []
    
    # Look for patterns like "Chunk 3" or "chunks 4-6"
    chunk_patterns = re.findall(r"[Cc]hunk\s+(\d+)(?:\s*-\s*(\d+))?", message)
    
    for start, end in chunk_patterns:
        start_idx = int(start)
        if end and end.strip():  # If it's a range
            end_idx = int(end)
            for i in range(start_idx, end_idx + 1):
                chunk_refs.append(i)
        else:  # If it's a single chunk
            chunk_refs.append(start_idx)
    
    return chunk_refs

# Define the project types and input states
class ProjectType:
    NEW = "new"
    EXISTING = "existing"

class NewProjectInput(TypedDict):
    title: str
    synopsis: str
    manuscript: str
    notes: Optional[Dict[str, Any]]

class ExistingProjectInput(TypedDict):
    project_id: str

class ProjectData(TypedDict):
    id: str
    title: str
    synopsis: str
    manuscript: str
    manuscript_chunks: List[Dict[str, Any]]
    notes: Optional[Dict[str, Any]]
    type: str
    quality_assessment: Dict[str, Any]
    created_at: str

class InputState(TypedDict):
    project_type: str
    project_data: Dict[str, Any]
    task: str

# Define the research state classes
class ResearchState(TypedDict):
    query: str
    results: List[Dict[str, Any]]
    summary: str

class DomainResearchState(ResearchState):
    domain_specific_data: Dict[str, Any]

class CulturalResearchState(ResearchState):
    cultural_context: Dict[str, Any]

class MarketResearchState(ResearchState):
    market_trends: Dict[str, Any]

class FactVerificationState(ResearchState):
    verification_status: Dict[str, bool]

# State for the storybook application
class AgentState(TypedDict):
    # storybook states
    project: Optional[ProjectData]
    phase: Optional[str]
    phase_history: Optional[Dict[str, List[Dict[str, Any]]]]
    current_input: Optional[Dict[str, Any]]
    messages: Optional[List[Dict[str, Any]]]
    # Tracking state
    count: Annotated[int, operator.add]
    lnode: str

class Configuration(BaseModel):
    quality_gates: Dict[str, Dict[str, Any]] = Field(default_factory=dict)
    
    @classmethod
    def from_runnable_config(cls, config: Dict[str, Any]) -> 'Configuration':
        """Extract configuration from a runnable config."""
        configurable = config.get("configurable", {})
        return cls(
            quality_gates=configurable.get("quality_gates", {})
        )

class storybookConfig(BaseModel):
    model_name: str
    temperature: float
    max_tokens: Optional[int] = None
    
    model_config = {
        "extra": "forbid"
    }

def split_manuscript(manuscript: str, chunk_size: int = 1000, chunk_overlap: int = 0) -> List[Dict[str, Any]]:
    """Split a manuscript into manageable chunks using RecursiveCharacterTextSplitter."""
    if not manuscript:
        return []
    
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    texts = text_splitter.split_text(manuscript)
    
    # Create chunks with metadata
    chunks = []
    for i, text in enumerate(texts):
        chunks.append({
            "chunk_id": i,
            "content": text,
            "start_char": manuscript.find(text),
            "end_char": manuscript.find(text) + len(text),
        })
    
    return chunks

def check_quality_gate(gate_name: str, quality_assessment: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """Check if a quality gate is passed."""
    gates = config.get("quality_gates", {})
    gate_config = gates.get(gate_name, {})
    
    if not gate_config:
        # If no gate is defined, default to passing
        return {"passed": True, "message": f"No quality gate defined for {gate_name}"}
    
    # Check each criterion in the gate
    passed = True
    reasons = []
    
    for criterion, threshold in gate_config.items():
        if criterion in quality_assessment:
            value = quality_assessment[criterion]
            if value < threshold:
                passed = False
                reasons.append(f"{criterion}: {value} (below threshold {threshold})")
        else:
            # If the criterion is not in the assessment, consider it failed
            passed = False
            reasons.append(f"{criterion}: not assessed (required)")
    
    return {
        "passed": passed,
        "message": "Quality gate passed" if passed else "Quality gate failed",
        "reasons": reasons
    }

def init_cuda():
    """Initialize CUDA and handle warnings appropriately"""
    if torch.cuda.is_available():
        try:
            # Print available GPU information
            gpu_count = torch.cuda.device_count()
            print(f"Found {gpu_count} GPU devices:")
            for i in range(gpu_count):
                device_props = torch.cuda.get_device_properties(i)
                print(f"  GPU {i}: {device_props.name}, {device_props.total_memory / 1e9:.2f} GB memory")
            
            # Initialize CUDA
            torch.cuda.init()
            print("CUDA initialized successfully")
            
            # Set appropriate memory optimization flags
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            print("TF32 enabled for matrix multiplications where supported")
            
            # Enable Flash Attention globally if available
            if hasattr(torch.backends, 'cuda') and hasattr(torch.backends.cuda, 'enable_flash_sdp'):
                torch.backends.cuda.enable_flash_sdp(True)
                print("Flash Attention enabled globally")
                
        except Exception as e:
            print(f"CUDA initialization warning (non-fatal): {str(e)}")
            # Continue execution - the warning doesn't prevent GPU usage
    else:
        print("CUDA not available - using CPU")

class AgentFactory:
    def __init__(self, config, model_config=None, tavily_client=None):
        # Check for HuggingFace token
        self.huggingface_token = os.getenv("HUGGINGFACE_API_TOKEN")
        if not self.huggingface_token:
            raise ValueError("HUGGINGFACE_API_TOKEN not found in environment variables")
        
        # Log into Hugging Face
        login(token=self.huggingface_token)
        
        # Initialize with default model config if none provided
        self.model_config = model_config or {
            "model_id": "HuggingFaceH4/zephyr-7b-beta",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": False,
            "repetition_penalty": 1.03
        }
        
        # Initialize HuggingFace pipeline
        self.model = self._create_huggingface_model(self.model_config)
        
        self.config = config
        self.tavily = tavily_client or TavilyClient(api_key=tavily_ai_api_key)
        
        # Setup callback manager for streaming and tracing
        self.callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
        if langchain_tracer:
            self.callback_manager.add_handler(langchain_tracer)
    
    def _create_huggingface_model(self, model_config):
        """Create a HuggingFacePipeline model with optimized settings for multi-GPU inference."""
        try:
            # Check for CUDA availability
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"Using device: {device}")
            
            # Set torch dtype based on device
            if device == "cuda":
                torch_dtype = torch.float16  # Use fp16 for GPU
            else:
                torch_dtype = torch.bfloat16  # Use bfloat16 for CPU
                
            # Model kwargs with optimizations for multi-GPU (4xL4) setup
            model_kwargs = {
                "device_map": "auto",  # Distribute across GPUs
                "torch_dtype": torch_dtype,
                "low_cpu_mem_usage": True,
            }
            
            # Enable Flash Attention 2 if available
            # This is especially beneficial for L4 GPUs
            model_kwargs["attn_implementation"] = "flash_attention_2"
            
            # Create pipeline with optimized settings
            llm = HuggingFacePipeline.from_model_id(
                model_id=model_config.get("model_id", "HuggingFaceH4/zephyr-7b-beta"),
                task=model_config.get("task", "text-generation"),
                pipeline_kwargs={
                    "max_new_tokens": model_config.get("max_new_tokens", 512),
                    "temperature": model_config.get("temperature", 0.1),
                    "do_sample": model_config.get("do_sample", True),
                    "repetition_penalty": model_config.get("repetition_penalty", 1.03),
                    "return_full_text": False,
                    "token": self.huggingface_token,
                },
                model_kwargs=model_kwargs
            )
            
            # Get the underlying model
            if hasattr(llm, "pipeline") and hasattr(llm.pipeline, "model"):
                model = llm.pipeline.model
                
                # Set static KV cache for better efficiency
                if hasattr(model.generation_config, 'cache_implementation'):
                    model.generation_config.cache_implementation = "static"
                    print("Static KV cache enabled for optimized inference")
                
                # Apply torch.compile for significant speedup if available
                if torch.cuda.is_available() and hasattr(torch, 'compile'):
                    try:
                        model.forward = torch.compile(
                            model.forward, 
                            mode="reduce-overhead", 
                            fullgraph=True
                        )
                        print("Model forward pass compiled with torch.compile")
                    except Exception as e:
                        print(f"Warning: Could not apply torch.compile: {e}")
            
            # Create ChatHuggingFace from the pipeline
            chat_model = ChatHuggingFace(llm=llm, callbacks=self.callback_manager)
            return chat_model
                
        except Exception as e:
            print(f"Error creating HuggingFace model: {str(e)}")
            # Fallback to a smaller model if there's an issue
            llm = HuggingFacePipeline.from_model_id(
                model_id="google/flan-t5-base",
                task="text2text-generation",
                pipeline_kwargs={
                    "max_new_tokens": 512,
                    "temperature": 0.1,
                    "token": self.huggingface_token,
                },
                model_kwargs={
                    "device_map": "auto",
                    "torch_dtype": torch.bfloat16,
                }
            )
            chat_model = ChatHuggingFace(llm=llm, callbacks=self.callback_manager)
            return chat_model
    
    def create_research_agent(self, research_type: str):
        """Create a research agent function."""
        
        def research_agent_function(state: AgentState) -> AgentState:
            """Research agent function that processes the current state."""
            project = state.get("project", {})
            current_input = state.get("current_input", {})
            
            # Extract research query from task or create one based on context
            research_query = current_input.get("research_query", "")
            if not research_query:
                task = current_input.get("task", "")
                manuscript_excerpt = ""
                if project.get("manuscript_chunks"):
                    manuscript_excerpt = project["manuscript_chunks"][0]["content"][:500]
                
                if research_type == "domain":
                    research_query = f"Technical information about: {task}"
                elif research_type == "cultural":
                    research_query = f"Cultural context related to: {task}"
                elif research_type == "market":
                    research_query = f"Market trends and audience preferences for: {task}"
                else:
                    research_query = f"Information about: {task}"
                
                # Add context from manuscript
                if manuscript_excerpt:
                    research_query += f" Context from manuscript: {manuscript_excerpt}"
            
            try:
                # Use Tavily for research
                search_result = self.tavily.search(query=research_query, search_depth="comprehensive")
                
                # Extract and format results
                results = []
                for result in search_result.get("results", []):
                    results.append({
                        "title": result.get("title", ""),
                        "content": result.get("content", ""),
                        "url": result.get("url", "")
                    })
                
                # Create a summary
                summary = search_result.get("answer", "No summary available")
                
                # Format research results
                research_results = f"RESEARCH TYPE: {research_type.upper()}\n\n"
                research_results += f"QUERY: {research_query}\n\n"
                research_results += f"SUMMARY: {summary}\n\n"
                research_results += "DETAILS:\n" + "\n\n".join(
                    [f"- {r['title']}: {r['content'][:300]}..." for r in results[:3]]
                )
                
                # Update state with research results
                updated_state = state.copy()
                updated_state["current_input"] = current_input.copy()
                updated_state["current_input"]["research_results"] = research_results
                updated_state["count"] = state.get("count", 0) + 1
                updated_state["lnode"] = f"{research_type}_research"
                
                # Add to messages for tracking
                updated_state["messages"] = state.get("messages", []) + [
                    {"role": "system", "content": f"Conducted {research_type} research on: {research_query}"},
                    {"role": "assistant", "content": research_results}
                ]
                
                return updated_state
            
            except Exception as e:
                print(f"Error conducting research: {str(e)}")
                
                # Update state with error
                updated_state = state.copy()
                updated_state["current_input"] = current_input.copy()
                updated_state["current_input"]["research_results"] = f"Research error: {str(e)}"
                updated_state["count"] = state.get("count", 0) + 1
                updated_state["lnode"] = f"{research_type}_research"
                
                # Add to messages for tracking
                updated_state["messages"] = state.get("messages", []) + [
                    {"role": "system", "content": f"Attempted {research_type} research on: {research_query}"},
                    {"role": "assistant", "content": f"Research error: {str(e)}"}
                ]
                
                return updated_state
        
        return research_agent_function
    
    def create_agent(self, agent_name: str, project_id: str):
        """Create a function for a specific agent."""
        agent_prompts = {
            "executive_director": """You are the Executive Director responsible for overall project direction. As an Executive Director, you lead a team with the following specialists: CREATIVE TEAM: - Creative Director: Responsible for artistic vision - Structure Architect: Designs story framework - Plot Development Specialist: Creates engaging plot - World Building Expert: Designs immersive settings - Character Psychology Specialist: Develops realistic characters - Character Voice Designer: Creates unique character expressions - Character Relationship Mapper: Designs character dynamics RESEARCH TEAM: - Domain Knowledge Specialist: Ensures technical accuracy - Cultural Authenticity Expert: Ensures authentic representation - Market Alignment Director: Assesses market viability EDITORIAL TEAM: - Editorial Director: Coordinates editing process - Structural Editor: Analyzes narrative structure - Prose Enhancement Specialist: Improves writing style - Dialogue Refinement Expert: Polishes dialogue FINALIZATION TEAM: - Positioning Specialist: Develops market positioning - Title/Blurb Optimizer: Creates marketing copy - Formatting Standards Expert: Prepares technical formatting Your responsibilities include: 1. Review manuscript sections and identify specific areas needing work 2. Assign SPECIFIC tasks to appropriate team members (e.g., "Creative Director, focus on improving character development in Chapter 2") 3. Make high-level decisions about project direction 4. Track quality issues and delegate them to specialists 5. Manage progression through development phases 6. Ensure quality standards are met before advancing to next phase Always be specific about which parts of the manuscript need work when delegating tasks. IMPORTANT: You must delegate tasks to specific specialists. DO NOT handle tasks yourself or delegate to another high-level director unless absolutely necessary. When you identify an issue or area for improvement, immediately delegate it to the most appropriate specialist from the list above.""", 
            
            "creative_director": """You are the Creative Director responsible for artistic vision. You work with the Executive Director and oversee the following team members: - Structure Architect: Designs story framework - Plot Development Specialist: Creates engaging plot - World Building Expert: Designs immersive settings - Character Psychology Specialist: Develops realistic characters - Character Voice Designer: Creates unique character expressions - Character Relationship Mapper: Designs character dynamics As a Creative Director, you should: 1. Guide creative direction based on Executive Director's instructions 2. Ensure artistic cohesion across the entire manuscript 3. Identify specific creative issues in the manuscript and delegate to specialists 4. Coordinate creative elements between team members 5. Maintain creative standards 6. Balance innovation and conventions IMPORTANT: You must delegate tasks to specific specialists. DO NOT handle tasks yourself or return to the Executive Director without delegating first. When you identify an issue or area for improvement, immediately delegate it to the most appropriate specialist from your team. Only return to the Executive Director after delegating appropriately.""", 
            
            "structure_architect": """You are the Structure Architect responsible for story framework. You report to the Creative Director and specialize in narrative structure. As a Structure Architect, you should: 1. Design and refine story structure based on specific manuscript sections 2. Identify structural weaknesses in assigned chapters/sections 3. Organize story elements for maximum impact 4. Ensure structural integrity across the narrative 5. Balance pacing and flow in specific sections 6. Provide concrete suggestions for structural improvements Focus on the specific manuscript sections you are assigned to analyze, and provide actionable recommendations.""",
            "plot_development_specialist": """You are the Plot Development Specialist responsible for plot design.
            You report to the Creative Director and specialize in plot construction. As a Plot Development Specialist, you should: 1. Design engaging plot structures for specific assigned sections 2. Create compelling story arcs when requested 3. Develop effective plot points for particular chapters 4. Balance pacing and tension in identified manuscript sections 5. Ensure plot coherence across assigned sections 6. Provide specific plot improvements for assigned sections Focus on the exact manuscript sections you are assigned to review, and provide actionable recommendations.""", 
            
            "world_building_expert": """You are the World Building Expert responsible for creating rich, believable settings.
            You report to the Creative Director and specialize in world creation. As a World Building Expert, you should: 1. Design coherent and immersive world systems for assigned sections 2. Develop cultural, social, and physical environments in specific chapters 3. Create consistent rules and logic for the story world 4. Ensure authenticity in setting details 5. Balance world complexity with story needs 6. Integrate setting elements naturally into identified manuscript sections Focus on the specific manuscript sections you are assigned to analyze, and provide actionable recommendations.""", 
            
            "character_psychology_specialist": """You are the Character Psychology Specialist responsible for deep character development.
            You report to the Creative Director and specialize in character psychology. As a Character Psychology Specialist, you should: 1. Design psychologically realistic characters in assigned manuscript sections 2. Develop complex motivations and internal conflicts for specific characters 3. Create believable character growth arcs 4. Ensure consistent character behaviors in the sections you review 5. Design meaningful character transformations 6. Balance internal and external conflicts Focus on the specific characters and manuscript sections you are assigned, and provide actionable recommendations.""", 
            
            "character_voice_designer": """You are the Character Voice Designer responsible for unique character expressions.
            You report to the Creative Director and specialize in character dialogue and voice. As a Character Voice Designer, you should: 1. Create distinct voices for each character in assigned manuscript sections 2. Develop consistent speech patterns for specific characters 3. Reflect character backgrounds in dialogue 4. Ensure authentic character expressions in the sections you review 5. Balance dialogue style with readability 6. Maintain voice consistency across assigned chapters Focus on the specific characters and manuscript sections you are assigned, and provide actionable recommendations.""", 
            
            "character_relationship_mapper": """You are the Character Relationship Mapper responsible for character dynamics.
            You report to the Creative Director and specialize in character relationships. As a Character Relationship Mapper, you should: 1. Design complex character relationships in assigned manuscript sections 2. Map character interactions and dynamics 3. Develop relationship arcs for specific character pairs 4. Ensure realistic relationship evolution 5. Create meaningful connections in the sections you review 6. Balance relationship complexity Focus on the specific characters and manuscript sections you are assigned, and provide actionable recommendations.""", 
            
            "domain_knowledge_specialist": """You are the Domain Knowledge Specialist responsible for technical accuracy.
            You report to the Executive Director and specialize in research and fact-checking. As a Domain Knowledge Specialist, you should: 1. Verify technical and specialized content in assigned manuscript sections 2. Research domain-specific details when requested 3. Ensure accurate representation of professional fields 4. Provide expert knowledge integration 5. Balance accuracy with readability 6. Maintain consistency in technical elements across assigned sections When you receive a research request, conduct the necessary research and provide factual context to the requesting specialist. Focus on the specific manuscript sections you are assigned, and provide actionable recommendations.""", 
            
            "cultural_authenticity_expert": """You are the Cultural Authenticity Expert responsible for cultural representation.
            You report to the Executive Director and specialize in cultural accuracy. As a Cultural Authenticity Expert, you should: 1. Ensure authentic cultural representation in assigned manuscript sections 2. Verify cultural details and practices 3. Prevent stereotypes and misrepresentation 4. Provide cultural context and nuance 5. Balance authenticity with accessibility 6. Maintain cultural sensitivity in all content you review When you receive a research request, conduct the necessary cultural research and provide context to the requesting specialist. Focus on the specific manuscript sections you are assigned, and provide actionable recommendations.""", 
            
            "market_alignment_director": """You are the Market Alignment Director responsible for market viability.
            You report directly to the Executive Director and specialize in commercial aspects. As a Market Alignment Director, you should: 1. Analyze current market trends and audience preferences for specific genres/categories 2. Evaluate commercial potential of assigned manuscript sections 3. Guide market positioning strategy 4. Ensure alignment with target audience expectations 5. Balance artistic vision with market demands 6. Provide strategic recommendations for specific improvements When you receive a research request, conduct market research and provide context to the requesting specialist. Focus on the specific manuscript sections you are assigned, and provide actionable recommendations.""", 
            
            "content_development_director": """You are the Content Development Director responsible for content creation.
            You report to the Executive Director and oversee the following team members: - Chapter Drafters: Create chapter content - Scene Construction Specialists: Build effective scenes - Dialogue Crafters: Write natural dialogue - Continuity Manager: Ensure consistency - Voice Consistency Monitor: Maintain narrative voice - Emotional Arc Designer: Create emotional journeys As a Content Development Director, you should: 1. Guide overall content development based on Executive Director's instructions 2. Ensure consistency in content creation across assigned manuscript sections 3. Delegate specific content tasks to your team members 4. Maintain quality standards in all content 5. Balance creativity with structure 6. Guide content integration IMPORTANT: You must delegate tasks to specific specialists. DO NOT handle tasks yourself or return to the Executive Director without delegating first. When you identify an issue or area for improvement, immediately delegate it to the most appropriate specialist from your team. Only return to the Executive Director after delegating appropriately.""", 
            
            "chapter_drafters": """You are the Chapter Drafter responsible for chapter creation.
            You report to the Content Development Director and specialize in chapter-level content. As a Chapter Drafter, you should: 1. Create engaging chapter content for assigned sections 2. Structure chapter flow and pacing 3. Develop chapter hooks and endings 4. Ensure chapter coherence 5. Balance chapter length and content 6. Maintain narrative momentum Focus on the specific manuscript sections you are assigned to develop, and provide actionable content.""", 
            
            "scene_construction_specialists": """You are the Scene Construction Specialist responsible for scene creation.
            You report to the Content Development Director and specialize in scene development. As a Scene Construction Specialist, you should: 1. Craft engaging scenes in assigned manuscript sections 2. Build proper scene structure for specific story moments 3. Create effective scene transitions 4. Balance scene elements (description, action, dialogue) 5. Maintain scene tension 6. Ensure each scene has a clear purpose Focus on the specific manuscript sections you are assigned to develop, and provide actionable content.""", 
            
            "dialogue_crafters": """You are the Dialogue Crafter responsible for conversation creation.
            You report to the Content Development Director and specialize in dialogue. As a Dialogue Crafter, you should: 1. Write natural dialogue for assigned manuscript sections 2. Create meaningful conversations between specific characters 3. Balance dialogue with action 4. Maintain character voices as defined by the Character Voice Designer 5. Use dialogue for story progression 6. Ensure dialogue authenticity Focus on the specific manuscript sections and characters you are assigned, and provide actionable content.""", 
            
            "continuity_manager": """You are the Continuity Manager responsible for story consistency.
            You report to the Content Development Director and specialize in narrative continuity. As a Continuity Manager, you should: 1. Track plot and character continuity across assigned manuscript sections 2. Maintain world consistency between chapters 3. Verify timeline accuracy 4. Check detail consistency (names, places, objects, etc.) 5. Identify specific continuity issues in assigned sections 6. Ensure logical progression Focus on the specific manuscript sections you are assigned to review, and provide actionable recommendations.""", 
            
            "voice_consistency_monitor": """You are the Voice Consistency Monitor responsible for narrative voice.
            You report to the Content Development Director and specialize in voice consistency. As a Voice Consistency Monitor, you should: 1. Maintain consistent narrative voice across assigned manuscript sections 2. Track character voice consistency between chapters 3. Ensure tonal consistency in specific sections 4. Monitor style consistency 5. Balance voice variations 6. Guide voice development Focus on the specific manuscript sections you are assigned to review, and provide actionable recommendations.""", 
            
            "emotional_arc_designer": """You are the Emotional Arc Designer responsible for emotional journeys.
            You report to the Content Development Director and specialize in emotional storytelling. As an Emotional Arc Designer, you should: 1. Create compelling emotional arcs in assigned manuscript sections 2. Design character emotional growth for specific moments 3. Develop emotional resonance in key scenes 4. Balance emotional intensity across chapters 5. Ensure emotional authenticity 6. Guide emotional progression Focus on the specific manuscript sections and characters you are assigned, and provide actionable recommendations.""", 
            
            "editorial_director": """You are the Editorial Director responsible for overall editing.
            You report to the Executive Director and oversee the following team members: - Structural Editor: Analyzes narrative structure - Character Arc Evaluator: Assesses character development - Thematic Coherence Analyst: Ensures theme consistency - Prose Enhancement Specialist: Improves writing style - Dialogue Refinement Expert: Polishes dialogue - Rhythm Cadence Optimizer: Ensures prose flow - Grammar Consistency Checker: Maintains technical accuracy - Fact Verification Specialist: Ensures factual accuracy As an Editorial Director, you should: 1. Guide editorial strategy based on Executive Director's instructions 2. Coordinate editing processes for assigned manuscript sections 3. Delegate specific editing tasks to your team 4. Maintain quality standards in all edits 5. Balance different edit types 6. Guide revision process IMPORTANT: You must delegate tasks to specific specialists. DO NOT handle tasks yourself or return to the Executive Director without delegating first. When you identify an issue or area for improvement, immediately delegate it to the most appropriate specialist from your team. Only return to the Executive Director after delegating appropriately.""", 
            
            "structural_editor": """You are the Structural Editor responsible for story structure.
            You report to the Editorial Director and specialize in narrative structure. As a Structural Editor, you should: 1. Analyze overall structure of assigned manuscript sections 2. Identify structural issues in specific chapters 3. Suggest structural improvements for particular sections 4. Balance story elements 5. Ensure logical flow 6. Guide structural revisions Focus on the specific manuscript sections you are assigned to review, and provide actionable recommendations.""", 
            
            "character_arc_evaluator": """You are the Character Arc Evaluator responsible for character development.
            You report to the Editorial Director and specialize in character arcs. As a Character Arc Evaluator, you should: 1. Assess character arcs in assigned manuscript sections 2. Evaluate character growth for specific characters 3. Identify character inconsistencies in particular chapters 4. Suggest character improvements 5. Ensure arc completion 6. Guide character revisions Focus on the specific characters and manuscript sections you are assigned, and provide actionable recommendations.""", 
            
            "thematic_coherence_analyst": """You are the Thematic Coherence Analyst responsible for theme development.
            You report to the Editorial Director and specialize in thematic analysis. As a Thematic Coherence Analyst, you should: 1. Analyze thematic elements in assigned manuscript sections 2. Ensure theme consistency across chapters 3. Strengthen theme development in specific sections 4. Balance theme presentation 5. Identify theme opportunities 6. Guide theme integration Focus on the specific manuscript sections you are assigned to review, and provide actionable recommendations.""", 
            
            "prose_enhancement_specialist": """You are the Prose Enhancement Specialist responsible for writing quality.
            You report to the Editorial Director and specialize in prose improvement. As a Prose Enhancement Specialist, you should: 1. Improve writing style in assigned manuscript sections 2. Enhance prose quality in specific paragraphs 3. Strengthen descriptions in particular scenes 4. Polish language use 5. Balance prose elements 6. Guide prose refinement Focus on the specific manuscript sections you are assigned to enhance, and provide actionable improvements.""", 
            
            "dialogue_refinement_expert": """You are the Dialogue Refinement Expert responsible for dialogue quality.
            You report to the Editorial Director and specialize in dialogue improvement. As a Dialogue Refinement Expert, you should: 1. Polish dialogue in assigned manuscript sections 2. Improve conversation flow between specific characters 3. Enhance character voices in particular scenes 4. Strengthen dialogue impact 5. Balance dialogue elements 6. Guide dialogue revision Focus on the specific manuscript sections and character conversations you are assigned, and provide actionable improvements.""", 
            
            "rhythm_cadence_optimizer": """You are the Rhythm Cadence Optimizer responsible for prose flow.
            You report to the Editorial Director and specialize in prose rhythm. As a Rhythm Cadence Optimizer, you should: 1. Analyze prose rhythm in assigned manuscript sections 2. Improve sentence flow in specific paragraphs 3. Balance pacing through sentence structure 4. Enhance readability 5. Optimize word patterns 6. Guide rhythm refinement Focus on the specific manuscript sections you are assigned to optimize, and provide actionable improvements.""", 
            
            "grammar_consistency_checker": """You are the Grammar Consistency Checker responsible for technical accuracy.
            You report to the Editorial Director and specialize in grammar and consistency. As a Grammar Consistency Checker, you should: 1. Verify grammar rules in assigned manuscript sections 2. Ensure consistent usage of style elements 3. Check punctuation in specific passages 4. Maintain style guidelines 5. Identify pattern issues 6. Guide technical corrections Focus on the specific manuscript sections you are assigned to check, and provide actionable corrections.""", 
            
            "fact_verification_specialist": """You are the Fact Verification Specialist responsible for accuracy.
            You report to the Editorial Director and specialize in fact-checking. As a Fact Verification Specialist, you should: 1. Verify factual content in assigned manuscript sections 2. Check research accuracy for specific topics 3. Validate references in particular passages 4. Ensure detail accuracy 5. Identify fact issues 6. Guide fact correction When you identify facts that need research, coordinate with the Domain Knowledge Specialist. Focus on the specific manuscript sections you are assigned, and provide actionable corrections.""", 
            
            "positioning_specialist": """You are the Positioning Specialist responsible for market positioning.
            You report to the Market Alignment Director and specialize in market positioning. As a Positioning Specialist, you should: 1. Analyze market position of the project based on manuscript content 2. Identify target audience for specific content elements 3. Develop positioning strategy 4. Guide marketing approach 5. Ensure market fit 6. Balance unique elements with market expectations Focus on the specific manuscript sections you are assigned to analyze, and provide actionable recommendations.""", 
            
            "title_blurb_optimizer": """You are the Title/Blurb Optimizer responsible for marketing copy.
            You report to the Market Alignment Director and specialize in promotional text. As a Title/Blurb Optimizer, you should: 1. Optimize title appeal based on manuscript content 2. Craft compelling blurbs that accurately represent the story 3. Create marketing hooks 4. Ensure accurate representation of the manuscript in marketing materials 5. Balance appeal with authenticity 6. Guide marketing language development Focus on the specific sections or elements you are assigned to optimize, and provide actionable copy.""", 
            
            "differentiation_strategist": """You are the Differentiation Strategist responsible for unique positioning.
            You report to the Market Alignment Director and specialize in market differentiation. As a Differentiation Strategist, you should: 1. Identify unique elements in the manuscript 2. Develop differentiation strategy based on content strengths 3. Guide unique positioning 4. Balance uniqueness with market expectations 5. Ensure market distinction 6. Guide positioning approach Focus on the specific manuscript elements you are assigned to analyze, and provide actionable recommendations.""", 
            
            "formatting_standards_expert": """You are the Formatting Standards Expert responsible for technical preparation.
            You report to the Editorial Director and specialize in formatting. As a Formatting Standards Expert, you should: 1. Verify formatting standards in assigned manuscript sections 2. Ensure technical compliance with publishing requirements 3. Check layout elements 4. Maintain consistency in formatting 5. Guide technical preparation 6. Balance format requirements with readability Focus on the specific manuscript sections you are assigned to format, and provide actionable recommendations."""
        }
        
        if agent_name not in agent_prompts:
            raise ValueError(f"Unknown agent: {agent_name}")
        
        # Create a function that uses the agent's prompt to process inputs
        def agent_function(state: AgentState) -> AgentState:
            """Agent function that processes the current state."""
            project = state.get("project", {})
            current_input = state.get("current_input", {})
            
            # Get relevant manuscript content
            manuscript_excerpt = ""
            section_to_review = current_input.get("section_to_review", "")
            chapter_to_review = current_input.get("chapter_to_review", "")
            referenced_chunks = current_input.get("referenced_chunks", [])
            
            if project.get("manuscript_chunks"):
                # If we have a specific section or chapter to review, try to find it
                if section_to_review or chapter_to_review:
                    search_term = section_to_review or chapter_to_review
                    # Convert to lowercase for case-insensitive search
                    search_term_lower = search_term.lower()
                    
                    # Search through chunks for the specified section
                    for chunk in project["manuscript_chunks"]:
                        if search_term_lower in chunk["content"].lower():
                            manuscript_excerpt = chunk["content"]
                            break
                    
                    # If nothing was found but we're the executive director, include more context
                    if not manuscript_excerpt and agent_name == "executive_director":
                        # For executive director, provide a larger portion of the manuscript
                        manuscript_excerpt = "\n\n".join([
                            f"Chunk {chunk['chunk_id']}: {chunk['content']}"
                            for chunk in project["manuscript_chunks"][:10]  # Increased from 3 to 10 chunks
                        ])
                # If specific chunks were referenced in the delegation
                elif referenced_chunks:
                    manuscript_excerpt = "\n\n".join([
                        f"Chunk {chunk['chunk_id']}: {chunk['content']}"
                        for chunk in project["manuscript_chunks"]
                        if chunk["chunk_id"] in referenced_chunks
                    ])
                else:
                    # For executive director, provide comprehensive manuscript view
                    if agent_name == "executive_director":
                        # Provide a larger portion of the manuscript with chunk identifiers
                        manuscript_excerpt = "\n\n".join([
                            f"Chunk {chunk['chunk_id']}: {chunk['content']}"
                            for chunk in project["manuscript_chunks"][:10]  # Increased from 3 to 10 chunks
                        ])
                    # For other directors, provide moderate context
                    elif agent_name in ["creative_director", "editorial_director", "content_development_director", "market_alignment_director"]:
                        manuscript_excerpt = "\n\n".join([
                            f"Chunk {chunk['chunk_id']}: {chunk['content']}"
                            for chunk in project["manuscript_chunks"][:5]  # Moderate amount for other directors
                        ])
                    else:
                        # For specialists, just the first chunk as a sample if no specific chunks are referenced
                        manuscript_excerpt = f"Chunk 0: {project['manuscript_chunks'][0]['content']}"
            
            # Create the context string
            context = (
                f"Project ID: {project.get('id', project_id)}\n"
                f"Title: {project.get('title', 'Untitled')}\n"
                f"Synopsis: {project.get('synopsis', 'No synopsis provided')}\n"
                f"Current task: {current_input.get('task', 'No task specified')}\n"
            )
            
            # Add research context if available
            research_context = ""
            if "research_results" in current_input:
                research_context = f"\nResearch Results:\n{current_input['research_results']}\n"
            
            # Make sure we have valid content
            system_content = agent_prompts[agent_name].strip()
            if not system_content:
                system_content = "You are an AI assistant helping with a writing project."
            
            # For executive director, add specific instruction to provide detailed feedback
            if agent_name == "executive_director":
                system_content += "\n\nIMPORTANT ADDITIONAL INSTRUCTIONS: You must thoroughly analyze the manuscript chunks provided below. For each chunk, identify specific issues, strengths, and provide detailed feedback. Then delegate SPECIFIC tasks to appropriate specialists. Always reference the exact chunks (e.g., 'Chunk 3 needs work on character development') when delegating tasks."
            
            # For specialized agents, add context about previous director instructions
            if agent_name not in ["executive_director", "creative_director", "editorial_director", "content_development_director", "market_alignment_director"]:
                # Find the last message from a director
                director_instructions = []
                for msg in reversed(state.get("messages", [])):
                    if msg.get("role") == "assistant" and any(director in msg.get("content", "") for director in ["Executive Director", "Creative Director", "Editorial Director", "Content Development Director", "Market Alignment Director"]):
                        director_instructions.append(msg.get("content", ""))
                        # Only get the most recent director message
                        break
                
                # Add the director's instructions to the system prompt
                if director_instructions:
                    system_content += "\n\nRecent director instructions:\n" + "\n".join(director_instructions)
            
            task = current_input.get('task', 'No task specified')
            human_content = (
                f"Task: {task}\n\n"
                f"Context:\n{context}\n"
                f"{research_context}"
                f"\nManuscript section to review:\n{manuscript_excerpt[:5000] if manuscript_excerpt else 'No specific manuscript section provided'}"
            ).strip()
            if not human_content:
                human_content = "Please help with this writing project."
            
            # Combine system and human content into a single message
            combined_content = f"{system_content}\n\n{human_content}"
            
            # Create a single HumanMessage with the combined content
            user_message = HumanMessage(content=combined_content)
            
            # Setup LangSmith trace
            config = RunnableConfig()
            if langchain_tracer:
                config["callbacks"] = [langchain_tracer]
                config["tags"] = [f"agent:{agent_name}", f"project:{project_id}", f"phase:{state.get('phase', 'unknown')}"]
                # Set up metadata for the trace
                config["metadata"] = {
                    "agent_name": agent_name,
                    "project_id": project_id,
                    "phase": state.get("phase", "unknown"),
                    "task": task
                }
            
            # Invoke the model with just a single message and tracing
            try:
                response = self.model.invoke([user_message], config=config)
            except Exception as e:
                print(f"Error invoking model: {str(e)}")
                response = AIMessage(content=f"Error: {str(e)}")
            
            # Update state
            updated_state = state.copy()
            updated_state["messages"] = state.get("messages", []) + [
                {"role": "user", "content": combined_content},
                {"role": "assistant", "content": response.content if hasattr(response, 'content') else str(response)}
            ]
            updated_state["count"] = state.get("count", 0) + 1
            updated_state["lnode"] = agent_name
            
            return updated_state
        
        return agent_function

def create_phase_graph(phase: str, project_id: str, config: Dict[str, Any], model_config=None) -> StateGraph:
    """Create a graph for a specific phase of the storybook process."""
    # Create an agent factory with model configuration
    agent_factory = AgentFactory(config, model_config=model_config)
    
    # Define available agents for each phase
    agents = {
        "initialization": [
            "executive_director",
            "creative_director",
            "market_alignment_director",
            "domain_knowledge_specialist",  # Add these specialists
            "cultural_authenticity_expert", # to initialization phase
            "positioning_specialist",
            "title_blurb_optimizer",
            "differentiation_strategist"
        ],
        "development": [
            "executive_director", "creative_director", "structure_architect",
            "plot_development_specialist", "world_building_expert",
            "character_psychology_specialist", "character_voice_designer",
            "character_relationship_mapper", "domain_knowledge_specialist",
            "cultural_authenticity_expert", "market_alignment_director"
        ],
        "creation": [
            "executive_director", "creative_director", "content_development_director",
            "chapter_drafters", "scene_construction_specialists", "dialogue_crafters",
            "continuity_manager", "voice_consistency_monitor", "emotional_arc_designer",
            "domain_knowledge_specialist"
        ],
        "refinement": [
            "executive_director", "creative_director", "editorial_director",
            "structural_editor", "character_arc_evaluator", "thematic_coherence_analyst",
            "prose_enhancement_specialist", "dialogue_refinement_expert",
            "rhythm_cadence_optimizer", "grammar_consistency_checker",
            "fact_verification_specialist", "market_alignment_director"
        ],
        "finalization": [
            "executive_director", "editorial_director", "market_alignment_director",
            "positioning_specialist", "title_blurb_optimizer",
            "differentiation_strategist", "formatting_standards_expert"
        ]
    }
    
    # Create the graph builder with SQLite checkpoint saver
    checkpoint_dir = os.path.join(os.getcwd(), "checkpoints")
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint_file = os.path.join(checkpoint_dir, f"{project_id}_{phase}.sqlite")
    
    # Create state graph with checkpoints
    builder = StateGraph(AgentState)
    builder.add_checkpoint_saver(
        SqliteSaver(checkpoint_file)
    )
    
    # Add agent nodes to the graph for this phase
    phase_agents = agents.get(phase, [])
    for agent_name in phase_agents:
        builder.add_node(agent_name, agent_factory.create_agent(agent_name, project_id))
    
    # Add research nodes
    builder.add_node("domain_research", agent_factory.create_research_agent("domain"))
    builder.add_node("cultural_research", agent_factory.create_research_agent("cultural"))
    builder.add_node("market_research", agent_factory.create_research_agent("market"))
    
    # Add the starting node - all phases start with executive director
    builder.set_entry_point("executive_director")
    
    # Define phase-specific routing
    if phase == "initialization":
        def route_after_executive_director(state: AgentState) -> str:
            """Route after the executive director node in initialization phase."""
            task = state.get("current_input", {}).get("task", "").lower()
            messages = state.get("messages", [])
            
            # Get the last assistant message if there is one
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if executive director specified research needs or delegated to specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the executive director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection - look for explicit delegation patterns
                if "domain knowledge specialist" in last_message_lower or "technical accuracy" in last_message_lower:
                    return "domain_knowledge_specialist"
                
                if "cultural authenticity expert" in last_message_lower or "cultural representation" in last_message_lower:
                    return "cultural_authenticity_expert"
                
                if "market alignment director" in last_message_lower or "market viability" in last_message_lower:
                    return "market_alignment_director"
                
                if "creative director" in last_message_lower:
                    return "creative_director"
                
                # Check for research instructions more broadly
                if ("research" in last_message_lower or "investigate" in last_message_lower):
                    if "market" in last_message_lower or "audience" in last_message_lower:
                        return "market_research"
                    elif "cultural" in last_message_lower or "heritage" in last_message_lower:
                        return "cultural_research"
                    else:
                        return "domain_research"
            
            # Count agent visits to prevent loops
            exec_visits = sum(1 for msg in messages if 
                             msg.get("role") == "user" and
                             "Executive Director" in msg.get("content", ""))
            
            # Default routing based on task keywords if no delegation was detected
            if "creative" in task or "story" in task:
                return "creative_director"
            elif "market" in task or "audience" in task:
                return "market_alignment_director"
            elif "research" in task or "information" in task:
                return "domain_research"
            elif exec_visits > 5:  # If we've visited executive_director too many times
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                
                # Force quality assessment update
                quality_assessment["planning_quality"] = 0.8
                quality_assessment["market_alignment"] = 0.7
                project["quality_assessment"] = quality_assessment
                
                return END
            else:
                # Check quality gate to possibly end this phase
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                gate_result = check_quality_gate(
                    "initialization_to_development",
                    quality_assessment,
                    {"quality_gates": config.get("quality_gates", {})}
                )
                
                if gate_result["passed"]:
                    return END
                else:
                    # If we've been to the executive director multiple times without delegation, try creative
                    if exec_visits > 2:
                        return "creative_director"
                    else:
                        # When in doubt, stick with executive director for another iteration
                        return "market_alignment_director"

        def route_after_creative_director(state: AgentState) -> str:
            """Route after the creative director in initialization phase."""
            messages = state.get("messages", [])
            
            # Get the last assistant message if there is one
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if creative director referred to specific needs or specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the creative director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection - look for specialist references
                if "structure architect" in last_message_lower:
                    return "structure_architect"
                
                if "plot development specialist" in last_message_lower:
                    return "plot_development_specialist"
                
                if "world building expert" in last_message_lower:
                    return "world_building_expert"
                
                if "character psychology specialist" in last_message_lower:
                    return "character_psychology_specialist"
                
                if "character voice designer" in last_message_lower:
                    return "character_voice_designer"
                
                if "character relationship mapper" in last_message_lower:
                    return "character_relationship_mapper"
                
                # Check for market or research needs
                if "market" in last_message_lower or "audience" in last_message_lower:
                    return "market_alignment_director"
                
                if "research" in last_message_lower or "domain knowledge" in last_message_lower:
                    return "domain_research"
            
            # Count agent visits to prevent loops
            creative_visits = sum(1 for msg in messages if
                                msg.get("role") == "user" and
                                "Creative Director" in msg.get("content", ""))
            
            if creative_visits > 2:  # If we've been to creative_director too many times
                # Go back to executive_director to reassess
                return "executive_director"
            else:
                # Stay with creative director for one more iteration to encourage delegation
                return "executive_director"
            
        def route_after_market_alignment_director(state: AgentState) -> str:
            """Route after the market alignment director in initialization phase."""
            messages = state.get("messages", [])
            
            # Check if market director wants to do research or delegates
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the market director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection
                if "positioning specialist" in last_message_lower:
                    return "positioning_specialist"
                
                if "title/blurb optimizer" in last_message_lower or "marketing copy" in last_message_lower:
                    return "title_blurb_optimizer"
                
                if "differentiation strategist" in last_message_lower:
                    return "differentiation_strategist"
                
                # Check for research needs
                if "research" in last_message_lower or "investigate" in last_message_lower:
                    return "market_research"
            
            # Count visits to market alignment director
            market_visits = sum(1 for msg in messages if
                               msg.get("role") == "user" and
                               "Market Alignment Director" in msg.get("content", ""))
            
            if market_visits > 2:
                # Return to executive director after a few visits
                return "executive_director"
            else:
                # Return to executive director
                return "executive_director"
            
        def route_after_research(state: AgentState) -> str:
            """Route after research nodes."""
            # Route back to the appropriate specialist based on research type
            research_type = state.get("lnode", "")
            
            if research_type == "domain_research":
                return "domain_knowledge_specialist"
            elif research_type == "cultural_research":
                return "cultural_authenticity_expert"
            elif research_type == "market_research":
                return "market_alignment_director"
            else:
                return "executive_director"
            
        # Add conditional edges
        builder.add_conditional_edges(
            "executive_director",
            route_after_executive_director
        )
        
        builder.add_conditional_edges(
            "creative_director",
            route_after_creative_director
        )
        
        builder.add_conditional_edges(
            "market_alignment_director",
            route_after_market_alignment_director
        )
        
        # Add research routing
        builder.add_conditional_edges(
            "domain_research",
            route_after_research
        )
        
        builder.add_conditional_edges(
            "cultural_research",
            route_after_research
        )
        
        builder.add_conditional_edges(
            "market_research",
            route_after_research
        )
        
        # Only add edges if both source and target nodes exist in this phase
        if "domain_knowledge_specialist" in phase_agents:
            builder.add_edge("domain_knowledge_specialist", "executive_director")
    
    elif phase == "development":
        def route_after_executive_director(state: AgentState) -> str:
            """Route after the executive director node in development phase."""
            task = state.get("current_input", {}).get("task", "").lower()
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if executive director specified different specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the executive director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "creative director", "structure architect", "plot development specialist",
                    "world building expert", "character psychology specialist",
                    "character voice designer", "character relationship mapper",
                    "domain knowledge specialist", "cultural authenticity expert"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
                
                # Check for market alignment
                if "market" in last_message_lower:
                    return "market_alignment_director"
                
                # Check for research needs
                if "research" in last_message_lower:
                    if "market" in last_message_lower:
                        return "market_research"
                    elif "cultural" in last_message_lower:
                        return "cultural_research"
                    else:
                        return "domain_research"
            
            # Count executive director visits to prevent loops
            exec_visits = sum(1 for msg in messages if
                              msg.get("role") == "user" and
                              "Executive Director" in msg.get("content", ""))

            # Default routing based on task keywords
            if "creative" in task or "story" in task:
                return "creative_director"
            elif "market" in task or "trend" in task:
                return "market_alignment_director"
            elif "research" in task or "knowledge" in task:
                return "domain_knowledge_specialist"
            elif exec_visits > 5:  # If we've visited executive_director too many times
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                quality_assessment["structure_quality"] = 0.8
                quality_assessment["character_depth"] = 0.8
                quality_assessment["world_building"] = 0.8
                project["quality_assessment"] = quality_assessment
                
                return END
            else:
                # Check quality gate to possibly end this phase
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                gate_result = check_quality_gate(
                    "development_to_creation",
                    quality_assessment,
                    {"quality_gates": config.get("quality_gates", {})}
                )
                if gate_result["passed"]:
                    return END
                else:
                    # If we've been to executive several times, try creative director
                    if exec_visits > 2:
                        return "creative_director"
                    else:
                        # Try structure architect for a change to encourage more diverse delegation
                        return "structure_architect"

        def route_after_creative_director(state: AgentState) -> str:
            """Route after the creative director node."""
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if creative director delegated to a specific specialist
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the creative director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "structure architect", "plot development specialist",
                    "world building expert", "character psychology specialist",
                    "character voice designer", "character relationship mapper"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
                
                # Check for research or domain knowledge needs
                if "domain knowledge" in last_message_lower or "research" in last_message_lower:
                    return "domain_knowledge_specialist"
            
            # Count creative director visits
            creative_visits = sum(1 for msg in messages if
                                msg.get("role") == "user" and
                                "Creative Director" in msg.get("content", ""))
                
            
            if creative_visits > 2:
                # After a few visits, if no delegation, return to executive
                return "executive_director"
            else:
                # Give one more chance for delegation
                return "structure_architect"

        # Add routing for research nodes
        def route_after_research(state: AgentState) -> str:
            """Route after research nodes."""
            research_type = state.get("lnode", "")
            
            if research_type == "domain_research":
                return "domain_knowledge_specialist"
            elif research_type == "cultural_research":
                return "cultural_authenticity_expert"
            elif research_type == "market_research":
                return "market_alignment_director"
            else:
                return "executive_director"

        builder.add_conditional_edges(
            "executive_director",
            route_after_executive_director
        )
        builder.add_conditional_edges(
            "creative_director",
            route_after_creative_director
        )
        
        # Add research routing
        builder.add_conditional_edges(
            "domain_research",
            route_after_research
        )
        
        builder.add_conditional_edges(
            "cultural_research",
            route_after_research
        )
        
        builder.add_conditional_edges(
            "market_research",
            route_after_research
        )
        # Connect domain knowledge specialist to domain research
        builder.add_edge("domain_knowledge_specialist", "domain_research")
        builder.add_edge("cultural_authenticity_expert", "cultural_research")
        builder.add_edge("market_alignment_director", "market_research")
        # Connect specialized agents to their supervisors
        for agent in ["structure_architect", "plot_development_specialist",
                     "world_building_expert", "character_psychology_specialist",
                     "character_voice_designer", "character_relationship_mapper"]:
            if agent in agents[phase]:  # Only add edges for agents that exist in this phase
                builder.add_edge(agent, "creative_director")
        for agent in ["domain_knowledge_specialist", "cultural_authenticity_expert"]:
            if agent in agents[phase]:  # Only add edges for agents that exist in this phase
                builder.add_edge(agent, "executive_director")
            
    elif phase == "creation":
        def route_after_executive_director(state: AgentState) -> str:
            """Route after the executive director node in creation phase."""
            task = state.get("current_input", {}).get("task", "").lower()
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if executive director specified different specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the executive director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "content development director", "creative director",
                    "chapter drafter", "scene construction specialist",
                    "dialogue crafter", "continuity manager",
                    "voice consistency monitor", "emotional arc designer"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
                
                # Check for domain knowledge or research
                if "domain knowledge" in last_message_lower or "research" in last_message_lower:
                    return "domain_knowledge_specialist"
            
            # Count executive director visits to prevent loops
            exec_visits = sum(1 for msg in messages if
                              msg.get("role") == "user" and
                              "Executive Director" in msg.get("content", ""))
            # Default routing based on task keywords
            if "content" in task or "development" in task:
                return "content_development_director"
            elif "creative" in task:
                return "creative_director"
            elif "research" in task:
                return "domain_knowledge_specialist"
            elif exec_visits > 5:  # If we've visited executive_director too many times
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                quality_assessment["content_quality"] = 0.8
                quality_assessment["narrative_flow"] = 0.8
                quality_assessment["dialogue_quality"] = 0.8
                project["quality_assessment"] = quality_assessment
                
                return END
            else:
                # Check quality gate to possibly end this phase
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                gate_result = check_quality_gate(
                    "creation_to_refinement",
                    quality_assessment,
                    {"quality_gates": config.get("quality_gates", {})}
                )
                if gate_result["passed"]:
                    return END
                else:
                    # Force delegation after a few iterations
                    if exec_visits > 1:
                        return "content_development_director"
                    else:
                        # Try content development director
                        return "content_development_director"

        def route_after_content_director(state: AgentState) -> str:
            """Route after the content development director node."""
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if content director delegated to a specific specialist
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the content director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "chapter drafter", "scene construction specialist",
                    "dialogue crafter", "continuity manager",
                    "voice consistency monitor", "emotional arc designer"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
                
                # Check for domain knowledge needs
                if "domain knowledge" in last_message_lower or "research" in last_message_lower:
                    return "domain_knowledge_specialist"
            
            # Count content director visits
            content_visits = sum(1 for msg in messages if
                                msg.get("role") == "user" and
                                "Content Development Director" in msg.get("content", ""))
                
            
            if content_visits > 2:
                # After multiple visits without delegation, try a specialist directly
                return "chapter_drafters"
            else:
                # Give another chance for delegation
                return "scene_construction_specialists"

        # Add routing for research
        def route_after_research(state: AgentState) -> str:
            """Route after research nodes."""
            research_type = state.get("lnode", "")
            
            if research_type == "domain_research":
                return "domain_knowledge_specialist"
            else:
                return "executive_director"

        builder.add_conditional_edges(
            "executive_director",
            route_after_executive_director
        )
        builder.add_conditional_edges(
            "content_development_director",
            route_after_content_director
        )
        
        # Add research routing
        builder.add_conditional_edges(
            "domain_research",
            route_after_research
        )
        # Connect domain knowledge specialist to domain research
        builder.add_edge("domain_knowledge_specialist", "domain_research")
        # Connect specialized agents to their supervisors
        for agent in ["chapter_drafters", "scene_construction_specialists",
                     "dialogue_crafters", "continuity_manager",
                     "voice_consistency_monitor", "emotional_arc_designer"]:
            if agent in agents[phase]:  # Only add edges for agents that exist in this phase
                builder.add_edge(agent, "content_development_director")
        builder.add_edge("creative_director", "executive_director")
            
    elif phase == "refinement":
        def route_after_executive_director(state: AgentState) -> str:
            """Route after the executive director node in refinement phase."""
            task = state.get("current_input", {}).get("task", "").lower()
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if executive director specified different specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the executive director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "editorial director", "creative director", "market alignment director",
                    "structural editor", "character arc evaluator", "thematic coherence analyst",
                    "prose enhancement specialist", "dialogue refinement expert",
                    "rhythm cadence optimizer", "grammar consistency checker",
                    "fact verification specialist"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
            
            # Count executive director visits to prevent loops
            exec_visits = sum(1 for msg in messages if
                              msg.get("role") == "user" and
                              "Executive Director" in msg.get("content", ""))
            # Default routing based on task keywords
            if "edit" in task:
                return "editorial_director"
            elif "creative" in task:
                return "creative_director"
            elif "market" in task:
                return "market_alignment_director"
            elif exec_visits > 5:  # If we've visited executive_director too many times
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                quality_assessment["editing_quality"] = 0.9
                quality_assessment["prose_quality"] = 0.9
                quality_assessment["thematic_coherence"] = 0.8
                project["quality_assessment"] = quality_assessment
                
                return END
            else:
                # Check quality gate to possibly end this phase
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                gate_result = check_quality_gate(
                    "refinement_to_finalization",
                    quality_assessment,
                    {"quality_gates": config.get("quality_gates", {})}
                )
                if gate_result["passed"]:
                    return END
                else:
                    # After a few iterations, force delegation
                    if exec_visits > 1:
                        return "editorial_director"
                    else:
                        return "editorial_director"

        def route_after_editorial_director(state: AgentState) -> str:
            """Route after the editorial director node."""
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if editorial director delegated to a specific specialist
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the editorial director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "structural editor", "character arc evaluator", "thematic coherence analyst",
                    "prose enhancement specialist", "dialogue refinement expert",
                    "rhythm cadence optimizer", "grammar consistency checker",
                    "fact verification specialist"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
            
            # Count editorial director visits
            editorial_visits = sum(1 for msg in messages if
                                  msg.get("role") == "user" and
                                  "Editorial Director" in msg.get("content", ""))
                
                
            if editorial_visits > 2:
                # After multiple visits without delegation, try a specialist directly
                return "prose_enhancement_specialist"
            else:
                # Encourage one more attempt at delegation
                return "structural_editor"

        # Add routing for fact verification and research
        def route_after_fact_verification(state: AgentState) -> str:
            """Route after fact verification specialist."""
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if fact verification needs research
            if last_message and "research" in last_message.lower():
                return "domain_research"
            else:
                return "editorial_director"
                
        def route_after_research(state: AgentState) -> str:
            """Route after domain research."""
            return "fact_verification_specialist"

        builder.add_conditional_edges(
            "executive_director",
            route_after_executive_director
        )
        builder.add_conditional_edges(
            "editorial_director",
            route_after_editorial_director
        )
        
        builder.add_conditional_edges(
            "fact_verification_specialist",
            route_after_fact_verification
        )
        
        builder.add_conditional_edges(
            "domain_research",
            route_after_research
        )
        # Connect specialized agents to their supervisors
        for agent in ["structural_editor", "character_arc_evaluator",
                     "thematic_coherence_analyst", "prose_enhancement_specialist",
                     "dialogue_refinement_expert", "rhythm_cadence_optimizer",
                     "grammar_consistency_checker"]:
            if agent in agents[phase]:  # Only add edges for agents that exist in this phase
                builder.add_edge(agent, "editorial_director")
        # Connect other directors back to executive director
        for agent in ["creative_director", "market_alignment_director"]:
            if agent in agents[phase]:
                builder.add_edge(agent, "executive_director")
            
    elif phase == "finalization":
        def route_after_executive_director(state: AgentState) -> str:
            """Route after the executive director node in finalization phase."""
            task = state.get("current_input", {}).get("task", "").lower()
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if executive director specified different specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the executive director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "editorial director", "market alignment director",
                    "positioning specialist", "title/blurb optimizer",
                    "differentiation strategist", "formatting standards expert"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
            
            # Count executive director visits to prevent loops
            exec_visits = sum(1 for msg in messages if
                              msg.get("role") == "user" and
                              "Executive Director" in msg.get("content", ""))
            # Default routing based on task keywords
            if "edit" in task:
                return "editorial_director"
            elif "market" in task:
                return "market_alignment_director"
            elif exec_visits > 5:  # If we've visited executive_director too many times
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                quality_assessment["market_readiness"] = 0.9
                quality_assessment["overall_quality"] = 0.9
                project["quality_assessment"] = quality_assessment
                
                return END
            else:
                # Check quality gate to possibly end this phase
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                gate_result = check_quality_gate(
                    "finalization_to_complete",
                    quality_assessment,
                    {"quality_gates": config.get("quality_gates", {})}
                )
                if gate_result["passed"]:
                    return END
                else:
                    # After a few iterations, try market alignment
                    if exec_visits > 1:
                        return "market_alignment_director"
                    else:
                        return "editorial_director"

        def route_after_market_director(state: AgentState) -> str:
            """Route after the market alignment director node."""
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if market director delegated to a specific specialist
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the market director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "positioning specialist", "title/blurb optimizer",
                    "differentiation strategist"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
                
                # Check for research needs
                if "research" in last_message_lower:
                    return "market_research"
            
            # Count market director visits
            market_visits = sum(1 for msg in messages if
                               msg.get("role") == "user" and
                               "Market Alignment Director" in msg.get("content", ""))
                
                
            if market_visits > 2:
                # After multiple visits, try a specialist directly
                return "positioning_specialist"
            else:
                # Give another chance for delegation
                return "title_blurb_optimizer"
                
        def route_after_research(state: AgentState) -> str:
            """Route after market research."""
            return "market_alignment_director"

        builder.add_conditional_edges(
            "executive_director",
            route_after_executive_director
        )
        builder.add_conditional_edges(
            "market_alignment_director",
            route_after_market_director
        )
        
        builder.add_conditional_edges(
            "market_research",
            route_after_research
        )
        # Connect specialized agents to their supervisors
        for agent in ["positioning_specialist", "title_blurb_optimizer", "differentiation_strategist"]:
            if agent in agents[phase]:  # Only add edges for agents that exist in this phase
                builder.add_edge(agent, "market_alignment_director")
        # Connect other specialists
        if "formatting_standards_expert" in agents[phase]:
            builder.add_edge("formatting_standards_expert", "editorial_director")
            
        # Connect other directors back to executive director
        builder.add_edge("editorial_director", "executive_director")
    
    return builder.compile()

def create_main_graph(config: Dict[str, Any], model_config=None) -> StateGraph:
    """Create a main composite graph that includes all phase graphs."""
    # Create a graph builder
    builder = StateGraph(AgentState)
    
    # Create all phase graphs
    phase_graphs = {}
    for phase in ["initialization", "development", "creation", "refinement", "finalization"]:
        phase_graphs[phase] = create_phase_graph(phase, "composite_project", config, model_config)
    
    # Add phase graphs as nodes to the main graph
    for phase, graph in phase_graphs.items():
        builder.add_node(phase, graph)
    
    # Set entry point to initialization phase
    builder.set_entry_point("initialization")
    
    # Define transitions between phases
    def route_after_phase(state: AgentState) -> str:
        """Route after a phase completes."""
        current_phase = state.get("phase", "initialization")
        
        # Define the phase progression
        phase_order = ["initialization", "development", "creation", "refinement", "finalization"]
        
        # Find the current phase index
        try:
            current_index = phase_order.index(current_phase)
        except ValueError:
            # If the current phase is not in the list, start with initialization
            return "initialization"
        
        # Move to the next phase if not at the end
        if current_index < len(phase_order) - 1:
            next_phase = phase_order[current_index + 1]
            # Update the state phase
            state["phase"] = next_phase
            return next_phase
        else:
            # If we're at the final phase, we're done
            return END
    
    # Add conditional edges between phases
    for phase in ["initialization", "development", "creation", "refinement"]:
        builder.add_conditional_edges(
            phase,
            route_after_phase
        )
    
    # Add a final edge from finalization to END
    builder.add_edge("finalization", END)
    
    return builder.compile()

# This function creates a comprehensive graph with all agents exposed
def create_storybook_graph(config: Dict[str, Any], model_config=None) -> StateGraph:
    """Create a comprehensive storybook graph with all agents fully exposed."""
    builder = StateGraph(AgentState)
    
    # Create agent factory
    agent_factory = AgentFactory(config, model_config=model_config)
    
    # Define all possible agents including specialists and research nodes
    all_agents = {
        # Directors
        "executive_director", "creative_director", "editorial_director",
        "content_development_director", "market_alignment_director",
        
        # Creative Team
        "structure_architect", "plot_development_specialist", "world_building_expert",
        "character_psychology_specialist", "character_voice_designer",
        "character_relationship_mapper",
        
        # Content Team
        "chapter_drafters", "scene_construction_specialists", "dialogue_crafters",
        "continuity_manager", "voice_consistency_monitor", "emotional_arc_designer",
        
        # Editorial Team
        "structural_editor", "character_arc_evaluator", "thematic_coherence_analyst",
        "prose_enhancement_specialist", "dialogue_refinement_expert",
        "rhythm_cadence_optimizer", "grammar_consistency_checker",
        "fact_verification_specialist",
        
        # Research Team
        "domain_knowledge_specialist", "cultural_authenticity_expert",
        
        # Marketing Team
        "positioning_specialist", "title_blurb_optimizer", "differentiation_strategist",
        "formatting_standards_expert",
        
        # Research Nodes
        "domain_research", "cultural_research", "market_research"
    }
    
    # Add all agents as nodes
    for agent_name in all_agents:
        if agent_name.endswith('_research'):
            research_type = agent_name.split('_')[0]
            builder.add_node(agent_name, agent_factory.create_research_agent(research_type))
        else:
            builder.add_node(agent_name, agent_factory.create_agent(agent_name, "storybook_project"))
    
    # Define all possible connections between agents
    connections = {
        "executive_director": [
            "creative_director", "editorial_director", "content_development_director",
            "market_alignment_director", "domain_knowledge_specialist",
            "cultural_authenticity_expert"
        ],
        "creative_director": [
            "structure_architect", "plot_development_specialist", "world_building_expert",
            "character_psychology_specialist", "character_voice_designer",
            "character_relationship_mapper", "executive_director"
        ],
        "editorial_director": [
            "structural_editor", "character_arc_evaluator", "thematic_coherence_analyst",
            "prose_enhancement_specialist", "dialogue_refinement_expert",
            "rhythm_cadence_optimizer", "grammar_consistency_checker",
            "fact_verification_specialist", "executive_director"
        ],
        "content_development_director": [
            "chapter_drafters", "scene_construction_specialists", "dialogue_crafters",
            "continuity_manager", "voice_consistency_monitor", "emotional_arc_designer",
            "executive_director"
        ],
        "market_alignment_director": [
            "positioning_specialist", "title_blurb_optimizer", "differentiation_strategist",
            "market_research", "executive_director"
        ],
        # Research connections
        "domain_knowledge_specialist": ["domain_research", "executive_director"],
        "cultural_authenticity_expert": ["cultural_research", "executive_director"],
        "fact_verification_specialist": ["domain_research", "editorial_director"]
    }
    
    # Add all connections
    for source, targets in connections.items():
        for target in targets:
            builder.add_edge(source, target)
    
    # Add return paths from specialists to their directors
    specialist_to_director = {
        # Creative team to creative director
        "structure_architect": "creative_director",
        "plot_development_specialist": "creative_director",
        "world_building_expert": "creative_director",
        "character_psychology_specialist": "creative_director",
        "character_voice_designer": "creative_director",
        "character_relationship_mapper": "creative_director",
        
        # Content team to content development director
        "chapter_drafters": "content_development_director",
        "scene_construction_specialists": "content_development_director",
        "dialogue_crafters": "content_development_director",
        "continuity_manager": "content_development_director",
        "voice_consistency_monitor": "content_development_director",
        "emotional_arc_designer": "content_development_director",
        
        # Editorial team to editorial director
        "structural_editor": "editorial_director",
        "character_arc_evaluator": "editorial_director",
        "thematic_coherence_analyst": "editorial_director",
        "prose_enhancement_specialist": "editorial_director",
        "dialogue_refinement_expert": "editorial_director",
        "rhythm_cadence_optimizer": "editorial_director",
        "grammar_consistency_checker": "editorial_director",
        
        # Marketing team to market alignment director
        "positioning_specialist": "market_alignment_director",
        "title_blurb_optimizer": "market_alignment_director",
        "differentiation_strategist": "market_alignment_director"
    }
    
    # Add return paths
    for specialist, director in specialist_to_director.items():
        builder.add_edge(specialist, director)
    
    # Add research return paths
    builder.add_edge("domain_research", "domain_knowledge_specialist")
    builder.add_edge("cultural_research", "cultural_authenticity_expert")
    builder.add_edge("market_research", "market_alignment_director")
    
    # Set entry point
    builder.set_entry_point("executive_director")
    
    # Add end state connections
    builder.add_edge("executive_director", END)
    
    return builder.compile()

class storybook:
    def __init__(self, model_config=None):
        # Check for HuggingFace token
        self.huggingface_token = os.getenv("HUGGINGFACE_API_TOKEN")
        if not self.huggingface_token:
            raise ValueError("HUGGINGFACE_API_TOKEN not found in environment variables")
        
        # Log into Hugging Face
        login(token=self.huggingface_token)
        
        # Default model config if none provided
        self.model_config = model_config or {
            "model_id": "HuggingFaceH4/zephyr-7b-beta",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": False,
            "repetition_penalty": 1.03
        }
        
        self.tavily = TavilyClient(api_key=tavily_ai_api_key)
        
        # Setup storybook configuration
        self.storybook_config = {
            "mongodb_connection_string": None,
            "mongodb_database_name": None,
            "quality_gates": {
                "initialization_to_development": {
                    "planning_quality": 0.7,
                    "market_alignment": 0.6
                },
                "development_to_creation": {
                    "structure_quality": 0.7,
                    "character_depth": 0.7,
                    "world_building": 0.7
                },
                "creation_to_refinement": {
                    "content_quality": 0.7,
                    "narrative_flow": 0.7,
                    "dialogue_quality": 0.7
                },
                "refinement_to_finalization": {
                    "editing_quality": 0.8,
                    "prose_quality": 0.8,
                    "thematic_coherence": 0.7
                },
                "finalization_to_complete": {
                    "market_readiness": 0.8,
                    "overall_quality": 0.8
                }
            }
        }
        
        # Build the storybook graphs for each phase
        self.phase_graphs = {}
        for phase in ["initialization", "development", "creation", "refinement", "finalization"]:
            self.phase_graphs[phase] = create_phase_graph(phase, "default_project", self.storybook_config, self.model_config)
            
        # Build the main composite graph
        self.graph = create_main_graph(self.storybook_config, self.model_config)
        
        # Build the comprehensive storybook graph with all agents
        self.storybook_graph = create_storybook_graph(self.storybook_config, self.model_config)
    
    def update_model_config(self, model_config):
        """Update the model configuration and recreate the agent factory."""
        self.model_config = model_config
        
        # Rebuild the storybook graphs for each phase
        self.phase_graphs = {}
        for phase in ["initialization", "development", "creation", "refinement", "finalization"]:
            self.phase_graphs[phase] = create_phase_graph(phase, "default_project", self.storybook_config, self.model_config)
            
        # Rebuild the main composite graph
        self.graph = create_main_graph(self.storybook_config, self.model_config)
        
        # Rebuild the comprehensive storybook graph
        self.storybook_graph = create_storybook_graph(self.storybook_config, self.model_config)
    
    def initialize_storybook_project(self, title: str, synopsis: str, manuscript: str, notes: Optional[Dict[str, Any]] = None):
        """Initialize a new storybook project"""
        project_id = str(uuid4())
        
        # Split the manuscript into manageable chunks
        manuscript_chunks = split_manuscript(manuscript)
        
        initial_state = {
            "project": {
                "id": project_id,
                "title": title,
                "synopsis": synopsis,
                "manuscript": manuscript,
                "manuscript_chunks": manuscript_chunks,
                "notes": notes or {},
                "type": "creation",
                "quality_assessment": {
                    "planning_quality": 0.6,  # Initial values
                    "market_alignment": 0.5,
                },
                "created_at": datetime.now().isoformat()
            },
            "phase": "initialization",
            "current_input": {
                "task": "Initial project assessment and planning",
                "phase": "initialization"
            },
            "phase_history": {},
            "messages": [],
            "count": 0,
            "lnode": ""
        }
        
        return initial_state
    
    def run_storybook_phase(self, state, phase="initialization"):
        """Run a specific phase of the storybook workflow"""
        if phase not in self.phase_graphs:
            raise ValueError(f"Unknown phase: {phase}")
        
        # Make sure the state includes the phase
        if "phase" not in state:
            state["phase"] = phase
            
        # Run the phase graph
        result = self.phase_graphs[phase].invoke(state)
        return result
        
    def run_storybook(self, state):
        """Run the full storybook workflow using the main graph"""
        result = self.graph.invoke(state)
        return result
    
    def get_available_checkpoints(self, project_id=None):
        """List available checkpoints"""
        checkpoint_dir = os.path.join(os.getcwd(), "checkpoints")
        if not os.path.exists(checkpoint_dir):
            return []
        
        checkpoints = []
        for file in os.listdir(checkpoint_dir):
            if file.endswith(".sqlite"):
                if project_id is None or file.startswith(f"{project_id}_"):
                    try:
                        # Extract project ID and phase from filename
                        parts = file.replace(".sqlite", "").split("_")
                        if len(parts) >= 2:
                            project_id = "_".join(parts[:-1])  # Handle project IDs that might contain underscores
                            phase = parts[-1]
                            
                            # Get file modification time
                            file_path = os.path.join(checkpoint_dir, file)
                            mod_time = os.path.getmtime(file_path)
                            mod_time_str = datetime.fromtimestamp(mod_time).strftime('%Y-%m-%d %H:%M:%S')
                            
                            checkpoints.append({
                                "project_id": project_id,
                                "phase": phase,
                                "checkpoint_file": file,
                                "last_modified": mod_time_str
                            })
                    except Exception as e:
                        print(f"Error parsing checkpoint file {file}: {str(e)}")
        
        # Sort by last modified time
        checkpoints.sort(key=lambda x: x["last_modified"], reverse=True)
        return checkpoints
    
    def load_checkpoint(self, checkpoint_file):
        """Load a specific checkpoint"""
        checkpoint_path = os.path.join(os.getcwd(), "checkpoints", checkpoint_file)
        if not os.path.exists(checkpoint_path):
            raise ValueError(f"Checkpoint file not found: {checkpoint_path}")
        
        # Extract phase from filename
        phase = checkpoint_file.split("_")[-1].replace(".sqlite", "")
        
        # Get the appropriate graph
        if phase not in self.phase_graphs:
            raise ValueError(f"Unknown phase in checkpoint: {phase}")
            
        graph = self.phase_graphs[phase]
        
        # Create a SQLite checkpoint saver
        saver = SqliteSaver(checkpoint_path)
        
        # Load the latest checkpoint
        thread_ids = saver.list_threads()
        if not thread_ids:
            raise ValueError(f"No threads found in checkpoint file {checkpoint_file}")
        
        # Use the most recent thread
        latest_thread_id = thread_ids[-1]
        state = saver.get_state(latest_thread_id)
        
        return state

class writer_gui:
    def __init__(self, storybook_instance):
        # Initialize model choices first
        self.model_choices = [
            "HuggingFaceH4/zephyr-7b-beta",
            "meta-llama/Llama-2-7b-chat-hf",
            "mistralai/Mistral-7B-Instruct-v0.2",
            "google/flan-t5-base",
            "facebook/bart-large-cnn",
            "gpt2",
            "distilgpt2",
            "microsoft/phi-2"
        ]
        
        # Then initialize other attributes
        self.storybook = storybook_instance
        self.partial_message = ""
        self.thread_id = -1
        self.thread = {"configurable": {"thread_id": str(self.thread_id)}}
        self.is_running = False
        self.agent_visits = {}
        self.project_id = None
        
        self.default_model_config = {
            "model_id": "HuggingFaceH4/zephyr-7b-beta",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": False,
            "repetition_penalty": 1.03
        }
        
        # Create interface last
        self.demo = self.create_interface()

    async def stream_response(self, message, word_delay=0.01):
        """Stream a response word by word with a delay"""
        words = message.split()
        partial = ""
        for word in words:
            partial += word + " "
            yield partial
            await asyncio.sleep(word_delay)

    async def run_storybook_with_model(self, title, synopsis, manuscript, notes_text, task, phase,
                                     model_id, model_task, temperature, max_new_tokens,
                                     repetition_penalty, do_sample, max_iterations=10):
        """Run the storybook workflow with streaming updates and custom model config"""
        if self.is_running:
            yield "Already running a session. Please wait for it to complete.", "", "", "", "", []
            return
        self.is_running = True
        iterations = 0
        state = None
        quality_str = ""
        stream_buffer = []
        
        try:
            # Create model configuration
            model_config = {
                "model_id": model_id,
                "task": model_task,
                "temperature": temperature,
                "max_new_tokens": max_new_tokens,
                "repetition_penalty": repetition_penalty,
                "do_sample": do_sample
            }
            
            # Update storybook with model config
            self.storybook.update_model_config(model_config)
            
            # Parse notes from text input
            notes = {}
            if notes_text:
                for line in notes_text.strip().split('\n'):
                    if ':' in line:
                        key, value = line.split(':', 1)
                        notes[key.strip()] = value.strip()
            
            # Initialize a new project
            initial_state = self.storybook.initialize_storybook_project(title, synopsis, manuscript, notes)
            self.project_id = initial_state["project"]["id"]
            
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Initialized project with ID: {self.project_id}")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Using model: {model_id}")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, "initialization", phase, self.project_id, "", stream_buffer
            
            # Set the phase in the state
            initial_state["phase"] = phase
            initial_state["current_input"]["phase"] = phase
            
            # Set the task
            if task:
                initial_state["current_input"]["task"] = task
            
            # Run the workflow for the specified phase
            state = initial_state
            while iterations < max_iterations:
                try:
                    stream_buffer.append(f"\n[{datetime.now().strftime('%H:%M:%S')}] --- Iteration {iterations + 1} in phase '{phase}' ---")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, state.get('lnode', 'starting'), phase, self.project_id, "", stream_buffer
                    
                    # Run the phase with real-time logging
                    agent_name = state.get("lnode", "starting")
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Working with agent: {agent_name}")
                    
                    # Run the phase and update state
                    state = self.storybook.run_storybook_phase(state, phase)
                    
                    # Extract the last message for display
                    if state.get("messages", []):
                        last_message = state["messages"][-1]
                        agent_name = state.get("lnode", "unknown")
                        content_preview = last_message.get('content', '')[:300]
                        if len(last_message.get('content', '')) > 300:
                            content_preview += "..."
                            
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] [{agent_name}]: {content_preview}")
                        
                    # Update quality assessment display
                    quality_assessment = state.get("project", {}).get("quality_assessment", {})
                    quality_str = "Quality Assessment:\n" + "\n".join(
                        [f"- {k}: {v:.2f}" for k, v in quality_assessment.items()]
                    )
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Updated quality assessment")
                    
                    # Check if workflow has ended
                    if state.get("lnode") == "END":
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Workflow completed successfully.")
                        break
                    
                    iterations += 1
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, state.get('lnode', 'working'), phase, self.project_id, quality_str, stream_buffer
                    
                except Exception as e:
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error in iteration: {str(e)}")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, "error", phase, self.project_id, "", stream_buffer
                    break
            
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Workflow finished after {iterations} iterations.")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, state.get('lnode', 'complete'), phase, self.project_id, quality_str, stream_buffer
        
        except Exception as e:
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Unhandled error: {str(e)}")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, "error", phase, self.project_id, "", stream_buffer
        
        finally:
            self.is_running = False

    def get_checkpoints(self):
        """Get available checkpoints for loading"""
        try:
            checkpoints = self.storybook.get_available_checkpoints()
            checkpoint_list = [f"{cp['project_id']} - {cp['phase']} ({cp['last_modified']})" for cp in checkpoints]
            checkpoint_files = [cp["checkpoint_file"] for cp in checkpoints]
            return checkpoint_list, checkpoint_files
        except Exception as e:
            print(f"Error getting checkpoints: {str(e)}")
            return [], []

    def load_project_from_checkpoint(self, checkpoint_file):
        """Load a project from a checkpoint file"""
        if not checkpoint_file:
            return "No checkpoint selected", "", "", "", "", []
        
        try:
            state = self.storybook.load_checkpoint(checkpoint_file)
            
            # Extract project details
            project = state.get("project", {})
            self.project_id = project.get("id", "unknown")
            phase = state.get("phase", "unknown")
            
            # Prepare status message
            stream_buffer = [
                f"[{datetime.now().strftime('%H:%M:%S')}] Loaded project {self.project_id} from checkpoint",
                f"[{datetime.now().strftime('%H:%M:%S')}] Title: {project.get('title', 'Untitled')}",
                f"[{datetime.now().strftime('%H:%M:%S')}] Phase: {phase}",
                f"[{datetime.now().strftime('%H:%M:%S')}] Last agent: {state.get('lnode', 'unknown')}"
            ]
            
            # Include quality assessment
            quality_assessment = project.get("quality_assessment", {})
            quality_str = "Quality Assessment:\n" + "\n".join(
                [f"- {k}: {v:.2f}" for k, v in quality_assessment.items()]
            )
            
            # Return the state information to update UI
            message = "\n".join(stream_buffer)
            return message, state.get('lnode', 'unknown'), phase, self.project_id, quality_str, stream_buffer, state
        except Exception as e:
            error_message = f"Error loading checkpoint: {str(e)}"
            return error_message, "", "", "", "", [error_message], None

    async def continue_from_checkpoint(self, state, task, phase, max_iterations=10):
        """Continue processing from a loaded checkpoint"""
        if self.is_running:
            yield "Already running a session. Please wait for it to complete.", "", "", "", "", []
            return
        
        if not state:
            yield "No state loaded from checkpoint", "", "", "", "", []
            return
            
        self.is_running = True
        iterations = 0
        quality_str = ""
        stream_buffer = []
        
        try:
            # Update the task and phase if provided
            if task:
                state["current_input"]["task"] = task
            
            if phase:
                state["phase"] = phase
                state["current_input"]["phase"] = phase
            
            self.project_id = state["project"]["id"]
            current_phase = state["phase"]
            
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Continuing project with ID: {self.project_id}")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Phase: {current_phase}")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Task: {state['current_input']['task']}")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, state.get('lnode', 'starting'), current_phase, self.project_id, "", stream_buffer
            
            # Run the workflow for the specified phase
            while iterations < max_iterations:
                try:
                    stream_buffer.append(f"\n[{datetime.now().strftime('%H:%M:%S')}] --- Iteration {iterations + 1} in phase '{current_phase}' ---")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, state.get('lnode', 'continuing'), current_phase, self.project_id, "", stream_buffer
                    
                    # Run the phase with real-time logging
                    agent_name = state.get("lnode", "continuing")
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Working with agent: {agent_name}")
                    
                    # Run the phase and update state
                    state = self.storybook.run_storybook_phase(state, current_phase)
                    
                    # Extract the last message for display
                    if state.get("messages", []):
                        last_message = state["messages"][-1]
                        agent_name = state.get("lnode", "unknown")
                        content_preview = last_message.get('content', '')[:300]
                        if len(last_message.get('content', '')) > 300:
                            content_preview += "..."
                            
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] [{agent_name}]: {content_preview}")
                        
                    # Update quality assessment display
                    quality_assessment = state.get("project", {}).get("quality_assessment", {})
                    quality_str = "Quality Assessment:\n" + "\n".join(
                        [f"- {k}: {v:.2f}" for k, v in quality_assessment.items()]
                    )
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Updated quality assessment")
                    
                    # Check if workflow has ended
                    if state.get("lnode") == "END":
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Workflow completed successfully.")
                        break
                    
                    iterations += 1
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, state.get('lnode', 'working'), current_phase, self.project_id, quality_str, stream_buffer
                    
                except Exception as e:
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error in iteration: {str(e)}")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, "error", current_phase, self.project_id, "", stream_buffer
                    break
            
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Workflow finished after {iterations} iterations.")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, state.get('lnode', 'complete'), current_phase, self.project_id, quality_str, stream_buffer
            
        except Exception as e:
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Unhandled error: {str(e)}")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, "error", state.get("phase", "unknown"), self.project_id, "", stream_buffer
        
        finally:
            self.is_running = False

    def create_interface(self):
        theme = gr.themes.Ocean(
            primary_hue="amber",
            secondary_hue="fuchsia",
            neutral_hue="slate",
            font=[gr.themes.GoogleFont('Work Sans'), 'ui-sans-serif', 'system-ui', 'sans-serif'],
        )
        
        with gr.Blocks(
            theme=theme,
            analytics_enabled=False
        ) as demo:
            gr.Markdown("# storybook writer")
            
            # First Row - Status Panel
            with gr.Row():
                sb_lnode_bx = gr.Textbox(label="Last Agent", scale=1)
                sb_phase_bx = gr.Textbox(label="Current Phase", scale=1)
                sb_thread_bx = gr.Textbox(label="Project ID", scale=1)
                sb_quality_bx = gr.Textbox(label="Quality Assessment", scale=2)
            
            # Hidden state for loaded checkpoints
            loaded_state = gr.State(None)
            
            # Stream output pane that will show real-time updates
            stream_output = gr.State([])
            
            with gr.Tabs() as tabs:
                # Model Configuration Tab
                with gr.Tab("Model Configuration"):
                    with gr.Row():
                        model_id = gr.Dropdown(
                            label="Model ID",
                            choices=self.model_choices,
                            value=self.default_model_config["model_id"]
                        )
                        model_task = gr.Dropdown(
                            label="Task",
                            choices=["text-generation", "text2text-generation", "summarization"],
                            value="text-generation"
                        )
                        
                    with gr.Row():
                        temperature = gr.Slider(
                            label="Temperature",
                            minimum=0.0,
                            maximum=1.0,
                            value=self.default_model_config["temperature"],
                            step=0.01
                        )
                        max_new_tokens = gr.Slider(
                            label="Max New Tokens",
                            minimum=10,
                            maximum=1024,
                            value=self.default_model_config["max_new_tokens"],
                            step=1
                        )
                        repetition_penalty = gr.Slider(
                            label="Repetition Penalty",
                            minimum=1.0,
                            maximum=2.0,
                            value=self.default_model_config["repetition_penalty"],
                            step=0.01
                        )
                        
                    with gr.Row():
                        do_sample = gr.Checkbox(
                            label="Do Sample",
                            value=self.default_model_config["do_sample"]
                        )
                
                # Task Control Tab
                with gr.Tab("Task Control"):
                    with gr.Row():
                        task_bx = gr.Textbox(
                            label="Task",
                            value="Initial project assessment and planning",
                            scale=2
                        )
                        phase_select = gr.Dropdown(
                            choices=["initialization", "development", "creation", "refinement", "finalization"],
                            value="initialization",
                            label="Phase",
                            scale=1
                        )
                
                # Project Content Tab
                with gr.Tab("Project Content"):
                    with gr.Row():
                        with gr.Column():
                            title_bx = gr.Textbox(
                                label="Title (required)",
                                value="The Hidden Quest"
                            )
                            synopsis_bx = gr.Textbox(
                                label="Synopsis (required)",
                                value="A young adventurer discovers a mysterious artifact...",
                                lines=4
                            )
                            
                        with gr.Column():
                            manuscript_bx = gr.Textbox(
                                label="Manuscript (required)",
                                value="Chapter 1\n\nThe sun was setting over...",
                                lines=10
                            )
                            notes_bx = gr.Textbox(
                                label="Notes (optional - key: value format)",
                                value="target_audience: Young Adult\ngenre: Fantasy",
                                lines=4
                            )
                
                # Checkpoint Tab
                with gr.Tab("Checkpoints"):
                    with gr.Row():
                        refresh_checkpoints_btn = gr.Button("Refresh Checkpoints")
                        checkpoint_list = gr.Dropdown(
                            label="Available Checkpoints",
                            choices=[],
                            value=None,
                            interactive=True
                        )
                        checkpoint_file = gr.State([])
                    
                    with gr.Row():
                        load_checkpoint_btn = gr.Button("Load Selected Checkpoint")
                        continue_checkpoint_btn = gr.Button("Continue from Checkpoint")
                        
            # Run Buttons - outside tabs
            with gr.Row():
                storybook_btn = gr.Button("Run New Project", variant="primary", scale=1)
                clear_btn = gr.Button("Clear Output", scale=1)
            
            # Streaming Output area with real-time updates
            sb_live = gr.Textbox(label="storybook Output", lines=20, autoscroll=True)
            
            # Event handlers
            # Refresh checkpoints
            refresh_checkpoints_btn.click(
                fn=self.get_checkpoints,
                inputs=[],
                outputs=[checkpoint_list, checkpoint_file]
            )
            
            # Load checkpoint
            load_checkpoint_btn.click(
                fn=lambda x, y: y[x] if x < len(y) else None,
                inputs=[checkpoint_list, checkpoint_file],
                outputs=gr.State(None)
            ).then(
                fn=self.load_project_from_checkpoint,
                inputs=[gr.State(None)],
                outputs=[sb_live, sb_lnode_bx, sb_phase_bx, sb_thread_bx, sb_quality_bx, stream_output, loaded_state]
            )
            
            # Continue from checkpoint
            continue_checkpoint_btn.click(
                fn=lambda: (
                    gr.update(value="Processing...", interactive=False),  # Update button
                    gr.update(value="Continuing from checkpoint...")      # Update output
                ),
                inputs=None,
                outputs=[continue_checkpoint_btn, sb_live]
            ).then(
                fn=self.continue_from_checkpoint,
                inputs=[loaded_state, task_bx, phase_select],
                outputs=[sb_live, sb_lnode_bx, sb_phase_bx, sb_thread_bx, sb_quality_bx, stream_output],
                show_progress=True
            ).then(
                fn=lambda: gr.update(value="Continue from Checkpoint", interactive=True),
                inputs=None,
                outputs=continue_checkpoint_btn
            )
            
            # Run new project
            storybook_btn.click(
                fn=lambda: (
                    gr.update(value="Processing...", interactive=False),  # Update button
                    gr.update(value="Starting workflow...")               # Update output
                ),
                inputs=None,
                outputs=[storybook_btn, sb_live]
            ).then(
                fn=self.run_storybook_with_model,
                inputs=[
                    title_bx, synopsis_bx, manuscript_bx, notes_bx, task_bx, phase_select,
                    model_id, model_task, temperature, max_new_tokens, repetition_penalty, do_sample
                ],
                outputs=[sb_live, sb_lnode_bx, sb_phase_bx, sb_thread_bx, sb_quality_bx, stream_output],
                show_progress=True
            ).then(
                fn=lambda: gr.update(value="Run New Project", variant="primary", interactive=True),
                inputs=None,
                outputs=storybook_btn
            )
            
            # Clear output
            clear_btn.click(
                fn=lambda: (
                    "", 
                    "", 
                    "",
                    "",
                    "",
                    []
                ),
                inputs=[],
                outputs=[sb_live, sb_lnode_bx, sb_phase_bx, sb_thread_bx, sb_quality_bx, stream_output]
            )
            
            # Help Section
            with gr.Accordion("Help & Information", open=False):
                gr.Markdown(
                    "# How to use storybook\n\n"
                    "1. Enter title and synopsis\n"
                    "2. Add manuscript content\n"
                    "3. Add optional notes\n"
                    "4. Configure model parameters\n"
                    "5. Choose task and phase\n"
                    "6. Click Run storybook\n\n"
                    "## Using Checkpoints\n"
                    "1. Go to the Checkpoints tab and click 'Refresh Checkpoints'\n"
                    "2. Select a checkpoint from the dropdown\n"
                    "3. Click 'Load Selected Checkpoint' to view project details\n"
                    "4. Click 'Continue from Checkpoint' to resume processing\n\n"
                    "## Phases\n"
                    "- Initialization: Project setup\n"
                    "- Development: Story building\n"
                    "- Creation: Content writing\n"
                    "- Refinement: Editing\n"
                    "- Finalization: Polishing"
                )
        
        # Initialize checkpoints
        def init_checkpoints():
            checkpoint_list_values, checkpoint_file_values = self.get_checkpoints()
            return checkpoint_list_values, checkpoint_file_values
        
        demo.load(
            fn=init_checkpoints,
            inputs=None,
            outputs=[checkpoint_list, checkpoint_file]
        )
                
        return demo

    def launch(self):
        self.demo.launch(share=False)  # Set share to False to avoid sharing issues

def visualize_storybook_graph(graph: StateGraph):
    """Create a visual representation of the storybook graph."""
    G = nx.DiGraph()
    
    # Add nodes
    for node in graph.nodes:
        G.add_node(node)
    
    # Add edges
    for node, edges in graph.edges.items():
        for edge in edges:
            G.add_edge(node, edge)
    
    # Set up the plot
    plt.figure(figsize=(20, 20))
    pos = nx.spring_layout(G, k=2, iterations=50)
    
    # Draw the graph
    nx.draw(G, pos, with_labels=True, node_color='lightblue',
            node_size=2000, font_size=8, font_weight='bold',
            arrows=True, edge_color='gray', arrowsize=20)
    
    plt.title("Storybook Agent Graph")
    return plt

# Create the main storybook instance at module level
sb_instance = storybook()
graph = sb_instance.graph

# Create a comprehensive workflow graph that includes all agents
# This will be accessed via the langgraph server API
storybook = sb_instance.storybook_graph

# Initialize CUDA handling
init_cuda()

# Suppress NVML warnings
warnings.filterwarnings("ignore", ".*Can't initialize NVML.*")

if __name__ == "__main__":
    app = writer_gui(sb_instance)
    app.launch()
</current_codebase>

<users_request>
1. Check the Langsmith Tracing implmenetation. The current env vars are incorrect and the latest langsmith SDK should automaticcaly pick up tracing if these envs are set:
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT="https://api.smith.langchain.com"
LANGSMITH_API_KEY="<your-api-key>"
LANGSMITH_PROJECT="storybook.old"
OPENAI_API_KEY="<your-openai-api-key>"

2. remove flash-attn. It is causing issues on HuggingFace. Follow the below exampl guidance on an alternative method for memory management:

Naive pipeline parallelism is supported out of the box. For this, simply load the model with device="auto" which will automatically place the different layers on the available GPUs as explained here. Note, however that while very effective, this naive pipeline parallelism does not tackle the issues of GPU idling. For this more advanced pipeline parallelism is required as explained here.

If you have access to an 8 x 80GB A100 node, you could load BLOOM as follows

Copied
!pip install transformers accelerate bitsandbytes optimum
Copied
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("bigscience/bloom", device_map="auto", pad_token_id=0)
By using device_map="auto" the attention layers would be equally distributed over all available GPUs.

In this guide, we will use bigcode/octocoder as it can be run on a single 40 GB A100 GPU device chip. Note that all memory and speed optimizations that we will apply going forward, are equally applicable to models that require model or tensor parallelism.

Since the model is loaded in bfloat16 precision, using our rule of thumb above, we would expect the memory requirement to run inference with bigcode/octocoder to be around 31 GB VRAM. Let’s give it a try.

We first load the model and tokenizer and then pass both to Transformers’ pipeline object.

Copied
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch

model = AutoModelForCausalLM.from_pretrained("bigcode/octocoder", torch_dtype=torch.bfloat16, device_map="auto", pad_token_id=0)
tokenizer = AutoTokenizer.from_pretrained("bigcode/octocoder")

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
Copied
prompt = "Question: Please write a function in Python that transforms bytes to Giga bytes.\n\nAnswer:"

result = pipe(prompt, max_new_tokens=60)[0]["generated_text"][len(prompt):]
result
Output:

Copied
Here is a Python function that transforms bytes to Giga bytes:\n\n```python\ndef bytes_to_giga_bytes(bytes):\n    return bytes / 1024 / 1024 / 1024\n```\n\nThis function takes a single
Nice, we can now directly use the result to convert bytes into Gigabytes.

Copied
def bytes_to_giga_bytes(bytes):
  return bytes / 1024 / 1024 / 1024
Let’s call torch.cuda.max_memory_allocated to measure the peak GPU memory allocation.

Copied
bytes_to_giga_bytes(torch.cuda.max_memory_allocated())
Output:

Copied
29.0260648727417
Close enough to our back-of-the-envelope computation! We can see the number is not exactly correct as going from bytes to kilobytes requires a multiplication of 1024 instead of 1000. Therefore the back-of-the-envelope formula can also be understood as an “at most X GB” computation. Note that if we had tried to run the model in full float32 precision, a whopping 64 GB of VRAM would have been required.

Almost all models are trained in bfloat16 nowadays, there is no reason to run the model in full float32 precision if your GPU supports bfloat16. Float32 won’t give better inference results than the precision that was used to train the model.

If you are unsure in which format the model weights are stored on the Hub, you can always look into the checkpoint’s config under "torch_dtype", e.g. here. It is recommended to set the model to the same precision type as written in the config when loading with from_pretrained(..., torch_dtype=...) except when the original type is float32 in which case one can use both float16 or bfloat16 for inference.

Let’s define a flush(...) function to free all allocated memory so that we can accurately measure the peak allocated GPU memory.

Copied
del pipe
del model

import gc
import torch

def flush():
  gc.collect()
  torch.cuda.empty_cache()
  torch.cuda.reset_peak_memory_stats()
Let’s call it now for the next experiment.

Copied
flush()
From the Accelerate library, you can also use a device-agnostic utility method called release_memory, which takes various hardware backends like XPU, MLU, NPU, MPS, and more into account.

Copied
from accelerate.utils import release_memory
# ...

release_memory(model)
Now what if your GPU does not have 32 GB of VRAM? It has been found that model weights can be quantized to 8-bit or 4-bits without a significant loss in performance (see Dettmers et al.). Model can be quantized to even 3 or 2 bits with an acceptable loss in performance as shown in the recent GPTQ paper 🤯.

Without going into too many details, quantization schemes aim at reducing the precision of weights while trying to keep the model’s inference results as accurate as possible (a.k.a as close as possible to bfloat16). Note that quantization works especially well for text generation since all we care about is choosing the set of most likely next tokens and don’t really care about the exact values of the next token logit distribution. All that matters is that the next token logit distribution stays roughly the same so that an argmax or topk operation gives the same results.

There are various quantization techniques, which we won’t discuss in detail here, but in general, all quantization techniques work as follows:

Quantize all weights to the target precision
Load the quantized weights, and pass the input sequence of vectors in bfloat16 precision
Dynamically dequantize weights to bfloat16 to perform the computation with their input vectors in bfloat16 precision
In a nutshell, this means that inputs-weight matrix multiplications, with
X
X being the inputs,
W
W being a weight matrix and
Y
Y being the output:
Y
=
X
∗
W
Y=X∗W

are changed to
Y
=
X
∗
dequantize
(
W
)
Y=X∗dequantize(W)

for every matrix multiplication. Dequantization and re-quantization is performed sequentially for all weight matrices as the inputs run through the network graph.

Therefore, inference time is often not reduced when using quantized weights, but rather increases. Enough theory, let’s give it a try! To quantize the weights with Transformers, you need to make sure that the bitsandbytes library is installed.

Copied
!pip install bitsandbytes
We can then load models in 8-bit quantization by simply adding a load_in_8bit=True flag to from_pretrained.

Copied
model = AutoModelForCausalLM.from_pretrained("bigcode/octocoder", load_in_8bit=True, pad_token_id=0)
Now, let’s run our example again and measure the memory usage.

Copied
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

result = pipe(prompt, max_new_tokens=60)[0]["generated_text"][len(prompt):]
result
Output:

Copied
Here is a Python function that transforms bytes to Giga bytes:\n\n```python\ndef bytes_to_giga_bytes(bytes):\n    return bytes / 1024 / 1024 / 1024\n```\n\nThis function takes a single
Nice, we’re getting the same result as before, so no loss in accuracy! Let’s look at how much memory was used this time.

Copied
bytes_to_giga_bytes(torch.cuda.max_memory_allocated())
Output:

Copied
15.219234466552734
Significantly less! We’re down to just a bit over 15 GBs and could therefore run this model on consumer GPUs like the 4090. We’re seeing a very nice gain in memory efficiency and more or less no degradation to the model’s output. However, we can also notice a slight slow-down during inference.

We delete the models and flush the memory again.

Copied
del model
del pipe
Copied
flush()
Let’s see what peak GPU memory consumption 4-bit quantization gives. Quantizing the model to 4-bit can be done with the same API as before - this time by passing load_in_4bit=True instead of load_in_8bit=True.

Copied
model = AutoModelForCausalLM.from_pretrained("bigcode/octocoder", load_in_4bit=True, low_cpu_mem_usage=True, pad_token_id=0)

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

result = pipe(prompt, max_new_tokens=60)[0]["generated_text"][len(prompt):]
result
Output:

Copied
Here is a Python function that transforms bytes to Giga bytes:\n\n```\ndef bytes_to_gigabytes(bytes):\n    return bytes / 1024 / 1024 / 1024\n```\n\nThis function takes a single argument
We’re almost seeing the same output text as before - just the python is missing just before the code snippet. Let’s see how much memory was required.

Copied
bytes_to_giga_bytes(torch.cuda.max_memory_allocated())
</user_request>
Model: After reviewing your code and the requested changes, I need to update the LangSmith tracing implementation and remove Flash Attention references. These modifications will improve compatibility and address the issues mentioned.

<final_code>
import os
from dotenv import load_dotenv
import warnings
import utils
import gradio as gr

# Load environment variables from .env file or Secret Manager
_ = load_dotenv("../.env")
tavily_ai_api_key = os.getenv("TAVILY_API_KEY")
huggingface_token = os.getenv("HUGGINGFACE_API_TOKEN")
warnings.filterwarnings("ignore", message=".*TqdmWarning.*")

from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated, List, Dict, Any, Optional, Literal, cast, Union
import operator
from langgraph.checkpoint.sqlite import SqliteSaver
from langchain_core.messages import (
    SystemMessage,
    HumanMessage,
    AIMessage,
)
from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch
from huggingface_hub import login
from pydantic import BaseModel, Field
from langchain_text_splitters import RecursiveCharacterTextSplitter
from tavily import TavilyClient
import os
import sqlite3
from datetime import datetime
from uuid import uuid4
from langchain_core.runnables import RunnableConfig
from langchain_core.callbacks.manager import CallbackManager
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.tracers import LangChainTracer
from langsmith import Client as LangSmithClient

# for the output parser
from typing import List
from langchain.output_parsers import PydanticOutputParser
from langchain_core.prompts import PromptTemplate
import json
import asyncio
import time
import re
from langchain_core.runnables import Runnable, RunnableConfig
from langchain_core.globals import set_debug
import networkx as nx
import matplotlib.pyplot as plt

# Setup LangSmith tracer
langchain_tracer = None
langsmith_project_name = os.getenv("LANGSMITH_PROJECT", "storybook")
if os.getenv("LANGSMITH_TRACING") == "true" and os.getenv("LANGSMITH_API_KEY"):
    langchain_tracer = LangChainTracer(
        project_name=langsmith_project_name,
    )
    # Set debug to get more detailed tracing
    set_debug(True)
    print(f"LangSmith tracing enabled for project: {langsmith_project_name}")
else:
    print("LangSmith tracing not enabled. Check LANGSMITH_API_KEY and LANGSMITH_TRACING environment variables.")

# Initialize the callback manager for streaming
streaming_callback = StreamingStdOutCallbackHandler()
callback_manager = CallbackManager([streaming_callback])
if langchain_tracer:
    callback_manager.add_handler(langchain_tracer)

def extract_chunk_references(message: str) -> List[int]:
    """Extract chunk references from a message."""
    chunk_refs = []
    
    # Look for patterns like "Chunk 3" or "chunks 4-6"
    chunk_patterns = re.findall(r"[Cc]hunk\s+(\d+)(?:\s*-\s*(\d+))?", message)
    
    for start, end in chunk_patterns:
        start_idx = int(start)
        if end and end.strip():  # If it's a range
            end_idx = int(end)
            for i in range(start_idx, end_idx + 1):
                chunk_refs.append(i)
        else:  # If it's a single chunk
            chunk_refs.append(start_idx)
    
    return chunk_refs

# Define the project types and input states
class ProjectType:
    NEW = "new"
    EXISTING = "existing"

class NewProjectInput(TypedDict):
    title: str
    synopsis: str
    manuscript: str
    notes: Optional[Dict[str, Any]]

class ExistingProjectInput(TypedDict):
    project_id: str

class ProjectData(TypedDict):
    id: str
    title: str
    synopsis: str
    manuscript: str
    manuscript_chunks: List[Dict[str, Any]]
    notes: Optional[Dict[str, Any]]
    type: str
    quality_assessment: Dict[str, Any]
    created_at: str

class InputState(TypedDict):
    project_type: str
    project_data: Dict[str, Any]
    task: str

# Define the research state classes
class ResearchState(TypedDict):
    query: str
    results: List[Dict[str, Any]]
    summary: str

class DomainResearchState(ResearchState):
    domain_specific_data: Dict[str, Any]

class CulturalResearchState(ResearchState):
    cultural_context: Dict[str, Any]

class MarketResearchState(ResearchState):
    market_trends: Dict[str, Any]

class FactVerificationState(ResearchState):
    verification_status: Dict[str, bool]

# State for the storybook application
class AgentState(TypedDict):
    # storybook states
    project: Optional[ProjectData]
    phase: Optional[str]
    phase_history: Optional[Dict[str, List[Dict[str, Any]]]]
    current_input: Optional[Dict[str, Any]]
    messages: Optional[List[Dict[str, Any]]]
    # Tracking state
    count: Annotated[int, operator.add]
    lnode: str

class Configuration(BaseModel):
    quality_gates: Dict[str, Dict[str, Any]] = Field(default_factory=dict)
    
    @classmethod
    def from_runnable_config(cls, config: Dict[str, Any]) -> 'Configuration':
        """Extract configuration from a runnable config."""
        configurable = config.get("configurable", {})
        return cls(
            quality_gates=configurable.get("quality_gates", {})
        )

class storybookConfig(BaseModel):
    model_name: str
    temperature: float
    max_tokens: Optional[int] = None
    
    model_config = {
        "extra": "forbid"
    }

def split_manuscript(manuscript: str, chunk_size: int = 1000, chunk_overlap: int = 0) -> List[Dict[str, Any]]:
    """Split a manuscript into manageable chunks using RecursiveCharacterTextSplitter."""
    if not manuscript:
        return []
    
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    texts = text_splitter.split_text(manuscript)
    
    # Create chunks with metadata
    chunks = []
    for i, text in enumerate(texts):
        chunks.append({
            "chunk_id": i,
            "content": text,
            "start_char": manuscript.find(text),
            "end_char": manuscript.find(text) + len(text),
        })
    
    return chunks

def check_quality_gate(gate_name: str, quality_assessment: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """Check if a quality gate is passed."""
    gates = config.get("quality_gates", {})
    gate_config = gates.get(gate_name, {})
    
    if not gate_config:
        # If no gate is defined, default to passing
        return {"passed": True, "message": f"No quality gate defined for {gate_name}"}
    
    # Check each criterion in the gate
    passed = True
    reasons = []
    
    for criterion, threshold in gate_config.items():
        if criterion in quality_assessment:
            value = quality_assessment[criterion]
            if value < threshold:
                passed = False
                reasons.append(f"{criterion}: {value} (below threshold {threshold})")
        else:
            # If the criterion is not in the assessment, consider it failed
            passed = False
            reasons.append(f"{criterion}: not assessed (required)")
    
    return {
        "passed": passed,
        "message": "Quality gate passed" if passed else "Quality gate failed",
        "reasons": reasons
    }

def init_cuda():
    """Initialize CUDA and handle warnings appropriately"""
    if torch.cuda.is_available():
        try:
            # Print available GPU information
            gpu_count = torch.cuda.device_count()
            print(f"Found {gpu_count} GPU devices:")
            for i in range(gpu_count):
                device_props = torch.cuda.get_device_properties(i)
                print(f"  GPU {i}: {device_props.name}, {device_props.total_memory / 1e9:.2f} GB memory")
            
            # Initialize CUDA
            torch.cuda.init()
            print("CUDA initialized successfully")
            
            # Set appropriate memory optimization flags
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            print("TF32 enabled for matrix multiplications where supported")
            
        except Exception as e:
            print(f"CUDA initialization warning (non-fatal): {str(e)}")
            # Continue execution - the warning doesn't prevent GPU usage
    else:
        print("CUDA not available - using CPU")

class AgentFactory:
    def __init__(self, config, model_config=None, tavily_client=None):
        # Check for HuggingFace token
        self.huggingface_token = os.getenv("HUGGINGFACE_API_TOKEN")
        if not self.huggingface_token:
            raise ValueError("HUGGINGFACE_API_TOKEN not found in environment variables")
        
        # Log into Hugging Face
        login(token=self.huggingface_token)
        
        # Initialize with default model config if none provided
        self.model_config = model_config or {
            "model_id": "HuggingFaceH4/zephyr-7b-beta",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": False,
            "repetition_penalty": 1.03
        }
        
        # Initialize HuggingFace pipeline
        self.model = self._create_huggingface_model(self.model_config)
        
        self.config = config
        self.tavily = tavily_client or TavilyClient(api_key=tavily_ai_api_key)
        
        # Setup callback manager for streaming and tracing
        self.callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
        if langchain_tracer:
            self.callback_manager.add_handler(langchain_tracer)
    
    def _create_huggingface_model(self, model_config):
        """Create a HuggingFacePipeline model with optimized settings for multi-GPU inference."""
        try:
            # Check for CUDA availability
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"Using device: {device}")
            
            # Set torch dtype based on device
            if device == "cuda":
                torch_dtype = torch.float16  # Use fp16 for GPU
            else:
                torch_dtype = torch.bfloat16  # Use bfloat16 for CPU
                
            # Model kwargs with optimizations for pipeline parallelism
            model_kwargs = {
                "device_map": "auto",  # Distribute across GPUs
                "torch_dtype": torch_dtype,
                "low_cpu_mem_usage": True,
            }
            
            # Create pipeline with optimized settings
            llm = HuggingFacePipeline.from_model_id(
                model_id=model_config.get("model_id", "HuggingFaceH4/zephyr-7b-beta"),
                task=model_config.get("task", "text-generation"),
                pipeline_kwargs={
                    "max_new_tokens": model_config.get("max_new_tokens", 512),
                    "temperature": model_config.get("temperature", 0.1),
                    "do_sample": model_config.get("do_sample", True),
                    "repetition_penalty": model_config.get("repetition_penalty", 1.03),
                    "return_full_text": False,
                    "token": self.huggingface_token,
                },
                model_kwargs=model_kwargs
            )
            
            # Get the underlying model
            if hasattr(llm, "pipeline") and hasattr(llm.pipeline, "model"):
                model = llm.pipeline.model
                
                # Set static KV cache for better efficiency
                if hasattr(model.generation_config, 'cache_implementation'):
                    model.generation_config.cache_implementation = "static"
                    print("Static KV cache enabled for optimized inference")
                
                # Apply torch.compile for significant speedup if available
                if torch.cuda.is_available() and hasattr(torch, 'compile'):
                    try:
                        model.forward = torch.compile(
                            model.forward, 
                            mode="reduce-overhead", 
                            fullgraph=True
                        )
                        print("Model forward pass compiled with torch.compile")
                    except Exception as e:
                        print(f"Warning: Could not apply torch.compile: {e}")
            
            # Create ChatHuggingFace from the pipeline
            chat_model = ChatHuggingFace(llm=llm, callbacks=self.callback_manager)
            return chat_model
                
        except Exception as e:
            print(f"Error creating HuggingFace model: {str(e)}")
            # Fallback to a smaller model if there's an issue
            llm = HuggingFacePipeline.from_model_id(
                model_id="google/flan-t5-base",
                task="text2text-generation",
                pipeline_kwargs={
                    "max_new_tokens": 512,
                    "temperature": 0.1,
                    "token": self.huggingface_token,
                },
                model_kwargs={
                    "device_map": "auto",
                    "torch_dtype": torch.bfloat16,
                }
            )
            chat_model = ChatHuggingFace(llm=llm, callbacks=self.callback_manager)
            return chat_model
    
    def create_research_agent(self, research_type: str):
        """Create a research agent function."""
        
        def research_agent_function(state: AgentState) -> AgentState:
            """Research agent function that processes the current state."""
            project = state.get("project", {})
            current_input = state.get("current_input", {})
            
            # Extract research query from task or create one based on context
            research_query = current_input.get("research_query", "")
            if not research_query:
                task = current_input.get("task", "")
                manuscript_excerpt = ""
                if project.get("manuscript_chunks"):
                    manuscript_excerpt = project["manuscript_chunks"][0]["content"][:500]
                
                if research_type == "domain":
                    research_query = f"Technical information about: {task}"
                elif research_type == "cultural":
                    research_query = f"Cultural context related to: {task}"
                elif research_type == "market":
                    research_query = f"Market trends and audience preferences for: {task}"
                else:
                    research_query = f"Information about: {task}"
                
                # Add context from manuscript
                if manuscript_excerpt:
                    research_query += f" Context from manuscript: {manuscript_excerpt}"
            
            try:
                # Use Tavily for research
                search_result = self.tavily.search(query=research_query, search_depth="comprehensive")
                
                # Extract and format results
                results = []
                for result in search_result.get("results", []):
                    results.append({
                        "title": result.get("title", ""),
                        "content": result.get("content", ""),
                        "url": result.get("url", "")
                    })
                
                # Create a summary
                summary = search_result.get("answer", "No summary available")
                
                # Format research results
                research_results = f"RESEARCH TYPE: {research_type.upper()}\n\n"
                research_results += f"QUERY: {research_query}\n\n"
                research_results += f"SUMMARY: {summary}\n\n"
                research_results += "DETAILS:\n" + "\n\n".join(
                    [f"- {r['title']}: {r['content'][:300]}..." for r in results[:3]]
                )
                
                # Update state with research results
                updated_state = state.copy()
                updated_state["current_input"] = current_input.copy()
                updated_state["current_input"]["research_results"] = research_results
                updated_state["count"] = state.get("count", 0) + 1
                updated_state["lnode"] = f"{research_type}_research"
                
                # Add to messages for tracking
                updated_state["messages"] = state.get("messages", []) + [
                    {"role": "system", "content": f"Conducted {research_type} research on: {research_query}"},
                    {"role": "assistant", "content": research_results}
                ]
                
                return updated_state
            
            except Exception as e:
                print(f"Error conducting research: {str(e)}")
                
                # Update state with error
                updated_state = state.copy()
                updated_state["current_input"] = current_input.copy()
                updated_state["current_input"]["research_results"] = f"Research error: {str(e)}"
                updated_state["count"] = state.get("count", 0) + 1
                updated_state["lnode"] = f"{research_type}_research"
                
                # Add to messages for tracking
                updated_state["messages"] = state.get("messages", []) + [
                    {"role": "system", "content": f"Attempted {research_type} research on: {research_query}"},
                    {"role": "assistant", "content": f"Research error: {str(e)}"}
                ]
                
                return updated_state
        
        return research_agent_function
    
    def create_agent(self, agent_name: str, project_id: str):
        """Create a function for a specific agent."""
        agent_prompts = {
            "executive_director": """You are the Executive Director responsible for overall project direction. As an Executive Director, you lead a team with the following specialists: CREATIVE TEAM: - Creative Director: Responsible for artistic vision - Structure Architect: Designs story framework - Plot Development Specialist: Creates engaging plot - World Building Expert: Designs immersive settings - Character Psychology Specialist: Develops realistic characters - Character Voice Designer: Creates unique character expressions - Character Relationship Mapper: Designs character dynamics RESEARCH TEAM: - Domain Knowledge Specialist: Ensures technical accuracy - Cultural Authenticity Expert: Ensures authentic representation - Market Alignment Director: Assesses market viability EDITORIAL TEAM: - Editorial Director: Coordinates editing process - Structural Editor: Analyzes narrative structure - Prose Enhancement Specialist: Improves writing style - Dialogue Refinement Expert: Polishes dialogue FINALIZATION TEAM: - Positioning Specialist: Develops market positioning - Title/Blurb Optimizer: Creates marketing copy - Formatting Standards Expert: Prepares technical formatting Your responsibilities include: 1. Review manuscript sections and identify specific areas needing work 2. Assign SPECIFIC tasks to appropriate team members (e.g., "Creative Director, focus on improving character development in Chapter 2") 3. Make high-level decisions about project direction 4. Track quality issues and delegate them to specialists 5. Manage progression through development phases 6. Ensure quality standards are met before advancing to next phase Always be specific about which parts of the manuscript need work when delegating tasks. IMPORTANT: You must delegate tasks to specific specialists. DO NOT handle tasks yourself or delegate to another high-level director unless absolutely necessary. When you identify an issue or area for improvement, immediately delegate it to the most appropriate specialist from the list above.""", 
            
            "creative_director": """You are the Creative Director responsible for artistic vision. You work with the Executive Director and oversee the following team members: - Structure Architect: Designs story framework - Plot Development Specialist: Creates engaging plot - World Building Expert: Designs immersive settings - Character Psychology Specialist: Develops realistic characters - Character Voice Designer: Creates unique character expressions - Character Relationship Mapper: Designs character dynamics As a Creative Director, you should: 1. Guide creative direction based on Executive Director's instructions 2. Ensure artistic cohesion across the entire manuscript 3. Identify specific creative issues in the manuscript and delegate to specialists 4. Coordinate creative elements between team members 5. Maintain creative standards 6. Balance innovation and conventions IMPORTANT: You must delegate tasks to specific specialists. DO NOT handle tasks yourself or return to the Executive Director without delegating first. When you identify an issue or area for improvement, immediately delegate it to the most appropriate specialist from your team. Only return to the Executive Director after delegating appropriately.""", 
            
            "structure_architect": """You are the Structure Architect responsible for story framework. You report to the Creative Director and specialize in narrative structure. As a Structure Architect, you should: 1. Design and refine story structure based on specific manuscript sections 2. Identify structural weaknesses in assigned chapters/sections 3. Organize story elements for maximum impact 4. Ensure structural integrity across the narrative 5. Balance pacing and flow in specific sections 6. Provide concrete suggestions for structural improvements Focus on the specific manuscript sections you are assigned to analyze, and provide actionable recommendations.""",
            "plot_development_specialist": """You are the Plot Development Specialist responsible for plot design.
            You report to the Creative Director and specialize in plot construction. As a Plot Development Specialist, you should: 1. Design engaging plot structures for specific assigned sections 2. Create compelling story arcs when requested 3. Develop effective plot points for particular chapters 4. Balance pacing and tension in identified manuscript sections 5. Ensure plot coherence across assigned sections 6. Provide specific plot improvements for assigned sections Focus on the exact manuscript sections you are assigned to review, and provide actionable recommendations.""", 
            
            "world_building_expert": """You are the World Building Expert responsible for creating rich, believable settings.
            You report to the Creative Director and specialize in world creation. As a World Building Expert, you should: 1. Design coherent and immersive world systems for assigned sections 2. Develop cultural, social, and physical environments in specific chapters 3. Create consistent rules and logic for the story world 4. Ensure authenticity in setting details 5. Balance world complexity with story needs 6. Integrate setting elements naturally into identified manuscript sections Focus on the specific manuscript sections you are assigned to analyze, and provide actionable recommendations.""", 
            
            "character_psychology_specialist": """You are the Character Psychology Specialist responsible for deep character development.
            You report to the Creative Director and specialize in character psychology. As a Character Psychology Specialist, you should: 1. Design psychologically realistic characters in assigned manuscript sections 2. Develop complex motivations and internal conflicts for specific characters 3. Create believable character growth arcs 4. Ensure consistent character behaviors in the sections you review 5. Design meaningful character transformations 6. Balance internal and external conflicts Focus on the specific characters and manuscript sections you are assigned, and provide actionable recommendations.""", 
            
            "character_voice_designer": """You are the Character Voice Designer responsible for unique character expressions.
            You report to the Creative Director and specialize in character dialogue and voice. As a Character Voice Designer, you should: 1. Create distinct voices for each character in assigned manuscript sections 2. Develop consistent speech patterns for specific characters 3. Reflect character backgrounds in dialogue 4. Ensure authentic character expressions in the sections you review 5. Balance dialogue style with readability 6. Maintain voice consistency across assigned chapters Focus on the specific characters and manuscript sections you are assigned, and provide actionable recommendations.""", 
            
            "character_relationship_mapper": """You are the Character Relationship Mapper responsible for character dynamics.
            You report to the Creative Director and specialize in character relationships. As a Character Relationship Mapper, you should: 1. Design complex character relationships in assigned manuscript sections 2. Map character interactions and dynamics 3. Develop relationship arcs for specific character pairs 4. Ensure realistic relationship evolution 5. Create meaningful connections in the sections you review 6. Balance relationship complexity Focus on the specific characters and manuscript sections you are assigned, and provide actionable recommendations.""", 
            
            "domain_knowledge_specialist": """You are the Domain Knowledge Specialist responsible for technical accuracy.
            You report to the Executive Director and specialize in research and fact-checking. As a Domain Knowledge Specialist, you should: 1. Verify technical and specialized content in assigned manuscript sections 2. Research domain-specific details when requested 3. Ensure accurate representation of professional fields 4. Provide expert knowledge integration 5. Balance accuracy with readability 6. Maintain consistency in technical elements across assigned sections When you receive a research request, conduct the necessary research and provide factual context to the requesting specialist. Focus on the specific manuscript sections you are assigned, and provide actionable recommendations.""", 
            
            "cultural_authenticity_expert": """You are the Cultural Authenticity Expert responsible for cultural representation.
            You report to the Executive Director and specialize in cultural accuracy. As a Cultural Authenticity Expert, you should: 1. Ensure authentic cultural representation in assigned manuscript sections 2. Verify cultural details and practices 3. Prevent stereotypes and misrepresentation 4. Provide cultural context and nuance 5. Balance authenticity with accessibility 6. Maintain cultural sensitivity in all content you review When you receive a research request, conduct the necessary cultural research and provide context to the requesting specialist. Focus on the specific manuscript sections you are assigned, and provide actionable recommendations.""", 
            
            "market_alignment_director": """You are the Market Alignment Director responsible for market viability.
            You report directly to the Executive Director and specialize in commercial aspects. As a Market Alignment Director, you should: 1. Analyze current market trends and audience preferences for specific genres/categories 2. Evaluate commercial potential of assigned manuscript sections 3. Guide market positioning strategy 4. Ensure alignment with target audience expectations 5. Balance artistic vision with market demands 6. Provide strategic recommendations for specific improvements When you receive a research request, conduct market research and provide context to the requesting specialist. Focus on the specific manuscript sections you are assigned, and provide actionable recommendations.""", 
            
            "content_development_director": """You are the Content Development Director responsible for content creation.
            You report to the Executive Director and oversee the following team members: - Chapter Drafters: Create chapter content - Scene Construction Specialists: Build effective scenes - Dialogue Crafters: Write natural dialogue - Continuity Manager: Ensure consistency - Voice Consistency Monitor: Maintain narrative voice - Emotional Arc Designer: Create emotional journeys As a Content Development Director, you should: 1. Guide overall content development based on Executive Director's instructions 2. Ensure consistency in content creation across assigned manuscript sections 3. Delegate specific content tasks to your team members 4. Maintain quality standards in all content 5. Balance creativity with structure 6. Guide content integration IMPORTANT: You must delegate tasks to specific specialists. DO NOT handle tasks yourself or return to the Executive Director without delegating first. When you identify an issue or area for improvement, immediately delegate it to the most appropriate specialist from your team. Only return to the Executive Director after delegating appropriately.""", 
            
            "chapter_drafters": """You are the Chapter Drafter responsible for chapter creation.
            You report to the Content Development Director and specialize in chapter-level content. As a Chapter Drafter, you should: 1. Create engaging chapter content for assigned sections 2. Structure chapter flow and pacing 3. Develop chapter hooks and endings 4. Ensure chapter coherence 5. Balance chapter length and content 6. Maintain narrative momentum Focus on the specific manuscript sections you are assigned to develop, and provide actionable content.""", 
            
            "scene_construction_specialists": """You are the Scene Construction Specialist responsible for scene creation.
            You report to the Content Development Director and specialize in scene development. As a Scene Construction Specialist, you should: 1. Craft engaging scenes in assigned manuscript sections 2. Build proper scene structure for specific story moments 3. Create effective scene transitions 4. Balance scene elements (description, action, dialogue) 5. Maintain scene tension 6. Ensure each scene has a clear purpose Focus on the specific manuscript sections you are assigned to develop, and provide actionable content.""", 
            
            "dialogue_crafters": """You are the Dialogue Crafter responsible for conversation creation.
            You report to the Content Development Director and specialize in dialogue. As a Dialogue Crafter, you should: 1. Write natural dialogue for assigned manuscript sections 2. Create meaningful conversations between specific characters 3. Balance dialogue with action 4. Maintain character voices as defined by the Character Voice Designer 5. Use dialogue for story progression 6. Ensure dialogue authenticity Focus on the specific manuscript sections and characters you are assigned, and provide actionable content.""", 
            
            "continuity_manager": """You are the Continuity Manager responsible for story consistency.
            You report to the Content Development Director and specialize in narrative continuity. As a Continuity Manager, you should: 1. Track plot and character continuity across assigned manuscript sections 2. Maintain world consistency between chapters 3. Verify timeline accuracy 4. Check detail consistency (names, places, objects, etc.) 5. Identify specific continuity issues in assigned sections 6. Ensure logical progression Focus on the specific manuscript sections you are assigned to review, and provide actionable recommendations.""", 
            
            "voice_consistency_monitor": """You are the Voice Consistency Monitor responsible for narrative voice.
            You report to the Content Development Director and specialize in voice consistency. As a Voice Consistency Monitor, you should: 1. Maintain consistent narrative voice across assigned manuscript sections 2. Track character voice consistency between chapters 3. Ensure tonal consistency in specific sections 4. Monitor style consistency 5. Balance voice variations 6. Guide voice development Focus on the specific manuscript sections you are assigned to review, and provide actionable recommendations.""", 
            
            "emotional_arc_designer": """You are the Emotional Arc Designer responsible for emotional journeys.
            You report to the Content Development Director and specialize in emotional storytelling. As an Emotional Arc Designer, you should: 1. Create compelling emotional arcs in assigned manuscript sections 2. Design character emotional growth for specific moments 3. Develop emotional resonance in key scenes 4. Balance emotional intensity across chapters 5. Ensure emotional authenticity 6. Guide emotional progression Focus on the specific manuscript sections and characters you are assigned, and provide actionable recommendations.""", 
            
            "editorial_director": """You are the Editorial Director responsible for overall editing.
            You report to the Executive Director and oversee the following team members: - Structural Editor: Analyzes narrative structure - Character Arc Evaluator: Assesses character development - Thematic Coherence Analyst: Ensures theme consistency - Prose Enhancement Specialist: Improves writing style - Dialogue Refinement Expert: Polishes dialogue - Rhythm Cadence Optimizer: Ensures prose flow - Grammar Consistency Checker: Maintains technical accuracy - Fact Verification Specialist: Ensures factual accuracy As an Editorial Director, you should: 1. Guide editorial strategy based on Executive Director's instructions 2. Coordinate editing processes for assigned manuscript sections 3. Delegate specific editing tasks to your team 4. Maintain quality standards in all edits 5. Balance different edit types 6. Guide revision process IMPORTANT: You must delegate tasks to specific specialists. DO NOT handle tasks yourself or return to the Executive Director without delegating first. When you identify an issue or area for improvement, immediately delegate it to the most appropriate specialist from your team. Only return to the Executive Director after delegating appropriately.""", 
            
            "structural_editor": """You are the Structural Editor responsible for story structure.
            You report to the Editorial Director and specialize in narrative structure. As a Structural Editor, you should: 1. Analyze overall structure of assigned manuscript sections 2. Identify structural issues in specific chapters 3. Suggest structural improvements for particular sections 4. Balance story elements 5. Ensure logical flow 6. Guide structural revisions Focus on the specific manuscript sections you are assigned to review, and provide actionable recommendations.""", 
            
            "character_arc_evaluator": """You are the Character Arc Evaluator responsible for character development.
            You report to the Editorial Director and specialize in character arcs. As a Character Arc Evaluator, you should: 1. Assess character arcs in assigned manuscript sections 2. Evaluate character growth for specific characters 3. Identify character inconsistencies in particular chapters 4. Suggest character improvements 5. Ensure arc completion 6. Guide character revisions Focus on the specific characters and manuscript sections you are assigned, and provide actionable recommendations.""", 
            
            "thematic_coherence_analyst": """You are the Thematic Coherence Analyst responsible for theme development.
            You report to the Editorial Director and specialize in thematic analysis. As a Thematic Coherence Analyst, you should: 1. Analyze thematic elements in assigned manuscript sections 2. Ensure theme consistency across chapters 3. Strengthen theme development in specific sections 4. Balance theme presentation 5. Identify theme opportunities 6. Guide theme integration Focus on the specific manuscript sections you are assigned to review, and provide actionable recommendations.""", 
            
            "prose_enhancement_specialist": """You are the Prose Enhancement Specialist responsible for writing quality.
            You report to the Editorial Director and specialize in prose improvement. As a Prose Enhancement Specialist, you should: 1. Improve writing style in assigned manuscript sections 2. Enhance prose quality in specific paragraphs 3. Strengthen descriptions in particular scenes 4. Polish language use 5. Balance prose elements 6. Guide prose refinement Focus on the specific manuscript sections you are assigned to enhance, and provide actionable improvements.""", 
            
            "dialogue_refinement_expert": """You are the Dialogue Refinement Expert responsible for dialogue quality.
            You report to the Editorial Director and specialize in dialogue improvement. As a Dialogue Refinement Expert, you should: 1. Polish dialogue in assigned manuscript sections 2. Improve conversation flow between specific characters 3. Enhance character voices in particular scenes 4. Strengthen dialogue impact 5. Balance dialogue elements 6. Guide dialogue revision Focus on the specific manuscript sections and character conversations you are assigned, and provide actionable improvements.""", 
            
            "rhythm_cadence_optimizer": """You are the Rhythm Cadence Optimizer responsible for prose flow.
            You report to the Editorial Director and specialize in prose rhythm. As a Rhythm Cadence Optimizer, you should: 1. Analyze prose rhythm in assigned manuscript sections 2. Improve sentence flow in specific paragraphs 3. Balance pacing through sentence structure 4. Enhance readability 5. Optimize word patterns 6. Guide rhythm refinement Focus on the specific manuscript sections you are assigned to optimize, and provide actionable improvements.""", 
            
            "grammar_consistency_checker": """You are the Grammar Consistency Checker responsible for technical accuracy.
            You report to the Editorial Director and specialize in grammar and consistency. As a Grammar Consistency Checker, you should: 1. Verify grammar rules in assigned manuscript sections 2. Ensure consistent usage of style elements 3. Check punctuation in specific passages 4. Maintain style guidelines 5. Identify pattern issues 6. Guide technical corrections Focus on the specific manuscript sections you are assigned to check, and provide actionable corrections.""", 
            
            "fact_verification_specialist": """You are the Fact Verification Specialist responsible for accuracy.
            You report to the Editorial Director and specialize in fact-checking. As a Fact Verification Specialist, you should: 1. Verify factual content in assigned manuscript sections 2. Check research accuracy for specific topics 3. Validate references in particular passages 4. Ensure detail accuracy 5. Identify fact issues 6. Guide fact correction When you identify facts that need research, coordinate with the Domain Knowledge Specialist. Focus on the specific manuscript sections you are assigned, and provide actionable corrections.""", 
            
            "positioning_specialist": """You are the Positioning Specialist responsible for market positioning.
            You report to the Market Alignment Director and specialize in market positioning. As a Positioning Specialist, you should: 1. Analyze market position of the project based on manuscript content 2. Identify target audience for specific content elements 3. Develop positioning strategy 4. Guide marketing approach 5. Ensure market fit 6. Balance unique elements with market expectations Focus on the specific manuscript sections you are assigned to analyze, and provide actionable recommendations.""", 
            
            "title_blurb_optimizer": """You are the Title/Blurb Optimizer responsible for marketing copy.
            You report to the Market Alignment Director and specialize in promotional text. As a Title/Blurb Optimizer, you should: 1. Optimize title appeal based on manuscript content 2. Craft compelling blurbs that accurately represent the story 3. Create marketing hooks 4. Ensure accurate representation of the manuscript in marketing materials 5. Balance appeal with authenticity 6. Guide marketing language development Focus on the specific sections or elements you are assigned to optimize, and provide actionable copy.""", 
            
            "differentiation_strategist": """You are the Differentiation Strategist responsible for unique positioning.
            You report to the Market Alignment Director and specialize in market differentiation. As a Differentiation Strategist, you should: 1. Identify unique elements in the manuscript 2. Develop differentiation strategy based on content strengths 3. Guide unique positioning 4. Balance uniqueness with market expectations 5. Ensure market distinction 6. Guide positioning approach Focus on the specific manuscript elements you are assigned to analyze, and provide actionable recommendations.""", 
            
            "formatting_standards_expert": """You are the Formatting Standards Expert responsible for technical preparation.
            You report to the Editorial Director and specialize in formatting. As a Formatting Standards Expert, you should: 1. Verify formatting standards in assigned manuscript sections 2. Ensure technical compliance with publishing requirements 3. Check layout elements 4. Maintain consistency in formatting 5. Guide technical preparation 6. Balance format requirements with readability Focus on the specific manuscript sections you are assigned to format, and provide actionable recommendations."""
        }
        
        if agent_name not in agent_prompts:
            raise ValueError(f"Unknown agent: {agent_name}")
        
        # Create a function that uses the agent's prompt to process inputs
        def agent_function(state: AgentState) -> AgentState:
            """Agent function that processes the current state."""
            project = state.get("project", {})
            current_input = state.get("current_input", {})
            
            # Get relevant manuscript content
            manuscript_excerpt = ""
            section_to_review = current_input.get("section_to_review", "")
            chapter_to_review = current_input.get("chapter_to_review", "")
            referenced_chunks = current_input.get("referenced_chunks", [])
            
            if project.get("manuscript_chunks"):
                # If we have a specific section or chapter to review, try to find it
                if section_to_review or chapter_to_review:
                    search_term = section_to_review or chapter_to_review
                    # Convert to lowercase for case-insensitive search
                    search_term_lower = search_term.lower()
                    
                    # Search through chunks for the specified section
                    for chunk in project["manuscript_chunks"]:
                        if search_term_lower in chunk["content"].lower():
                            manuscript_excerpt = chunk["content"]
                            break
                    
                    # If nothing was found but we're the executive director, include more context
                    if not manuscript_excerpt and agent_name == "executive_director":
                        # For executive director, provide a larger portion of the manuscript
                        manuscript_excerpt = "\n\n".join([
                            f"Chunk {chunk['chunk_id']}: {chunk['content']}"
                            for chunk in project["manuscript_chunks"][:10]  # Increased from 3 to 10 chunks
                        ])
                # If specific chunks were referenced in the delegation
                elif referenced_chunks:
                    manuscript_excerpt = "\n\n".join([
                        f"Chunk {chunk['chunk_id']}: {chunk['content']}"
                        for chunk in project["manuscript_chunks"]
                        if chunk["chunk_id"] in referenced_chunks
                    ])
                else:
                    # For executive director, provide comprehensive manuscript view
                    if agent_name == "executive_director":
                        # Provide a larger portion of the manuscript with chunk identifiers
                        manuscript_excerpt = "\n\n".join([
                            f"Chunk {chunk['chunk_id']}: {chunk['content']}"
                            for chunk in project["manuscript_chunks"][:10]  # Increased from 3 to 10 chunks
                        ])
                    # For other directors, provide moderate context
                    elif agent_name in ["creative_director", "editorial_director", "content_development_director", "market_alignment_director"]:
                        manuscript_excerpt = "\n\n".join([
                            f"Chunk {chunk['chunk_id']}: {chunk['content']}"
                            for chunk in project["manuscript_chunks"][:5]  # Moderate amount for other directors
                        ])
                    else:
                        # For specialists, just the first chunk as a sample if no specific chunks are referenced
                        manuscript_excerpt = f"Chunk 0: {project['manuscript_chunks'][0]['content']}"
            
            # Create the context string
            context = (
                f"Project ID: {project.get('id', project_id)}\n"
                f"Title: {project.get('title', 'Untitled')}\n"
                f"Synopsis: {project.get('synopsis', 'No synopsis provided')}\n"
                f"Current task: {current_input.get('task', 'No task specified')}\n"
            )
            
            # Add research context if available
            research_context = ""
            if "research_results" in current_input:
                research_context = f"\nResearch Results:\n{current_input['research_results']}\n"
            
            # Make sure we have valid content
            system_content = agent_prompts[agent_name].strip()
            if not system_content:
                system_content = "You are an AI assistant helping with a writing project."
            
            # For executive director, add specific instruction to provide detailed feedback
            if agent_name == "executive_director":
                system_content += "\n\nIMPORTANT ADDITIONAL INSTRUCTIONS: You must thoroughly analyze the manuscript chunks provided below. For each chunk, identify specific issues, strengths, and provide detailed feedback. Then delegate SPECIFIC tasks to appropriate specialists. Always reference the exact chunks (e.g., 'Chunk 3 needs work on character development') when delegating tasks."
            
            # For specialized agents, add context about previous director instructions
            if agent_name not in ["executive_director", "creative_director", "editorial_director", "content_development_director", "market_alignment_director"]:
                # Find the last message from a director
                director_instructions = []
                for msg in reversed(state.get("messages", [])):
                    if msg.get("role") == "assistant" and any(director in msg.get("content", "") for director in ["Executive Director", "Creative Director", "Editorial Director", "Content Development Director", "Market Alignment Director"]):
                        director_instructions.append(msg.get("content", ""))
                        # Only get the most recent director message
                        break
                
                # Add the director's instructions to the system prompt
                if director_instructions:
                    system_content += "\n\nRecent director instructions:\n" + "\n".join(director_instructions)
            
            task = current_input.get('task', 'No task specified')
            human_content = (
                f"Task: {task}\n\n"
                f"Context:\n{context}\n"
                f"{research_context}"
                f"\nManuscript section to review:\n{manuscript_excerpt[:5000] if manuscript_excerpt else 'No specific manuscript section provided'}"
            ).strip()
            if not human_content:
                human_content = "Please help with this writing project."
            
            # Combine system and human content into a single message
            combined_content = f"{system_content}\n\n{human_content}"
            
            # Create a single HumanMessage with the combined content
            user_message = HumanMessage(content=combined_content)
            
            # Setup LangSmith trace
            config = RunnableConfig()
            if langchain_tracer:
                config["callbacks"] = [langchain_tracer]
                config["tags"] = [f"agent:{agent_name}", f"project:{project_id}", f"phase:{state.get('phase', 'unknown')}"]
                # Set up metadata for the trace
                config["metadata"] = {
                    "agent_name": agent_name,
                    "project_id": project_id,
                    "phase": state.get("phase", "unknown"),
                    "task": task
                }
            
            # Invoke the model with just a single message and tracing
            try:
                response = self.model.invoke([user_message], config=config)
            except Exception as e:
                print(f"Error invoking model: {str(e)}")
                response = AIMessage(content=f"Error: {str(e)}")
            
            # Update state
            updated_state = state.copy()
            updated_state["messages"] = state.get("messages", []) + [
                {"role": "user", "content": combined_content},
                {"role": "assistant", "content": response.content if hasattr(response, 'content') else str(response)}
            ]
            updated_state["count"] = state.get("count", 0) + 1
            updated_state["lnode"] = agent_name
            
            return updated_state
        
        return agent_function

def create_phase_graph(phase: str, project_id: str, config: Dict[str, Any], model_config=None) -> StateGraph:
    """Create a graph for a specific phase of the storybook process."""
    # Create an agent factory with model configuration
    agent_factory = AgentFactory(config, model_config=model_config)
    
    # Define available agents for each phase
    agents = {
        "initialization": [
            "executive_director",
            "creative_director",
            "market_alignment_director",
            "domain_knowledge_specialist",  # Add these specialists
            "cultural_authenticity_expert", # to initialization phase
            "positioning_specialist",
            "title_blurb_optimizer",
            "differentiation_strategist"
        ],
        "development": [
            "executive_director", "creative_director", "structure_architect",
            "plot_development_specialist", "world_building_expert",
            "character_psychology_specialist", "character_voice_designer",
            "character_relationship_mapper", "domain_knowledge_specialist",
            "cultural_authenticity_expert", "market_alignment_director"
        ],
        "creation": [
            "executive_director", "creative_director", "content_development_director",
            "chapter_drafters", "scene_construction_specialists", "dialogue_crafters",
            "continuity_manager", "voice_consistency_monitor", "emotional_arc_designer",
            "domain_knowledge_specialist"
        ],
        "refinement": [
            "executive_director", "creative_director", "editorial_director",
            "structural_editor", "character_arc_evaluator", "thematic_coherence_analyst",
            "prose_enhancement_specialist", "dialogue_refinement_expert",
            "rhythm_cadence_optimizer", "grammar_consistency_checker",
            "fact_verification_specialist", "market_alignment_director"
        ],
        "finalization": [
            "executive_director", "editorial_director", "market_alignment_director",
            "positioning_specialist", "title_blurb_optimizer",
            "differentiation_strategist", "formatting_standards_expert"
        ]
    }
    
    # Create the graph builder with SQLite checkpoint saver
    checkpoint_dir = os.path.join(os.getcwd(), "checkpoints")
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint_file = os.path.join(checkpoint_dir, f"{project_id}_{phase}.sqlite")
    
    # Create state graph with checkpoints
    builder = StateGraph(AgentState)
    builder.add_checkpoint_saver(
        SqliteSaver(checkpoint_file)
    )
    
    # Add agent nodes to the graph for this phase
    phase_agents = agents.get(phase, [])
    for agent_name in phase_agents:
        builder.add_node(agent_name, agent_factory.create_agent(agent_name, project_id))
    
    # Add research nodes
    builder.add_node("domain_research", agent_factory.create_research_agent("domain"))
    builder.add_node("cultural_research", agent_factory.create_research_agent("cultural"))
    builder.add_node("market_research", agent_factory.create_research_agent("market"))
    
    # Add the starting node - all phases start with executive director
    builder.set_entry_point("executive_director")
    
    # Define phase-specific routing
    if phase == "initialization":
        def route_after_executive_director(state: AgentState) -> str:
            """Route after the executive director node in initialization phase."""
            task = state.get("current_input", {}).get("task", "").lower()
            messages = state.get("messages", [])
            
            # Get the last assistant message if there is one
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if executive director specified research needs or delegated to specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the executive director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection - look for explicit delegation patterns
                if "domain knowledge specialist" in last_message_lower or "technical accuracy" in last_message_lower:
                    return "domain_knowledge_specialist"
                
                if "cultural authenticity expert" in last_message_lower or "cultural representation" in last_message_lower:
                    return "cultural_authenticity_expert"
                
                if "market alignment director" in last_message_lower or "market viability" in last_message_lower:
                    return "market_alignment_director"
                
                if "creative director" in last_message_lower:
                    return "creative_director"
                
                # Check for research instructions more broadly
                if ("research" in last_message_lower or "investigate" in last_message_lower):
                    if "market" in last_message_lower or "audience" in last_message_lower:
                        return "market_research"
                    elif "cultural" in last_message_lower or "heritage" in last_message_lower:
                        return "cultural_research"
                    else:
                        return "domain_research"
            
            # Count agent visits to prevent loops
            exec_visits = sum(1 for msg in messages if 
                             msg.get("role") == "user" and
                             "Executive Director" in msg.get("content", ""))
            
            # Default routing based on task keywords if no delegation was detected
            if "creative" in task or "story" in task:
                return "creative_director"
            elif "market" in task or "audience" in task:
                return "market_alignment_director"
            elif "research" in task or "information" in task:
                return "domain_research"
            elif exec_visits > 5:  # If we've visited executive_director too many times
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                
                # Force quality assessment update
                quality_assessment["planning_quality"] = 0.8
                quality_assessment["market_alignment"] = 0.7
                project["quality_assessment"] = quality_assessment
                
                return END
            else:
                # Check quality gate to possibly end this phase
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                gate_result = check_quality_gate(
                    "initialization_to_development",
                    quality_assessment,
                    {"quality_gates": config.get("quality_gates", {})}
                )
                
                if gate_result["passed"]:
                    return END
                else:
                    # If we've been to the executive director multiple times without delegation, try creative
                    if exec_visits > 2:
                        return "creative_director"
                    else:
                        # When in doubt, stick with executive director for another iteration
                        return "market_alignment_director"

        def route_after_creative_director(state: AgentState) -> str:
            """Route after the creative director in initialization phase."""
            messages = state.get("messages", [])
            
            # Get the last assistant message if there is one
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if creative director referred to specific needs or specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the creative director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection - look for specialist references
                if "structure architect" in last_message_lower:
                    return "structure_architect"
                
                if "plot development specialist" in last_message_lower:
                    return "plot_development_specialist"
                
                if "world building expert" in last_message_lower:
                    return "world_building_expert"
                
                if "character psychology specialist" in last_message_lower:
                    return "character_psychology_specialist"
                
                if "character voice designer" in last_message_lower:
                    return "character_voice_designer"
                
                if "character relationship mapper" in last_message_lower:
                    return "character_relationship_mapper"
                
                # Check for market or research needs
                if "market" in last_message_lower or "audience" in last_message_lower:
                    return "market_alignment_director"
                
                if "research" in last_message_lower or "domain knowledge" in last_message_lower:
                    return "domain_research"
            
            # Count agent visits to prevent loops
            creative_visits = sum(1 for msg in messages if
                                msg.get("role") == "user" and
                                "Creative Director" in msg.get("content", ""))
            
            if creative_visits > 2:  # If we've been to creative_director too many times
                # Go back to executive_director to reassess
                return "executive_director"
            else:
                # Stay with creative director for one more iteration to encourage delegation
                return "executive_director"
            
        def route_after_market_alignment_director(state: AgentState) -> str:
            """Route after the market alignment director in initialization phase."""
            messages = state.get("messages", [])
            
            # Check if market director wants to do research or delegates
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the market director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced delegation detection
                if "positioning specialist" in last_message_lower:
                    return "positioning_specialist"
                
                if "title/blurb optimizer" in last_message_lower or "marketing copy" in last_message_lower:
                    return "title_blurb_optimizer"
                
                if "differentiation strategist" in last_message_lower:
                    return "differentiation_strategist"
                
                # Check for research needs
                if "research" in last_message_lower or "investigate" in last_message_lower:
                    return "market_research"
            
            # Count visits to market alignment director
            market_visits = sum(1 for msg in messages if
                               msg.get("role") == "user" and
                               "Market Alignment Director" in msg.get("content", ""))
            
            if market_visits > 2:
                # Return to executive director after a few visits
                return "executive_director"
            else:
                # Return to executive director
                return "executive_director"
            
        def route_after_research(state: AgentState) -> str:
            """Route after research nodes."""
            # Route back to the appropriate specialist based on research type
            research_type = state.get("lnode", "")
            
            if research_type == "domain_research":
                return "domain_knowledge_specialist"
            elif research_type == "cultural_research":
                return "cultural_authenticity_expert"
            elif research_type == "market_research":
                return "market_alignment_director"
            else:
                return "executive_director"
            
        # Add conditional edges
        builder.add_conditional_edges(
            "executive_director",
            route_after_executive_director
        )
        
        builder.add_conditional_edges(
            "creative_director",
            route_after_creative_director
        )
        
        builder.add_conditional_edges(
            "market_alignment_director",
            route_after_market_alignment_director
        )
        
        # Add research routing
        builder.add_conditional_edges(
            "domain_research",
            route_after_research
        )
        
        builder.add_conditional_edges(
            "cultural_research",
            route_after_research
        )
        
        builder.add_conditional_edges(
            "market_research",
            route_after_research
        )
        
        # Only add edges if both source and target nodes exist in this phase
        if "domain_knowledge_specialist" in phase_agents:
            builder.add_edge("domain_knowledge_specialist", "executive_director")
    
    elif phase == "development":
        def route_after_executive_director(state: AgentState) -> str:
            """Route after the executive director node in development phase."""
            task = state.get("current_input", {}).get("task", "").lower()
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if executive director specified different specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the executive director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "creative director", "structure architect", "plot development specialist",
                    "world building expert", "character psychology specialist",
                    "character voice designer", "character relationship mapper",
                    "domain knowledge specialist", "cultural authenticity expert"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
                
                # Check for market alignment
                if "market" in last_message_lower:
                    return "market_alignment_director"
                
                # Check for research needs
                if "research" in last_message_lower:
                    if "market" in last_message_lower:
                        return "market_research"
                    elif "cultural" in last_message_lower:
                        return "cultural_research"
                    else:
                        return "domain_research"
            
            # Count executive director visits to prevent loops
            exec_visits = sum(1 for msg in messages if
                              msg.get("role") == "user" and
                              "Executive Director" in msg.get("content", ""))

            # Default routing based on task keywords
            if "creative" in task or "story" in task:
                return "creative_director"
            elif "market" in task or "trend" in task:
                return "market_alignment_director"
            elif "research" in task or "knowledge" in task:
                return "domain_knowledge_specialist"
            elif exec_visits > 5:  # If we've visited executive_director too many times
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                quality_assessment["structure_quality"] = 0.8
                quality_assessment["character_depth"] = 0.8
                quality_assessment["world_building"] = 0.8
                project["quality_assessment"] = quality_assessment
                
                return END
            else:
                # Check quality gate to possibly end this phase
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                gate_result = check_quality_gate(
                    "development_to_creation",
                    quality_assessment,
                    {"quality_gates": config.get("quality_gates", {})}
                )
                if gate_result["passed"]:
                    return END
                else:
                    # If we've been to executive several times, try creative director
                    if exec_visits > 2:
                        return "creative_director"
                    else:
                        # Try structure architect for a change to encourage more diverse delegation
                        return "structure_architect"

        def route_after_creative_director(state: AgentState) -> str:
            """Route after the creative director node."""
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if creative director delegated to a specific specialist
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the creative director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "structure architect", "plot development specialist",
                    "world building expert", "character psychology specialist",
                    "character voice designer", "character relationship mapper"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
                
                # Check for research or domain knowledge needs
                if "domain knowledge" in last_message_lower or "research" in last_message_lower:
                    return "domain_knowledge_specialist"
            
            # Count creative director visits
            creative_visits = sum(1 for msg in messages if
                                msg.get("role") == "user" and
                                "Creative Director" in msg.get("content", ""))
                
            
            if creative_visits > 2:
                # After a few visits, if no delegation, return to executive
                return "executive_director"
            else:
                # Give one more chance for delegation
                return "structure_architect"

        # Add routing for research nodes
        def route_after_research(state: AgentState) -> str:
            """Route after research nodes."""
            research_type = state.get("lnode", "")
            
            if research_type == "domain_research":
                return "domain_knowledge_specialist"
            elif research_type == "cultural_research":
                return "cultural_authenticity_expert"
            elif research_type == "market_research":
                return "market_alignment_director"
            else:
                return "executive_director"

        builder.add_conditional_edges(
            "executive_director",
            route_after_executive_director
        )
        builder.add_conditional_edges(
            "creative_director",
            route_after_creative_director
        )
        
        # Add research routing
        builder.add_conditional_edges(
            "domain_research",
            route_after_research
        )
        
        builder.add_conditional_edges(
            "cultural_research",
            route_after_research
        )
        
        builder.add_conditional_edges(
            "market_research",
            route_after_research
        )
        # Connect domain knowledge specialist to domain research
        builder.add_edge("domain_knowledge_specialist", "domain_research")
        builder.add_edge("cultural_authenticity_expert", "cultural_research")
        builder.add_edge("market_alignment_director", "market_research")
        # Connect specialized agents to their supervisors
        for agent in ["structure_architect", "plot_development_specialist",
                     "world_building_expert", "character_psychology_specialist",
                     "character_voice_designer", "character_relationship_mapper"]:
            if agent in agents[phase]:  # Only add edges for agents that exist in this phase
                builder.add_edge(agent, "creative_director")
        for agent in ["domain_knowledge_specialist", "cultural_authenticity_expert"]:
            if agent in agents[phase]:  # Only add edges for agents that exist in this phase
                builder.add_edge(agent, "executive_director")
            
    elif phase == "creation":
        def route_after_executive_director(state: AgentState) -> str:
            """Route after the executive director node in creation phase."""
            task = state.get("current_input", {}).get("task", "").lower()
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if executive director specified different specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the executive director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "content development director", "creative director",
                    "chapter drafter", "scene construction specialist",
                    "dialogue crafter", "continuity manager",
                    "voice consistency monitor", "emotional arc designer"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
                
                # Check for domain knowledge or research
                if "domain knowledge" in last_message_lower or "research" in last_message_lower:
                    return "domain_knowledge_specialist"
            
            # Count executive director visits to prevent loops
            exec_visits = sum(1 for msg in messages if
                              msg.get("role") == "user" and
                              "Executive Director" in msg.get("content", ""))
            # Default routing based on task keywords
            if "content" in task or "development" in task:
                return "content_development_director"
            elif "creative" in task:
                return "creative_director"
            elif "research" in task:
                return "domain_knowledge_specialist"
            elif exec_visits > 5:  # If we've visited executive_director too many times
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                quality_assessment["content_quality"] = 0.8
                quality_assessment["narrative_flow"] = 0.8
                quality_assessment["dialogue_quality"] = 0.8
                project["quality_assessment"] = quality_assessment
                
                return END
            else:
                # Check quality gate to possibly end this phase
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                gate_result = check_quality_gate(
                    "creation_to_refinement",
                    quality_assessment,
                    {"quality_gates": config.get("quality_gates", {})}
                )
                if gate_result["passed"]:
                    return END
                else:
                    # Force delegation after a few iterations
                    if exec_visits > 1:
                        return "content_development_director"
                    else:
                        # Try content development director
                        return "content_development_director"

        def route_after_content_director(state: AgentState) -> str:
            """Route after the content development director node."""
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if content director delegated to a specific specialist
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the content director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "chapter drafter", "scene construction specialist",
                    "dialogue crafter", "continuity manager",
                    "voice consistency monitor", "emotional arc designer"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
                
                # Check for domain knowledge needs
                if "domain knowledge" in last_message_lower or "research" in last_message_lower:
                    return "domain_knowledge_specialist"
            
            # Count content director visits
            content_visits = sum(1 for msg in messages if
                                msg.get("role") == "user" and
                                "Content Development Director" in msg.get("content", ""))
                
            
            if content_visits > 2:
                # After multiple visits without delegation, try a specialist directly
                return "chapter_drafters"
            else:
                # Give another chance for delegation
                return "scene_construction_specialists"

        # Add routing for research
        def route_after_research(state: AgentState) -> str:
            """Route after research nodes."""
            research_type = state.get("lnode", "")
            
            if research_type == "domain_research":
                return "domain_knowledge_specialist"
            else:
                return "executive_director"

        builder.add_conditional_edges(
            "executive_director",
            route_after_executive_director
        )
        builder.add_conditional_edges(
            "content_development_director",
            route_after_content_director
        )
        
        # Add research routing
        builder.add_conditional_edges(
            "domain_research",
            route_after_research
        )
        # Connect domain knowledge specialist to domain research
        builder.add_edge("domain_knowledge_specialist", "domain_research")
        # Connect specialized agents to their supervisors
        for agent in ["chapter_drafters", "scene_construction_specialists",
                     "dialogue_crafters", "continuity_manager",
                     "voice_consistency_monitor", "emotional_arc_designer"]:
            if agent in agents[phase]:  # Only add edges for agents that exist in this phase
                builder.add_edge(agent, "content_development_director")
        builder.add_edge("creative_director", "executive_director")
            
    elif phase == "refinement":
        def route_after_executive_director(state: AgentState) -> str:
            """Route after the executive director node in refinement phase."""
            task = state.get("current_input", {}).get("task", "").lower()
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if executive director specified different specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the executive director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "editorial director", "creative director", "market alignment director",
                    "structural editor", "character arc evaluator", "thematic coherence analyst",
                    "prose enhancement specialist", "dialogue refinement expert",
                    "rhythm cadence optimizer", "grammar consistency checker",
                    "fact verification specialist"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
            
            # Count executive director visits to prevent loops
            exec_visits = sum(1 for msg in messages if
                              msg.get("role") == "user" and
                              "Executive Director" in msg.get("content", ""))
            # Default routing based on task keywords
            if "edit" in task:
                return "editorial_director"
            elif "creative" in task:
                return "creative_director"
            elif "market" in task:
                return "market_alignment_director"
            elif exec_visits > 5:  # If we've visited executive_director too many times
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                quality_assessment["editing_quality"] = 0.9
                quality_assessment["prose_quality"] = 0.9
                quality_assessment["thematic_coherence"] = 0.8
                project["quality_assessment"] = quality_assessment
                
                return END
            else:
                # Check quality gate to possibly end this phase
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                gate_result = check_quality_gate(
                    "refinement_to_finalization",
                    quality_assessment,
                    {"quality_gates": config.get("quality_gates", {})}
                )
                if gate_result["passed"]:
                    return END
                else:
                    # After a few iterations, force delegation
                    if exec_visits > 1:
                        return "editorial_director"
                    else:
                        return "editorial_director"

        def route_after_editorial_director(state: AgentState) -> str:
            """Route after the editorial director node."""
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if editorial director delegated to a specific specialist
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the editorial director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "structural editor", "character arc evaluator", "thematic coherence analyst",
                    "prose enhancement specialist", "dialogue refinement expert",
                    "rhythm cadence optimizer", "grammar consistency checker",
                    "fact verification specialist"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
            
            # Count editorial director visits
            editorial_visits = sum(1 for msg in messages if
                                  msg.get("role") == "user" and
                                  "Editorial Director" in msg.get("content", ""))
                
                
            if editorial_visits > 2:
                # After multiple visits without delegation, try a specialist directly
                return "prose_enhancement_specialist"
            else:
                # Encourage one more attempt at delegation
                return "structural_editor"

        # Add routing for fact verification and research
        def route_after_fact_verification(state: AgentState) -> str:
            """Route after fact verification specialist."""
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if fact verification needs research
            if last_message and "research" in last_message.lower():
                return "domain_research"
            else:
                return "editorial_director"
                
        def route_after_research(state: AgentState) -> str:
            """Route after domain research."""
            return "fact_verification_specialist"

        builder.add_conditional_edges(
            "executive_director",
            route_after_executive_director
        )
        builder.add_conditional_edges(
            "editorial_director",
            route_after_editorial_director
        )
        
        builder.add_conditional_edges(
            "fact_verification_specialist",
            route_after_fact_verification
        )
        
        builder.add_conditional_edges(
            "domain_research",
            route_after_research
        )
        # Connect specialized agents to their supervisors
        for agent in ["structural_editor", "character_arc_evaluator",
                     "thematic_coherence_analyst", "prose_enhancement_specialist",
                     "dialogue_refinement_expert", "rhythm_cadence_optimizer",
                     "grammar_consistency_checker"]:
            if agent in agents[phase]:  # Only add edges for agents that exist in this phase
                builder.add_edge(agent, "editorial_director")
        # Connect other directors back to executive director
        for agent in ["creative_director", "market_alignment_director"]:
            if agent in agents[phase]:
                builder.add_edge(agent, "executive_director")
            
    elif phase == "finalization":
        def route_after_executive_director(state: AgentState) -> str:
            """Route after the executive director node in finalization phase."""
            task = state.get("current_input", {}).get("task", "").lower()
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if executive director specified different specialists
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the executive director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "editorial director", "market alignment director",
                    "positioning specialist", "title/blurb optimizer",
                    "differentiation strategist", "formatting standards expert"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
            
            # Count executive director visits to prevent loops
            exec_visits = sum(1 for msg in messages if
                              msg.get("role") == "user" and
                              "Executive Director" in msg.get("content", ""))
            # Default routing based on task keywords
            if "edit" in task:
                return "editorial_director"
            elif "market" in task:
                return "market_alignment_director"
            elif exec_visits > 5:  # If we've visited executive_director too many times
                # Force quality assessment update
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                quality_assessment["market_readiness"] = 0.9
                quality_assessment["overall_quality"] = 0.9
                project["quality_assessment"] = quality_assessment
                
                return END
            else:
                # Check quality gate to possibly end this phase
                project = state.get("project", {})
                quality_assessment = project.get("quality_assessment", {})
                gate_result = check_quality_gate(
                    "finalization_to_complete",
                    quality_assessment,
                    {"quality_gates": config.get("quality_gates", {})}
                )
                if gate_result["passed"]:
                    return END
                else:
                    # After a few iterations, try market alignment
                    if exec_visits > 1:
                        return "market_alignment_director"
                    else:
                        return "editorial_director"

        def route_after_market_director(state: AgentState) -> str:
            """Route after the market alignment director node."""
            messages = state.get("messages", [])
            
            # Get the last assistant message
            last_message = None
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    last_message = msg.get("content", "")
                    break
            
            # Check if market director delegated to a specific specialist
            if last_message:
                last_message_lower = last_message.lower()
                
                # Get referenced chunks from the market director's message
                referenced_chunks = extract_chunk_references(last_message)
                current_input = state.get("current_input", {}).copy()
                current_input["referenced_chunks"] = referenced_chunks
                state["current_input"] = current_input
                
                # Enhanced detection of specialist delegation
                for specialist in [
                    "positioning specialist", "title/blurb optimizer",
                    "differentiation strategist"
                ]:
                    if specialist in last_message_lower:
                        return specialist.replace(" ", "_")
                
                # Check for research needs
                if "research" in last_message_lower:
                    return "market_research"
            
            # Count market director visits
            market_visits = sum(1 for msg in messages if
                               msg.get("role") == "user" and
                               "Market Alignment Director" in msg.get("content", ""))
                
                
            if market_visits > 2:
                # After multiple visits, try a specialist directly
                return "positioning_specialist"
            else:
                # Give another chance for delegation
                return "title_blurb_optimizer"
                
        def route_after_research(state: AgentState) -> str:
            """Route after market research."""
            return "market_alignment_director"

        builder.add_conditional_edges(
            "executive_director",
            route_after_executive_director
        )
        builder.add_conditional_edges(
            "market_alignment_director",
            route_after_market_director
        )
        
        builder.add_conditional_edges(
            "market_research",
            route_after_research
        )
        # Connect specialized agents to their supervisors
        for agent in ["positioning_specialist", "title_blurb_optimizer", "differentiation_strategist"]:
            if agent in agents[phase]:  # Only add edges for agents that exist in this phase
                builder.add_edge(agent, "market_alignment_director")
        # Connect other specialists
        if "formatting_standards_expert" in agents[phase]:
            builder.add_edge("formatting_standards_expert", "editorial_director")
            
        # Connect other directors back to executive director
        builder.add_edge("editorial_director", "executive_director")
    
    return builder.compile()

def create_main_graph(config: Dict[str, Any], model_config=None) -> StateGraph:
    """Create a main composite graph that includes all phase graphs."""
    # Create a graph builder
    builder = StateGraph(AgentState)
    
    # Create all phase graphs
    phase_graphs = {}
    for phase in ["initialization", "development", "creation", "refinement", "finalization"]:
        phase_graphs[phase] = create_phase_graph(phase, "composite_project", config, model_config)
    
    # Add phase graphs as nodes to the main graph
    for phase, graph in phase_graphs.items():
        builder.add_node(phase, graph)
    
    # Set entry point to initialization phase
    builder.set_entry_point("initialization")
    
    # Define transitions between phases
    def route_after_phase(state: AgentState) -> str:
        """Route after a phase completes."""
        current_phase = state.get("phase", "initialization")
        
        # Define the phase progression
        phase_order = ["initialization", "development", "creation", "refinement", "finalization"]
        
        # Find the current phase index
        try:
            current_index = phase_order.index(current_phase)
        except ValueError:
            # If the current phase is not in the list, start with initialization
            return "initialization"
        
        # Move to the next phase if not at the end
        if current_index < len(phase_order) - 1:
            next_phase = phase_order[current_index + 1]
            # Update the state phase
            state["phase"] = next_phase
            return next_phase
        else:
            # If we're at the final phase, we're done
            return END
    
    # Add conditional edges between phases
    for phase in ["initialization", "development", "creation", "refinement"]:
        builder.add_conditional_edges(
            phase,
            route_after_phase
        )
    
    # Add a final edge from finalization to END
    builder.add_edge("finalization", END)
    
    return builder.compile()

# This function creates a comprehensive graph with all agents exposed
def create_storybook_graph(config: Dict[str, Any], model_config=None) -> StateGraph:
    """Create a comprehensive storybook graph with all agents fully exposed."""
    builder = StateGraph(AgentState)
    
    # Create agent factory
    agent_factory = AgentFactory(config, model_config=model_config)
    
    # Define all possible agents including specialists and research nodes
    all_agents = {
        # Directors
        "executive_director", "creative_director", "editorial_director",
        "content_development_director", "market_alignment_director",
        
        # Creative Team
        "structure_architect", "plot_development_specialist", "world_building_expert",
        "character_psychology_specialist", "character_voice_designer",
        "character_relationship_mapper",
        
        # Content Team
        "chapter_drafters", "scene_construction_specialists", "dialogue_crafters",
        "continuity_manager", "voice_consistency_monitor", "emotional_arc_designer",
        
        # Editorial Team
        "structural_editor", "character_arc_evaluator", "thematic_coherence_analyst",
        "prose_enhancement_specialist", "dialogue_refinement_expert",
        "rhythm_cadence_optimizer", "grammar_consistency_checker",
        "fact_verification_specialist",
        
        # Research Team
        "domain_knowledge_specialist", "cultural_authenticity_expert",
        
        # Marketing Team
        "positioning_specialist", "title_blurb_optimizer", "differentiation_strategist",
        "formatting_standards_expert",
        
        # Research Nodes
        "domain_research", "cultural_research", "market_research"
    }
    
    # Add all agents as nodes
    for agent_name in all_agents:
        if agent_name.endswith('_research'):
            research_type = agent_name.split('_')[0]
            builder.add_node(agent_name, agent_factory.create_research_agent(research_type))
        else:
            builder.add_node(agent_name, agent_factory.create_agent(agent_name, "storybook_project"))
    
    # Define all possible connections between agents
    connections = {
        "executive_director": [
            "creative_director", "editorial_director", "content_development_director",
            "market_alignment_director", "domain_knowledge_specialist",
            "cultural_authenticity_expert"
        ],
        "creative_director": [
            "structure_architect", "plot_development_specialist", "world_building_expert",
            "character_psychology_specialist", "character_voice_designer",
            "character_relationship_mapper", "executive_director"
        ],
        "editorial_director": [
            "structural_editor", "character_arc_evaluator", "thematic_coherence_analyst",
            "prose_enhancement_specialist", "dialogue_refinement_expert",
            "rhythm_cadence_optimizer", "grammar_consistency_checker",
            "fact_verification_specialist", "executive_director"
        ],
        "content_development_director": [
            "chapter_drafters", "scene_construction_specialists", "dialogue_crafters",
            "continuity_manager", "voice_consistency_monitor", "emotional_arc_designer",
            "executive_director"
        ],
        "market_alignment_director": [
            "positioning_specialist", "title_blurb_optimizer", "differentiation_strategist",
            "market_research", "executive_director"
        ],
        # Research connections
        "domain_knowledge_specialist": ["domain_research", "executive_director"],
        "cultural_authenticity_expert": ["cultural_research", "executive_director"],
        "fact_verification_specialist": ["domain_research", "editorial_director"]
    }
    
    # Add all connections
    for source, targets in connections.items():
        for target in targets:
            builder.add_edge(source, target)
    
    # Add return paths from specialists to their directors
    specialist_to_director = {
        # Creative team to creative director
        "structure_architect": "creative_director",
        "plot_development_specialist": "creative_director",
        "world_building_expert": "creative_director",
        "character_psychology_specialist": "creative_director",
        "character_voice_designer": "creative_director",
        "character_relationship_mapper": "creative_director",
        
        # Content team to content development director
        "chapter_drafters": "content_development_director",
        "scene_construction_specialists": "content_development_director",
        "dialogue_crafters": "content_development_director",
        "continuity_manager": "content_development_director",
        "voice_consistency_monitor": "content_development_director",
        "emotional_arc_designer": "content_development_director",
        
        # Editorial team to editorial director
        "structural_editor": "editorial_director",
        "character_arc_evaluator": "editorial_director",
        "thematic_coherence_analyst": "editorial_director",
        "prose_enhancement_specialist": "editorial_director",
        "dialogue_refinement_expert": "editorial_director",
        "rhythm_cadence_optimizer": "editorial_director",
        "grammar_consistency_checker": "editorial_director",
        
        # Marketing team to market alignment director
        "positioning_specialist": "market_alignment_director",
        "title_blurb_optimizer": "market_alignment_director",
        "differentiation_strategist": "market_alignment_director"
    }
    
    # Add return paths
    for specialist, director in specialist_to_director.items():
        builder.add_edge(specialist, director)
    
    # Add research return paths
    builder.add_edge("domain_research", "domain_knowledge_specialist")
    builder.add_edge("cultural_research", "cultural_authenticity_expert")
    builder.add_edge("market_research", "market_alignment_director")
    
    # Set entry point
    builder.set_entry_point("executive_director")
    
    # Add end state connections
    builder.add_edge("executive_director", END)
    
    return builder.compile()

class storybook:
    def __init__(self, model_config=None):
        # Check for HuggingFace token
        self.huggingface_token = os.getenv("HUGGINGFACE_API_TOKEN")
        if not self.huggingface_token:
            raise ValueError("HUGGINGFACE_API_TOKEN not found in environment variables")
        
        # Log into Hugging Face
        login(token=self.huggingface_token)
        
        # Default model config if none provided
        self.model_config = model_config or {
            "model_id": "HuggingFaceH4/zephyr-7b-beta",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": False,
            "repetition_penalty": 1.03
        }
        
        self.tavily = TavilyClient(api_key=tavily_ai_api_key)
        
        # Setup storybook configuration
        self.storybook_config = {
            "mongodb_connection_string": None,
            "mongodb_database_name": None,
            "quality_gates": {
                "initialization_to_development": {
                    "planning_quality": 0.7,
                    "market_alignment": 0.6
                },
                "development_to_creation": {
                    "structure_quality": 0.7,
                    "character_depth": 0.7,
                    "world_building": 0.7
                },
                "creation_to_refinement": {
                    "content_quality": 0.7,
                    "narrative_flow": 0.7,
                    "dialogue_quality": 0.7
                },
                "refinement_to_finalization": {
                    "editing_quality": 0.8,
                    "prose_quality": 0.8,
                    "thematic_coherence": 0.7
                },
                "finalization_to_complete": {
                    "market_readiness": 0.8,
                    "overall_quality": 0.8
                }
            }
        }
        
        # Build the storybook graphs for each phase
        self.phase_graphs = {}
        for phase in ["initialization", "development", "creation", "refinement", "finalization"]:
            self.phase_graphs[phase] = create_phase_graph(phase, "default_project", self.storybook_config, self.model_config)
            
        # Build the main composite graph
        self.graph = create_main_graph(self.storybook_config, self.model_config)
        
        # Build the comprehensive storybook graph with all agents
        self.storybook_graph = create_storybook_graph(self.storybook_config, self.model_config)
    
    def update_model_config(self, model_config):
        """Update the model configuration and recreate the agent factory."""
        self.model_config = model_config
        
        # Rebuild the storybook graphs for each phase
        self.phase_graphs = {}
        for phase in ["initialization", "development", "creation", "refinement", "finalization"]:
            self.phase_graphs[phase] = create_phase_graph(phase, "default_project", self.storybook_config, self.model_config)
            
        # Rebuild the main composite graph
        self.graph = create_main_graph(self.storybook_config, self.model_config)
        
        # Rebuild the comprehensive storybook graph
        self.storybook_graph = create_storybook_graph(self.storybook_config, self.model_config)
    
    def initialize_storybook_project(self, title: str, synopsis: str, manuscript: str, notes: Optional[Dict[str, Any]] = None):
        """Initialize a new storybook project"""
        project_id = str(uuid4())
        
        # Split the manuscript into manageable chunks
        manuscript_chunks = split_manuscript(manuscript)
        
        initial_state = {
            "project": {
                "id": project_id,
                "title": title,
                "synopsis": synopsis,
                "manuscript": manuscript,
                "manuscript_chunks": manuscript_chunks,
                "notes": notes or {},
                "type": "creation",
                "quality_assessment": {
                    "planning_quality": 0.6,  # Initial values
                    "market_alignment": 0.5,
                },
                "created_at": datetime.now().isoformat()
            },
            "phase": "initialization",
            "current_input": {
                "task": "Initial project assessment and planning",
                "phase": "initialization"
            },
            "phase_history": {},
            "messages": [],
            "count": 0,
            "lnode": ""
        }
        
        return initial_state
    
    def run_storybook_phase(self, state, phase="initialization"):
        """Run a specific phase of the storybook workflow"""
        if phase not in self.phase_graphs:
            raise ValueError(f"Unknown phase: {phase}")
        
        # Make sure the state includes the phase
        if "phase" not in state:
            state["phase"] = phase
            
        # Run the phase graph
        result = self.phase_graphs[phase].invoke(state)
        return result
        
    def run_storybook(self, state):
        """Run the full storybook workflow using the main graph"""
        result = self.graph.invoke(state)
        return result
    
    def get_available_checkpoints(self, project_id=None):
        """List available checkpoints"""
        checkpoint_dir = os.path.join(os.getcwd(), "checkpoints")
        if not os.path.exists(checkpoint_dir):
            return []
        
        checkpoints = []
        for file in os.listdir(checkpoint_dir):
            if file.endswith(".sqlite"):
                if project_id is None or file.startswith(f"{project_id}_"):
                    try:
                        # Extract project ID and phase from filename
                        parts = file.replace(".sqlite", "").split("_")
                        if len(parts) >= 2:
                            project_id = "_".join(parts[:-1])  # Handle project IDs that might contain underscores
                            phase = parts[-1]
                            
                            # Get file modification time
                            file_path = os.path.join(checkpoint_dir, file)
                            mod_time = os.path.getmtime(file_path)
                            mod_time_str = datetime.fromtimestamp(mod_time).strftime('%Y-%m-%d %H:%M:%S')
                            
                            checkpoints.append({
                                "project_id": project_id,
                                "phase": phase,
                                "checkpoint_file": file,
                                "last_modified": mod_time_str
                            })
                    except Exception as e:
                        print(f"Error parsing checkpoint file {file}: {str(e)}")
        
        # Sort by last modified time
        checkpoints.sort(key=lambda x: x["last_modified"], reverse=True)
        return checkpoints
    
    def load_checkpoint(self, checkpoint_file):
        """Load a specific checkpoint"""
        checkpoint_path = os.path.join(os.getcwd(), "checkpoints", checkpoint_file)
        if not os.path.exists(checkpoint_path):
            raise ValueError(f"Checkpoint file not found: {checkpoint_path}")
        
        # Extract phase from filename
        phase = checkpoint_file.split("_")[-1].replace(".sqlite", "")
        
        # Get the appropriate graph
        if phase not in self.phase_graphs:
            raise ValueError(f"Unknown phase in checkpoint: {phase}")
            
        graph = self.phase_graphs[phase]
        
        # Create a SQLite checkpoint saver
        saver = SqliteSaver(checkpoint_path)
        
        # Load the latest checkpoint
        thread_ids = saver.list_threads()
        if not thread_ids:
            raise ValueError(f"No threads found in checkpoint file {checkpoint_file}")
        
        # Use the most recent thread
        latest_thread_id = thread_ids[-1]
        state = saver.get_state(latest_thread_id)
        
        return state

class writer_gui:
    def __init__(self, storybook_instance):
        # Initialize model choices first
        self.model_choices = [
            "HuggingFaceH4/zephyr-7b-beta",
            "meta-llama/Llama-2-7b-chat-hf",
            "mistralai/Mistral-7B-Instruct-v0.2",
            "google/flan-t5-base",
            "facebook/bart-large-cnn",
            "gpt2",
            "distilgpt2",
            "microsoft/phi-2"
        ]
        
        # Then initialize other attributes
        self.storybook = storybook_instance
        self.partial_message = ""
        self.thread_id = -1
        self.thread = {"configurable": {"thread_id": str(self.thread_id)}}
        self.is_running = False
        self.agent_visits = {}
        self.project_id = None
        
        self.default_model_config = {
            "model_id": "HuggingFaceH4/zephyr-7b-beta",
            "task": "text-generation",
            "temperature": 0.1,
            "max_new_tokens": 512,
            "do_sample": False,
            "repetition_penalty": 1.03
        }
        
        # Create interface last
        self.demo = self.create_interface()

    async def stream_response(self, message, word_delay=0.01):
        """Stream a response word by word with a delay"""
        words = message.split()
        partial = ""
        for word in words:
            partial += word + " "
            yield partial
            await asyncio.sleep(word_delay)

    async def run_storybook_with_model(self, title, synopsis, manuscript, notes_text, task, phase,
                                     model_id, model_task, temperature, max_new_tokens,
                                     repetition_penalty, do_sample, max_iterations=10):
        """Run the storybook workflow with streaming updates and custom model config"""
        if self.is_running:
            yield "Already running a session. Please wait for it to complete.", "", "", "", "", []
            return
        self.is_running = True
        iterations = 0
        state = None
        quality_str = ""
        stream_buffer = []
        
        try:
            # Create model configuration
            model_config = {
                "model_id": model_id,
                "task": model_task,
                "temperature": temperature,
                "max_new_tokens": max_new_tokens,
                "repetition_penalty": repetition_penalty,
                "do_sample": do_sample
            }
            
            # Update storybook with model config
            self.storybook.update_model_config(model_config)
            
            # Parse notes from text input
            notes = {}
            if notes_text:
                for line in notes_text.strip().split('\n'):
                    if ':' in line:
                        key, value = line.split(':', 1)
                        notes[key.strip()] = value.strip()
            
            # Initialize a new project
            initial_state = self.storybook.initialize_storybook_project(title, synopsis, manuscript, notes)
            self.project_id = initial_state["project"]["id"]
            
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Initialized project with ID: {self.project_id}")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Using model: {model_id}")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, "initialization", phase, self.project_id, "", stream_buffer
            
            # Set the phase in the state
            initial_state["phase"] = phase
            initial_state["current_input"]["phase"] = phase
            
            # Set the task
            if task:
                initial_state["current_input"]["task"] = task
            
            # Run the workflow for the specified phase
            state = initial_state
            while iterations < max_iterations:
                try:
                    stream_buffer.append(f"\n[{datetime.now().strftime('%H:%M:%S')}] --- Iteration {iterations + 1} in phase '{phase}' ---")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, state.get('lnode', 'starting'), phase, self.project_id, "", stream_buffer
                    
                    # Run the phase with real-time logging
                    agent_name = state.get("lnode", "starting")
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Working with agent: {agent_name}")
                    
                    # Run the phase and update state
                    state = self.storybook.run_storybook_phase(state, phase)
                    
                    # Extract the last message for display
                    if state.get("messages", []):
                        last_message = state["messages"][-1]
                        agent_name = state.get("lnode", "unknown")
                        content_preview = last_message.get('content', '')[:300]
                        if len(last_message.get('content', '')) > 300:
                            content_preview += "..."
                            
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] [{agent_name}]: {content_preview}")
                        
                    # Update quality assessment display
                    quality_assessment = state.get("project", {}).get("quality_assessment", {})
                    quality_str = "Quality Assessment:\n" + "\n".join(
                        [f"- {k}: {v:.2f}" for k, v in quality_assessment.items()]
                    )
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Updated quality assessment")
                    
                    # Check if workflow has ended
                    if state.get("lnode") == "END":
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Workflow completed successfully.")
                        break
                    
                    iterations += 1
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, state.get('lnode', 'working'), phase, self.project_id, quality_str, stream_buffer
                    
                except Exception as e:
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error in iteration: {str(e)}")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, "error", phase, self.project_id, "", stream_buffer
                    break
            
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Workflow finished after {iterations} iterations.")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, state.get('lnode', 'complete'), phase, self.project_id, quality_str, stream_buffer
        
        except Exception as e:
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Unhandled error: {str(e)}")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, "error", phase, self.project_id, "", stream_buffer
        
        finally:
            self.is_running = False

    def get_checkpoints(self):
        """Get available checkpoints for loading"""
        try:
            checkpoints = self.storybook.get_available_checkpoints()
            checkpoint_list = [f"{cp['project_id']} - {cp['phase']} ({cp['last_modified']})" for cp in checkpoints]
            checkpoint_files = [cp["checkpoint_file"] for cp in checkpoints]
            return checkpoint_list, checkpoint_files
        except Exception as e:
            print(f"Error getting checkpoints: {str(e)}")
            return [], []

    def load_project_from_checkpoint(self, checkpoint_file):
        """Load a project from a checkpoint file"""
        if not checkpoint_file:
            return "No checkpoint selected", "", "", "", "", []
        
        try:
            state = self.storybook.load_checkpoint(checkpoint_file)
            
            # Extract project details
            project = state.get("project", {})
            self.project_id = project.get("id", "unknown")
            phase = state.get("phase", "unknown")
            
            # Prepare status message
            stream_buffer = [
                f"[{datetime.now().strftime('%H:%M:%S')}] Loaded project {self.project_id} from checkpoint",
                f"[{datetime.now().strftime('%H:%M:%S')}] Title: {project.get('title', 'Untitled')}",
                f"[{datetime.now().strftime('%H:%M:%S')}] Phase: {phase}",
                f"[{datetime.now().strftime('%H:%M:%S')}] Last agent: {state.get('lnode', 'unknown')}"
            ]
            
            # Include quality assessment
            quality_assessment = project.get("quality_assessment", {})
            quality_str = "Quality Assessment:\n" + "\n".join(
                [f"- {k}: {v:.2f}" for k, v in quality_assessment.items()]
            )
            
            # Return the state information to update UI
            message = "\n".join(stream_buffer)
            return message, state.get('lnode', 'unknown'), phase, self.project_id, quality_str, stream_buffer, state
        except Exception as e:
            error_message = f"Error loading checkpoint: {str(e)}"
            return error_message, "", "", "", "", [error_message], None

    async def continue_from_checkpoint(self, state, task, phase, max_iterations=10):
        """Continue processing from a loaded checkpoint"""
        if self.is_running:
            yield "Already running a session. Please wait for it to complete.", "", "", "", "", []
            return
        
        if not state:
            yield "No state loaded from checkpoint", "", "", "", "", []
            return
            
        self.is_running = True
        iterations = 0
        quality_str = ""
        stream_buffer = []
        
        try:
            # Update the task and phase if provided
            if task:
                state["current_input"]["task"] = task
            
            if phase:
                state["phase"] = phase
                state["current_input"]["phase"] = phase
            
            self.project_id = state["project"]["id"]
            current_phase = state["phase"]
            
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Continuing project with ID: {self.project_id}")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Phase: {current_phase}")
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Task: {state['current_input']['task']}")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, state.get('lnode', 'starting'), current_phase, self.project_id, "", stream_buffer
            
            # Run the workflow for the specified phase
            while iterations < max_iterations:
                try:
                    stream_buffer.append(f"\n[{datetime.now().strftime('%H:%M:%S')}] --- Iteration {iterations + 1} in phase '{current_phase}' ---")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, state.get('lnode', 'continuing'), current_phase, self.project_id, "", stream_buffer
                    
                    # Run the phase with real-time logging
                    agent_name = state.get("lnode", "continuing")
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Working with agent: {agent_name}")
                    
                    # Run the phase and update state
                    state = self.storybook.run_storybook_phase(state, current_phase)
                    
                    # Extract the last message for display
                    if state.get("messages", []):
                        last_message = state["messages"][-1]
                        agent_name = state.get("lnode", "unknown")
                        content_preview = last_message.get('content', '')[:300]
                        if len(last_message.get('content', '')) > 300:
                            content_preview += "..."
                            
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] [{agent_name}]: {content_preview}")
                        
                    # Update quality assessment display
                    quality_assessment = state.get("project", {}).get("quality_assessment", {})
                    quality_str = "Quality Assessment:\n" + "\n".join(
                        [f"- {k}: {v:.2f}" for k, v in quality_assessment.items()]
                    )
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Updated quality assessment")
                    
                    # Check if workflow has ended
                    if state.get("lnode") == "END":
                        stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Workflow completed successfully.")
                        break
                    
                    iterations += 1
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, state.get('lnode', 'working'), current_phase, self.project_id, quality_str, stream_buffer
                    
                except Exception as e:
                    stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Error in iteration: {str(e)}")
                    self.partial_message = "\n".join(stream_buffer)
                    yield self.partial_message, "error", current_phase, self.project_id, "", stream_buffer
                    break
            
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Workflow finished after {iterations} iterations.")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, state.get('lnode', 'complete'), current_phase, self.project_id, quality_str, stream_buffer
            
        except Exception as e:
            stream_buffer.append(f"[{datetime.now().strftime('%H:%M:%S')}] Unhandled error: {str(e)}")
            self.partial_message = "\n".join(stream_buffer)
            yield self.partial_message, "error", state.get("phase", "unknown"), self.project_id, "", stream_buffer
        
        finally:
            self.is_running = False

    def create_interface(self):
        theme = gr.themes.Ocean(
            primary_hue="amber",
            secondary_hue="fuchsia",
            neutral_hue="slate",
            font=[gr.themes.GoogleFont('Work Sans'), 'ui-sans-serif', 'system-ui', 'sans-serif'],
        )
        
        with gr.Blocks(
            theme=theme,
            analytics_enabled=False
        ) as demo:
            gr.Markdown("# storybook writer")
            
            # First Row - Status Panel
            with gr.Row():
                sb_lnode_bx = gr.Textbox(label="Last Agent", scale=1)
                sb_phase_bx = gr.Textbox(label="Current Phase", scale=1)
                sb_thread_bx = gr.Textbox(label="Project ID", scale=1)
                sb_quality_bx = gr.Textbox(label="Quality Assessment", scale=2)
            
            # Hidden state for loaded checkpoints
            loaded_state = gr.State(None)
            
            # Stream output pane that will show real-time updates
            stream_output = gr.State([])
            
            with gr.Tabs() as tabs:
                # Model Configuration Tab
                with gr.Tab("Model Configuration"):
                    with gr.Row():
                        model_id = gr.Dropdown(
                            label="Model ID",
                            choices=self.model_choices,
                            value=self.default_model_config["model_id"]
                        )
                        model_task = gr.Dropdown(
                            label="Task",
                            choices=["text-generation", "text2text-generation", "summarization"],
                            value="text-generation"
                        )
                        
                    with gr.Row():
                        temperature = gr.Slider(
                            label="Temperature",
                            minimum=0.0,
                            maximum=1.0,
                            value=self.default_model_config["temperature"],
                            step=0.01
                        )
                        max_new_tokens = gr.Slider(
                            label="Max New Tokens",
                            minimum=10,
                            maximum=1024,
                            value=self.default_model_config["max_new_tokens"],
                            step=1
                        )
                        repetition_penalty = gr.Slider(
                            label="Repetition Penalty",
                            minimum=1.0,
                            maximum=2.0,
                            value=self.default_model_config["repetition_penalty"],
                            step=0.01
                        )
                        
                    with gr.Row():
                        do_sample = gr.Checkbox(
                            label="Do Sample",
                            value=self.default_model_config["do_sample"]
                        )
                
                # Task Control Tab
                with gr.Tab("Task Control"):
                    with gr.Row():
                        task_bx = gr.Textbox(
                            label="Task",
                            value="Initial project assessment and planning",
                            scale=2
                        )
                        phase_select = gr.Dropdown(
                            choices=["initialization", "development", "creation", "refinement", "finalization"],
                            value="initialization",
                            label="Phase",
                            scale=1
                        )
                
                # Project Content Tab
                with gr.Tab("Project Content"):
                    with gr.Row():
                        with gr.Column():
                            title_bx = gr.Textbox(
                                label="Title (required)",
                                value="The Hidden Quest"
                            )
                            synopsis_bx = gr.Textbox(
                                label="Synopsis (required)",
                                value="A young adventurer discovers a mysterious artifact...",
                                lines=4
                            )
                            
                        with gr.Column():
                            manuscript_bx = gr.Textbox(
                                label="Manuscript (required)",
                                value="Chapter 1\n\nThe sun was setting over...",
                                lines=10
                            )
                            notes_bx = gr.Textbox(
                                label="Notes (optional - key: value format)",
                                value="target_audience: Young Adult\ngenre: Fantasy",
                                lines=4
                            )
                
                # Checkpoint Tab
                with gr.Tab("Checkpoints"):
                    with gr.Row():
                        refresh_checkpoints_btn = gr.Button("Refresh Checkpoints")
                        checkpoint_list = gr.Dropdown(
                            label="Available Checkpoints",
                            choices=[],
                            value=None,
                            interactive=True
                        )
                        checkpoint_file = gr.State([])
                    
                    with gr.Row():
                        load_checkpoint_btn = gr.Button("Load Selected Checkpoint")
                        continue_checkpoint_btn = gr.Button("Continue from Checkpoint")
                        
            # Run Buttons - outside tabs
            with gr.Row():
                storybook_btn = gr.Button("Run New Project", variant="primary", scale=1)
                clear_btn = gr.Button("Clear Output", scale=1)
            
            # Streaming Output area with real-time updates
            sb_live = gr.Textbox(label="storybook Output", lines=20, autoscroll=True)
            
            # Event handlers
            # Refresh checkpoints
            refresh_checkpoints_btn.click(
                fn=self.get_checkpoints,
                inputs=[],
                outputs=[checkpoint_list, checkpoint_file]
            )
            
            # Load checkpoint
            load_checkpoint_btn.click(
                fn=lambda x, y: y[x] if x < len(y) else None,
                inputs=[checkpoint_list, checkpoint_file],
                outputs=gr.State(None)
            ).then(
                fn=self.load_project_from_checkpoint,
                inputs=[gr.State(None)],
                outputs=[sb_live, sb_lnode_bx, sb_phase_bx, sb_thread_bx, sb_quality_bx, stream_output, loaded_state]
            )
            
            # Continue from checkpoint
            continue_checkpoint_btn.click(
                fn=lambda: (
                    gr.update(value="Processing...", interactive=False),  # Update button
                    gr.update(value="Continuing from checkpoint...")      # Update output
                ),
                inputs=None,
                outputs=[continue_checkpoint_btn, sb_live]
            ).then(
                fn=self.continue_from_checkpoint,
                inputs=[loaded_state, task_bx, phase_select],
                outputs=[sb_live, sb_lnode_bx, sb_phase_bx, sb_thread_bx, sb_quality_bx, stream_output],
                show_progress=True
            ).then(
                fn=lambda: gr.update(value="Continue from Checkpoint", interactive=True),
                inputs=None,
                outputs=continue_checkpoint_btn
            )
            
            # Run new project
            storybook_btn.click(
                fn=lambda: (
                    gr.update(value="Processing...", interactive=False),  # Update button
                    gr.update(value="Starting workflow...")               # Update output
                ),
                inputs=None,
                outputs=[storybook_btn, sb_live]
            ).then(
                fn=self.run_storybook_with_model,
                inputs=[
                    title_bx, synopsis_bx, manuscript_bx, notes_bx, task_bx, phase_select,
                    model_id, model_task, temperature, max_new_tokens, repetition_penalty, do_sample
                ],
                outputs=[sb_live, sb_lnode_bx, sb_phase_bx, sb_thread_bx, sb_quality_bx, stream_output],
                show_progress=True
            ).then(
                fn=lambda: gr.update(value="Run New Project", variant="primary", interactive=True),
                inputs=None,
                outputs=storybook_btn
            )
            
            # Clear output
            clear_btn.click(
                fn=lambda: (
                    "", 
                    "", 
                    "",
                    "",
                    "",
                    []
                ),
                inputs=[],
                outputs=[sb_live, sb_lnode_bx, sb_phase_bx, sb_thread_bx, sb_quality_bx, stream_output]
            )
            
            # Help Section
            with gr.Accordion("Help & Information", open=False):
                gr.Markdown(
                    "# How to use storybook\n\n"
                    "1. Enter title and synopsis\n"
                    "2. Add manuscript content\n"
                    "3. Add optional notes\n"
                    "4. Configure model parameters\n"
                    "5. Choose task and phase\n"
                    "6. Click Run storybook\n\n"
                    "## Using Checkpoints\n"
                    "1. Go to the Checkpoints tab and click 'Refresh Checkpoints'\n"
                    "2. Select a checkpoint from the dropdown\n"
                    "3. Click 'Load Selected Checkpoint' to view project details\n"
                    "4. Click 'Continue from Checkpoint' to resume processing\n\n"
                    "## Phases\n"
                    "- Initialization: Project setup\n"
                    "- Development: Story building\n"
                    "- Creation: Content writing\n"
                    "- Refinement: Editing\n"
                    "- Finalization: Polishing"
                )
        
        # Initialize checkpoints
        def init_checkpoints():
            checkpoint_list_values, checkpoint_file_values = self.get_checkpoints()
            return checkpoint_list_values, checkpoint_file_values
        
        demo.load(
            fn=init_checkpoints,
            inputs=None,
            outputs=[checkpoint_list, checkpoint_file]
        )
                
        return demo

    def launch(self):
        self.demo.launch(share=False)  # Set share to False to avoid sharing issues

def visualize_storybook_graph(graph: StateGraph):
    """Create a visual representation of the storybook graph."""
    G = nx.DiGraph()
    
    # Add nodes
    for node in graph.nodes:
        G.add_node(node)
    
    # Add edges
    for node, edges in graph.edges.items():
        for edge in edges:
            G.add_edge(node, edge)
    
    # Set up the plot
    plt.figure(figsize=(20, 20))
    pos = nx.spring_layout(G, k=2, iterations=50)
    
    # Draw the graph
    nx.draw(G, pos, with_labels=True, node_color='lightblue',
            node_size=2000, font_size=8, font_weight='bold',
            arrows=True, edge_color='gray', arrowsize=20)
    
    plt.title("Storybook Agent Graph")
    return plt

# Create the main storybook instance at module level
sb_instance = storybook()
graph = sb_instance.graph

# Create a comprehensive workflow graph that includes all agents
# This will be accessed via the langgraph server API
storybook = sb_instance.storybook_graph

# Initialize CUDA handling
init_cuda()

# Suppress NVML warnings
warnings.filterwarnings("ignore", ".*Can't initialize NVML.*")

if __name__ == "__main__":
    app = writer_gui(sb_instance)
    app.launch()